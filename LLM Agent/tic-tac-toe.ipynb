{"cells":[{"cell_type":"markdown","metadata":{"id":"bYqX37_W4Jhr"},"source":["To train this agent, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n","\n","<div class=\"align-center\">\n","<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n","<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n","<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n","\n","Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n","\n","</div>\n","\n","<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n","\n","This notebook shows how to train a Qwen 2.5 3B model to play tic tac toe. It will demonstrate how to set up a multi-turn agent, how to train it, and how to evaluate it.\n","\n","Completions will be logged to OpenPipe, and metrics will be logged to Weights & Biases.\n","\n","You will learn how to construct an [agentic environment](#Environment), how to define a [rollout](#Rollout), and how to run a [training loop](#Loop).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkwIEWa24Jht"},"outputs":[],"source":["!pip install \"numpy<2.0.0\""]},{"cell_type":"markdown","metadata":{"id":"0bZbs-ZS4Jhu"},"source":["### WARNING:\n","\n","If you are running in Google Colab and installing numpy does not say \"Requirement already satisfied: numpy<2.0.0\" then click \"Runtime\" and \"Restart Session.\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04bJgk4Y4Jhv"},"outputs":[],"source":["# make sure we're using numpy 1.*.*\n","import numpy as np\n","\n","if (np.__version__).startswith(\"1.\"):\n","    print(\"Numpy version is 1.*.*, you're good to go!\")\n","else:\n","    raise ValueError(\"Please restart your runtime using the above instructions!\")"]},{"cell_type":"markdown","metadata":{"id":"asd8mnG14Jhv"},"source":["### Environment Variables\n","\n","Later on in the notebook, we'll be creating a model that can automatically logs metrics to Weights & Biases. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n","\n","You can also optionally initiate an OpenPipe client to report completions to a [dashboard](https://app.openpipe.ai) to get a feel for what the completions your model is generating look like, and how they change over time. Logging to OpenPipe is free, but is not required for training!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zu_s1k4W4Jhv"},"outputs":[],"source":["import os\n","\n","\n","# Optional\n","WANDB_API_KEY = \"\"\n","if WANDB_API_KEY:\n","    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n","\n","# Optional\n","OPENPIPE_API_KEY = \"\"\n","if OPENPIPE_API_KEY:\n","    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY"]},{"cell_type":"markdown","metadata":{"id":"w4uZj1Ar4Jhv"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfMJkbFO4Jhv"},"outputs":[],"source":["%%capture\n","!uv pip install openpipe-art openpipe --prerelease allow --no-cache-dir"]},{"cell_type":"markdown","metadata":{"id":"nKQFRihl4Jhw"},"source":["### Agentic Environment\n","\n","<a name=\"Environment\"></a>\n","\n","ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment in which the agent can play tic tac toe.\n","\n","Feel free to read as much or as little of this section's code as you'd like. The important thing to understand is that we're defining the rules of this agent's environment. In many cases, this will already be defined by the task you're trying to solve, but if you need to define a custom environment, this is how you do it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6HvwXBN4Jhw"},"outputs":[],"source":["import random\n","from typing import TypedDict\n","from typing import Literal\n","import xml.etree.ElementTree as ET\n","\n","\n","class TicTacToeGame(TypedDict):\n","    board: list[list[str]]\n","    agent_symbol: Literal[\"x\", \"o\"]\n","    opponent_symbol: Literal[\"x\", \"o\"]\n","\n","\n","def generate_game(board_length: int = 3) -> TicTacToeGame:\n","    board = [[\"_\" for _ in range(board_length)] for _ in range(board_length)]\n","    agent_symbol = random.choice([\"x\", \"o\"])\n","    opponent_symbol = \"x\" if agent_symbol == \"o\" else \"o\"\n","    return {\n","        \"board\": board,\n","        \"agent_symbol\": agent_symbol,\n","        \"opponent_symbol\": opponent_symbol,\n","    }\n","\n","\n","def render_board(game: TicTacToeGame) -> str:\n","    board = game[\"board\"]\n","    board_length = len(board)\n","    # print something like this:\n","    #    1   2   3\n","    # A  _ | x | x\n","    # B  o | _ | _\n","    # C  _ | o | _\n","    # where _ is an empty cell\n","\n","    board_str = \"   \" + \"   \".join([str(i + 1) for i in range(board_length)]) + \"\\n\"\n","    for i in range(board_length):\n","        board_str += f\"{chr(65 + i)}  {board[i][0]} | {board[i][1]} | {board[i][2]}\\n\"\n","    return board_str\n","\n","\n","def get_opponent_move(game: TicTacToeGame) -> tuple[int, int]:\n","    # get a random empty cell\n","    empty_cells = [\n","        (i, j) for i in range(3) for j in range(3) if game[\"board\"][i][j] == \"_\"\n","    ]\n","    return random.choice(empty_cells)\n","\n","\n","def apply_agent_move(game: TicTacToeGame, move: str) -> None:\n","    board_length = len(game[\"board\"])\n","\n","    try:\n","        root = ET.fromstring(move)\n","        square = root.text\n","    except Exception:\n","        raise ValueError(\"Invalid xml\")\n","\n","    try:\n","        row_index = ord(square[0]) - 65\n","        col_index = int(square[1]) - 1\n","    except Exception as e:\n","        print(e)\n","        raise ValueError(\"Unable to parse square\")\n","\n","    if (\n","        row_index < 0\n","        or row_index >= board_length\n","        or col_index < 0\n","        or col_index >= board_length\n","    ):\n","        raise ValueError(\n","            f\"Invalid move, row or column out of bounds: {row_index}, {col_index}\"\n","        )\n","\n","    # check if the move is valid\n","    if game[\"board\"][row_index][col_index] != \"_\":\n","        raise ValueError(\"Square already occupied\")\n","\n","    game[\"board\"][row_index][col_index] = game[\"agent_symbol\"]\n","\n","\n","def check_winner(board: list[list[str]]) -> Literal[\"x\", \"o\", \"draw\", None]:\n","    board_length = len(board)\n","    # check rows\n","    for row in board:\n","        if row.count(row[0]) == board_length and row[0] != \"_\":\n","            return row[0]\n","    # check columns\n","    for col in range(board_length):\n","        if [board[row][col] for row in range(board_length)].count(\n","            board[0][col]\n","        ) == board_length and board[0][col] != \"_\":\n","            return board[0][col]\n","\n","    # top right to bottom left\n","    upward_diagonal = [board[i][board_length - i - 1] for i in range(board_length)]\n","    if (\n","        upward_diagonal.count(upward_diagonal[0]) == board_length\n","        and upward_diagonal[0] != \"_\"\n","    ):\n","        return upward_diagonal[0]\n","\n","    # top left to bottom right\n","    downward_diagonal = [board[i][i] for i in range(board_length)]\n","    if (\n","        downward_diagonal.count(downward_diagonal[0]) == board_length\n","        and downward_diagonal[0] != \"_\"\n","    ):\n","        return downward_diagonal[0]\n","\n","    # check for draw\n","    if all(cell != \"_\" for row in board for cell in row):\n","        return \"draw\"\n","    return None\n"]},{"cell_type":"markdown","metadata":{"id":"kJZ8xUo14Jhw"},"source":["### Creating a Model\n","\n","Now that we've defined the rules of our environment, we can create a model that will learn to play 2048. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd1dzUpe4Jhw","outputId":"1f4fe35a-ec75-4378-a413-e9b84987bb73"},"outputs":[{"name":"stdout","output_type":"stream","text":["OpenPipe client initialized\n"]}],"source":["import art\n","from dotenv import load_dotenv\n","\n","from openpipe.client import OpenPipe\n","from art.local import LocalBackend\n","\n","load_dotenv()\n","\n","op_client = OpenPipe()\n","print(\"OpenPipe client initialized\")\n","\n","random.seed(42)\n","\n","backend = LocalBackend(path=\"./.art\")"]},{"cell_type":"markdown","metadata":{"id":"8J3xGzxD4Jhx"},"source":["### Creating a Model\n","\n","Now that we've defined the rules of our environment, we can create a model that will learn to play tic tac toe. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HTIp1ua4Jhx"},"outputs":[],"source":["import os\n","\n","model = art.TrainableModel(\n","    name=\"001-script\", project=\"tic-tac-toe-local\", base_model=\"Qwen/Qwen2.5-3B-Instruct\"\n",")\n","await model.register(backend)"]},{"cell_type":"markdown","metadata":{"id":"j0A8ijFp4Jhx"},"source":["### Defining a Rollout\n","\n","<a name=\"Rollout\"></a>\n","\n","A rollout is a single episode of an agent performing its task. It generates one or more trajectories, which are lists of messages and choices.\n","\n","In this example, the rollout function generates a game of tic tac toe, and the agent plays it until the game is finished. It then returns a trajectory which contains all the `system` and `user` messages presented to the agent, as well as all the `choices` that the agent made.\n","\n","When the game is finished the `reward` for the agent's performance is calculated based on whether the agent won, lost, drew, or errored, which is then assigned to the trajectory.\n","\n","This rollout function will be called many times in parallel during each step of the training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRFE4tE-4Jhx"},"outputs":[],"source":["import art\n","import openai\n","import time\n","import math\n","from pydantic import BaseModel\n","\n","class TicTacToeScenario(BaseModel):\n","    step: int\n","\n","@art.retry(exceptions=(openai.LengthFinishReasonError,))\n","async def rollout(\n","    model: art.Model, scenario: TicTacToeScenario\n",") -> art.Trajectory:\n","    game = generate_game()\n","\n","    trajectory = art.Trajectory(\n","        messages_and_choices=[\n","            {\n","                \"role\": \"system\",\n","                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return your move as an XML object with a single property 'move', like so: <move>A1</move>. Optional moves are 'A1', 'B3', 'C2', etc. You are the {game['agent_symbol']} symbol.\",\n","            }\n","        ],\n","        reward=0,\n","    )\n","\n","    move_number = 0\n","\n","    if game[\"agent_symbol\"] == \"o\":\n","        starting_opponent_move = get_opponent_move(game)\n","        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n","            \"opponent_symbol\"\n","        ]\n","\n","    while check_winner(game[\"board\"]) is None:\n","        trajectory.messages_and_choices.append(\n","            {\"role\": \"user\", \"content\": render_board(game)}\n","        )\n","\n","        requested_at = int(time.time() * 1000)\n","        messages = trajectory.messages()\n","\n","        try:\n","            client = model.openai_client()\n","            chat_completion = await client.chat.completions.create(\n","                model=model.get_inference_name(),\n","                messages=messages,\n","                max_completion_tokens=128,\n","            )\n","            last_completion = chat_completion\n","        except openai.LengthFinishReasonError as e:\n","            raise e\n","        except Exception as e:\n","            print(\"caught exception generating chat completion\")\n","            print(e)\n","            global failing_trajectory\n","            failing_trajectory = trajectory\n","            raise e\n","\n","        try:\n","            if op_client.api_key:\n","                op_client.report(\n","                    requested_at=requested_at,\n","                    received_at=int(time.time() * 1000),\n","                    req_payload={\n","                        \"model\": model.name,\n","                        \"messages\": messages,\n","                        \"metadata\": {\n","                            \"notebook-id\": \"tic-tac-toe\",\n","                            \"step\": str(scenario.step),\n","                            \"move_number\": str(move_number),\n","                        },\n","                    },\n","                    resp_payload=chat_completion,\n","                    status_code=200,\n","                )\n","        except Exception as e:\n","            print(f\"Error reporting to OpenPipe: {e}\")\n","\n","        choice = chat_completion.choices[0]\n","        content = choice.message.content\n","        assert isinstance(content, str)\n","        trajectory.messages_and_choices.append(choice)\n","\n","        try:\n","            apply_agent_move(game, content)\n","        except ValueError:\n","            trajectory.reward = -1 + (math.log(move_number + 1) / math.log(100))\n","            break\n","\n","        if check_winner(game[\"board\"]) is not None:\n","            break\n","        move_number += 1\n","\n","        opponent_move = get_opponent_move(game)\n","        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n","\n","    winner = check_winner(game[\"board\"])\n","\n","    if winner == game[\"agent_symbol\"]:\n","        trajectory.reward = 1\n","        trajectory.metrics[\"win\"] = 1\n","    elif winner == game[\"opponent_symbol\"]:\n","        trajectory.reward = 0\n","        trajectory.metrics[\"win\"] = 0\n","    elif winner == \"draw\":\n","        trajectory.reward = 0.5\n","        trajectory.metrics[\"win\"] = 0.5\n","\n","    trajectory.metrics[\"num_moves\"] = move_number\n","\n","    try:\n","        if op_client.api_key:\n","            op_client.update_log_metadata(\n","                filters=[\n","                    {\n","                        \"field\": \"completionId\",\n","                        \"equals\": last_completion.id,\n","                    }\n","                ],\n","                metadata={\n","                    \"reward\": str(trajectory.reward),\n","                    \"reward_assigned\": \"true\",\n","                },\n","            )\n","    except Exception as e:\n","        print(f\"Error updating log metadata: {e}\")\n","\n","        print(trajectory.reward)\n","\n","    return trajectory\n"]},{"cell_type":"markdown","metadata":{"id":"-hwwe0xJ4Jhx"},"source":["<a name=\"Loop\"></a>\n","\n","### Training Loop\n","\n","The training loop is where the magic happens. For each of the 100 steps defined below, the rollout function will be called 200 times in parallel. This means that 200 games will be played at once. Each game will produce a trajectory, which will be used to update the model.\n","\n","The `gather` step will wait for all of the trajectories to be generated, then it will delete all but the most recent checkpoint and train the model on the new trajectories.\n","\n","Inference will be blocked until the training is complete.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUduqdPV4Jhy"},"outputs":[],"source":["for i in range(await model.get_step(), 100):\n","    train_groups = await art.gather_trajectory_groups(\n","        (\n","            art.TrajectoryGroup(\n","                rollout(model, TicTacToeScenario(step=i)) for _ in range(200)\n","            )\n","            for _ in range(1)\n","        ),\n","        pbar_desc=\"gather\",\n","    )\n","    await model.delete_checkpoints()\n","    await model.train(train_groups, config=art.TrainConfig(learning_rate=1e-4))"]},{"cell_type":"markdown","metadata":{"id":"dp1fZs6e4Jhy"},"source":["### Using the Model\n","\n","Just like that, you've trained an agent to play tic tac toe! Now it's time to use your model outside of ART, in the wild! The easiest way to do that is to load it from disk, where it was saved after each training step, and either run inference on it locally or upload it to a central hub like HuggingFace.\n","\n","Check out the code below for small demo of the model you just trained playing tic tac toe!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04lsj1jq4Jhy","outputId":"8d78d91e-3252-4585-d059-26fe1f4ae984"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading model from .art/tic-tac-toe-local/models/001-script/0100\n","\n","==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n","   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","\n","move 1\n","board:\n","   1   2   3\n","A  _ | _ | _\n","B  _ | _ | _\n","C  _ | _ | _\n","\n","agent move: <move>B1</move>\n","updated board:\n","   1   2   3\n","A  _ | _ | _\n","B  x | _ | _\n","C  _ | _ | _\n","\n","\n","move 3\n","board:\n","   1   2   3\n","A  _ | _ | _\n","B  x | _ | _\n","C  _ | o | _\n","\n","agent move: <move>A1</move>\n","updated board:\n","   1   2   3\n","A  x | _ | _\n","B  x | _ | _\n","C  _ | o | _\n","\n","\n","move 5\n","board:\n","   1   2   3\n","A  x | o | _\n","B  x | _ | _\n","C  _ | o | _\n","\n","agent move: <move>C1</move>\n","updated board:\n","   1   2   3\n","A  x | o | _\n","B  x | _ | _\n","C  x | o | _\n","\n","game finished in 5 moves\n","game won! 💪\n","final board:\n","\n","   1   2   3\n","A  x | o | _\n","B  x | _ | _\n","C  x | o | _\n","\n"]}],"source":["import torch\n","from unsloth import FastLanguageModel\n","\n","\n","# example: .art/tic-tac-toe/models/002/0003\n","lora_model_path = (\n","    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",")\n","\n","print(f\"loading model from {lora_model_path}\\n\")\n","\n","peft_model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=lora_model_path,\n","    max_seq_length=16384,\n","    dtype=torch.bfloat16,\n","    load_in_4bit=True,\n",")\n","FastLanguageModel.for_inference(peft_model)\n","\n","game = generate_game()\n","move_number = 0\n","\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return your move as an XML object with a single property 'move', like so: <move>A1</move>. Optional moves are 'A1', 'B3', 'C2', etc. You are the {game['agent_symbol']} symbol.\",\n","    },\n","]\n","\n","if game[\"agent_symbol\"] == \"o\":\n","    starting_opponent_move = get_opponent_move(game)\n","    game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n","        \"opponent_symbol\"\n","    ]\n","\n","while check_winner(game[\"board\"]) is None:\n","    rendered_board = render_board(game)\n","    messages.append({\"role\": \"user\", \"content\": rendered_board})\n","\n","    inputs = tokenizer.apply_chat_template(\n","        messages, return_tensors=\"pt\", add_generation_prompt=True\n","    ).to(\"cuda\")\n","\n","    content = \"\"\n","\n","    def get_completion() -> str:\n","        with torch.no_grad():\n","            outputs = peft_model.generate(\n","                input_ids=inputs,\n","                max_new_tokens=100,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","            )\n","            return tokenizer.decode(\n","                outputs[0][inputs.shape[1] :], skip_special_tokens=True\n","            )\n","\n","    try:\n","        content = get_completion()\n","    except Exception as e:\n","        print(\"caught exception generating chat completion\", e)\n","        raise e\n","\n","    messages.append({\"role\": \"assistant\", \"content\": content})\n","\n","    try:\n","        apply_agent_move(game, content)\n","        move_number += 1\n","    except ValueError:\n","        raise ValueError(f\"Invalid move on move {move_number}: {content}\")\n","\n","    # print the board every move\n","    print(f\"\\nmove {move_number}\")\n","    print(f\"board:\\n{rendered_board}\")\n","    print(f\"agent move: {content}\")\n","    print(f\"updated board:\\n{render_board(game)}\")\n","\n","    if check_winner(game[\"board\"]) is not None:\n","        break\n","    move_number += 1\n","\n","    opponent_move = get_opponent_move(game)\n","    game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n","\n","winner = check_winner(game[\"board\"])\n","\n","print(f\"game finished in {move_number} moves\")\n","\n","if winner == game[\"agent_symbol\"]:\n","    print(\"game won! 💪\")\n","elif winner == game[\"opponent_symbol\"]:\n","    print(\"game lost! 😢\")\n","elif winner == \"draw\":\n","    print(\"draw! 🤷‍♂️\")\n","\n","\n","print(f\"final board:\\n\\n{render_board(game)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"QGqky1sz4Jhy"},"source":["<div class=\"align-center\">\n","<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/ART_pill.png\" height=\"50\"></a>\n","<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/Discord_pill.png\" height=\"50\"></a>\n","<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n","\n","Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n","\n","</div>\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"https://github.com/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb","timestamp":1746573283608}]}},"nbformat":4,"nbformat_minor":0}