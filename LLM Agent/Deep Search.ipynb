{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1mISXVRx2Td0mxPxl27QqNP_ypRaiEZCA","authorship_tag":"ABX9TyOQeqER1/LitIIRx04VfUW1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"pKTBSZFCd8g9"}},{"cell_type":"code","source":["%%bash\n","pip install  --upgrade\\\n","    'vllm>=0.8.2' \\\n","    'transformers>=4.50.3' \\\n","    pyzmq \\\n","    unsloth \\\n","    accelerate \\\n","    bitsandbytes \\\n","    openai \\\n","    langchain-text-splitters \\\n","    peft \\\n","    FlagEmbedding \\\n","    datasets \\\n","    faiss-cpu \\\n","    langchain-text-splitters \\\n","    tavily-python \\\n","    \"flashinfer-python>=0.2.4\"  --extra-index-url https://flashinfer.ai/whl/cu124/torch2.6/\n","git clone https://github.com/ggml-org/llama.cpp.git\n","cd llama.cpp/gguf-py/ && pip install --editable .\n","pip install jupyter-kernel-gateway ipykernel\n","pip install --upgrade --no-deps numpy==1.26.4 pandas==2.2.2"],"metadata":{"id":"cxg_aTF0B-DJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744440340660,"user_tz":-540,"elapsed":20123,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"ae939ef3-734b-42f8-f22a-6be57f347467"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://flashinfer.ai/whl/cu124/torch2.6/\n","Requirement already satisfied: vllm>=0.8.2 in /usr/local/lib/python3.11/dist-packages (0.8.3)\n","Requirement already satisfied: transformers>=4.50.3 in /usr/local/lib/python3.11/dist-packages (4.51.2)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (26.4.0)\n","Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.19)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n","Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.1)\n","Requirement already satisfied: FlagEmbedding in /usr/local/lib/python3.11/dist-packages (1.3.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Collecting tavily-python\n","  Downloading tavily_python-0.5.4-py3-none-any.whl.metadata (91 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91.6/91.6 kB 6.4 MB/s eta 0:00:00\n","Requirement already satisfied: flashinfer-python>=0.2.4 in /usr/local/lib/python3.11/dist-packages (0.2.5+cu124torch2.6)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (5.5.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (5.9.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.26.4)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (4.67.1)\n","Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.0.4)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (9.0.0)\n","Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm>=0.8.2) (0.30.1)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.21.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (3.20.3)\n","Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.115.12)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (3.11.15)\n","Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (2.11.2)\n","Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.21.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (11.1.0)\n","Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (7.1.0)\n","Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.9.0)\n","Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.10.11)\n","Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.7.14)\n","Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.1.11)\n","Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.2.2)\n","Requirement already satisfied: xgrammar==0.1.17 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.1.17)\n","Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (4.13.1)\n","Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (3.18.0)\n","Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.2.1.1.post5)\n","Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.19.0)\n","Collecting gguf==0.10.0 (from vllm>=0.8.2)\n","  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (8.6.1)\n","Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm>=0.8.2) (1.5.4)\n","Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (4.11.0.86)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (6.0.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.8.1)\n","Requirement already satisfied: compressed-tensors==0.9.2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.9.2)\n","Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.18.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (3.1.1)\n","Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.0.5)\n","Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (3.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.14.1)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (1.11.1.4)\n","Requirement already satisfied: numba==0.61 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.61.0)\n","Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.2) (2.43.0)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (2.6.0+cu124)\n","Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (2.6.0+cu124)\n","Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.21.0+cu124)\n","Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.11/dist-packages (from vllm>=0.8.2) (0.0.29.post2)\n","Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm>=0.8.2) (0.8.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm>=0.8.2) (0.3.8)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61->vllm>=0.8.2) (0.44.0)\n","Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (0.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (3.1.6)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (1.6.0)\n","Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (5.6.3)\n","Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (0.36.2)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (4.23.0)\n","Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (24.6.1)\n","Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (20250224)\n","Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm>=0.8.2) (0.1.26)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (3.4.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm>=0.8.2) (1.13.1)\n","Requirement already satisfied: nanobind>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.17->vllm>=0.8.2) (2.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm>=0.8.2) (1.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.3) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.3) (2024.11.6)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.3) (0.5.3)\n","Requirement already satisfied: unsloth_zoo>=2025.3.17 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.17)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.18)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n","Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters) (0.3.51)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (3.4.1)\n","Requirement already satisfied: ir-datasets in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (0.5.10)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.46.1)\n","Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.0.7)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.0.20)\n","Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm>=0.8.2) (2.2.0)\n","Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.34.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (6.3.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm>=0.8.2) (1.18.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm>=0.8.2) (1.0.3)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.3.24)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (9.1.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.33)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm>=0.8.2) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm>=0.8.2) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm>=0.8.2) (0.4.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.2) (8.1.8)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.2) (1.1.0)\n","Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.2) (13.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm>=0.8.2) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm>=0.8.2) (2.3.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (25.1.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm>=0.8.2) (3.21.0)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (4.13.3)\n","Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (2.6.0)\n","Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (5.3.1)\n","Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (2.6)\n","Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (4.4.4)\n","Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n","Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n","Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (0.1.9)\n","Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (3.3.0)\n","Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (0.2.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->FlagEmbedding) (1.6.1)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.6)\n","Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm>=0.8.2) (2.7.0)\n","Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.15.2)\n","Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.14.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm>=0.8.2) (3.0.2)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.0.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm>=0.8.2) (2024.10.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm>=0.8.2) (0.24.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n","Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding) (1.0.0)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (1.1.0)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (0.21.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (15.0.1)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm>=0.8.2) (0.8.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (3.6.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm>=0.8.2) (1.5.4)\n","Using cached gguf-0.10.0-py3-none-any.whl (71 kB)\n","Downloading tavily_python-0.5.4-py3-none-any.whl (44 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 kB 3.3 MB/s eta 0:00:00\n","Installing collected packages: gguf, tavily-python\n","  Attempting uninstall: gguf\n","    Found existing installation: gguf 0.16.0\n","    Uninstalling gguf-0.16.0:\n","      Successfully uninstalled gguf-0.16.0\n","Successfully installed gguf-0.10.0 tavily-python-0.5.4\n","Obtaining file:///content/llama.cpp/gguf-py\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Checking if build backend supports build_editable: started\n","  Checking if build backend supports build_editable: finished with status 'done'\n","  Getting requirements to build editable: started\n","  Getting requirements to build editable: finished with status 'done'\n","  Preparing editable metadata (pyproject.toml): started\n","  Preparing editable metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (1.26.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (6.0.2)\n","Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (0.2.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from gguf==0.16.0) (4.67.1)\n","Building wheels for collected packages: gguf\n","  Building editable for gguf (pyproject.toml): started\n","  Building editable for gguf (pyproject.toml): finished with status 'done'\n","  Created wheel for gguf: filename=gguf-0.16.0-py3-none-any.whl size=3460 sha256=229fa87e0fcf06fd475f2202b6660b9177b6c0a0c1f3259ed622aedb06294ec4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mm5n15dl/wheels/a1/6c/c6/6dbfb804e7a1607174676026fc9bf5d1006ceff85ba5c680b6\n","Successfully built gguf\n","Installing collected packages: gguf\n","  Attempting uninstall: gguf\n","    Found existing installation: gguf 0.10.0\n","    Uninstalling gguf-0.10.0:\n","      Successfully uninstalled gguf-0.10.0\n","Successfully installed gguf-0.16.0\n","Requirement already satisfied: jupyter-kernel-gateway in /usr/local/lib/python3.11/dist-packages (3.0.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (6.17.1)\n","Requirement already satisfied: jupyter-client>=8.6 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (8.6.3)\n","Requirement already satisfied: jupyter-core>=5.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (5.7.2)\n","Requirement already satisfied: jupyter-server>=2.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (2.15.0)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (2.32.3)\n","Requirement already satisfied: tornado>=6.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (6.4.2)\n","Requirement already satisfied: traitlets>=5.14.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-kernel-gateway) (5.14.3)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.8.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.9.5)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (26.4.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=8.6->jupyter-kernel-gateway) (2.8.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=5.7->jupyter-kernel-gateway) (4.3.7)\n","Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (4.9.0)\n","Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (23.1.0)\n","Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (3.1.6)\n","Requirement already satisfied: jupyter-events>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (0.12.0)\n","Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (0.5.3)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (7.16.6)\n","Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (5.10.4)\n","Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (7.7.0)\n","Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (0.21.1)\n","Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (0.18.1)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server>=2.12->jupyter-kernel-gateway) (1.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyter-kernel-gateway) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyter-kernel-gateway) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyter-kernel-gateway) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyter-kernel-gateway) (2025.1.31)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server>=2.12->jupyter-kernel-gateway) (1.3.1)\n","Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server>=2.12->jupyter-kernel-gateway) (4.13.1)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server>=2.12->jupyter-kernel-gateway) (21.2.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyter-server>=2.12->jupyter-kernel-gateway) (3.0.2)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (4.23.0)\n","Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (3.3.0)\n","Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (6.0.2)\n","Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (0.36.2)\n","Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (0.1.4)\n","Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (0.1.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (4.13.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (3.1.3)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (0.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (1.5.1)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server>=2.12->jupyter-kernel-gateway) (2.21.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=8.6->jupyter-kernel-gateway) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (1.4.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (2024.10.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (0.24.0)\n","Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (1.5.1)\n","Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (3.0.0)\n","Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (1.3.0)\n","Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (24.11.1)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.12->jupyter-kernel-gateway) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=2.12->jupyter-kernel-gateway) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.12->jupyter-kernel-gateway) (2.22)\n","Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (1.3.0)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.12->jupyter-kernel-gateway) (2.9.0.20241206)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n"]},{"output_type":"stream","name":"stderr","text":["fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","vllm 0.8.3 requires gguf==0.10.0, but you have gguf 0.16.0 which is incompatible.\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_WRITE_TOKEN')\n","!huggingface-cli login --add-to-git-credential --token $HF_TOKEN\n","os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEz4lxmqeEih","executionInfo":{"status":"ok","timestamp":1744438770112,"user_tz":-540,"elapsed":1767,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"ee7e40e3-7162-419b-b68d-f1b832d046eb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Token is valid (permission: write).\n","The token `WriteToken` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"]}]},{"cell_type":"markdown","metadata":{"id":"_9QlQdVmA2TQ"},"source":["```bash\n","VLLM_BACKEND=FLASHINFER VLLM_USE_V1=1 VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 TOKENIZERS_PARALLELISM=true MAX_JOBS=2 vllm serve ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g --port 8877 --max-model-len 4096 --api-key token-abc123 --quantization compressed-tensors --max-num-seqs=1\n","```"]},{"cell_type":"markdown","source":["# Web Search"],"metadata":{"id":"D3vY4k59SJPU"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"088a415c-d647-4929-a8e1-53b459e8d9b5","executionInfo":{"status":"ok","timestamp":1744440340916,"user_tz":-540,"elapsed":252,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from tavily import TavilyClient\n","import asyncio, os, requests, time, json\n","from IPython.display import display, Markdown, Latex\n","\n","tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"2d41e290-4bc3-4eab-aa2a-5b7aba37f183","executionInfo":{"status":"ok","timestamp":1744442902232,"user_tz":-540,"elapsed":67,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from openai import OpenAI\n","import math\n","import time\n","import json\n","\n","client = OpenAI(\n","    base_url=\"http://localhost:8877/v1\",\n","    api_key=\"token-abc123\",\n",")"]},{"cell_type":"code","execution_count":188,"metadata":{"id":"2bf367e2-79bc-4af2-9914-c71bcea0a7e9","executionInfo":{"status":"ok","timestamp":1744444839335,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n","     # Collect all results\n","    sources_list = []\n","    for response in search_response:\n","        sources_list.extend(response['results'])\n","\n","    # Deduplicate by URL\n","    unique_sources = {source['url']: source for source in sources_list}\n","\n","    # Format output\n","    formatted_text = \"Content from sources:\\n\"\n","    for i, source in enumerate(unique_sources.values(), 1):\n","        formatted_text += f\"{'='*80}\\n\"  # Clear section separator\n","        formatted_text += f\"Source: {source['title']}\\n\"\n","        formatted_text += f\"{'-'*80}\\n\"  # Subsection separator\n","        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n","        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n","        if include_raw_content:\n","            # Using rough estimate of 4 characters per token\n","            char_limit = max_tokens_per_source * 4\n","            # Handle None raw_content\n","            raw_content = source.get('raw_content', '')\n","            if raw_content is None:\n","                raw_content = ''\n","                print(f\"Warning: No raw_content found for source {source['url']}\")\n","            if len(raw_content) > char_limit:\n","                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n","            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n","        formatted_text += f\"{'='*80}\\n\\n\" # End section separator\n","\n","    return formatted_text.strip()"]},{"cell_type":"code","execution_count":189,"metadata":{"id":"2440775f-4d4f-481e-9a58-a83f1984bb9e","executionInfo":{"status":"ok","timestamp":1744444840085,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def generate_response(message_list):\n","    completion = client.chat.completions.create(\n","        model = \"ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\",\n","        messages = message_list,\n","        max_tokens=2048,\n","        frequency_penalty=0.3,\n","        temperature=0.6,\n","        stream=True,\n","    )\n","\n","    final_answer = []\n","    assistant_response = \"\"\n","\n","    start = time.time()\n","\n","    # 스트림 모드에서는 completion.choices 를 반복문으로 순회\n","    for chunk in completion:\n","        chunk_content = chunk.choices[0].delta.content\n","\n","        if isinstance(chunk_content, str):\n","            final_answer.append(chunk_content)\n","            # 토큰 단위로 실시간 답변 출력\n","            print(chunk_content, end=\"\")\n","            assistant_response += chunk_content\n","\n","    end = time.time()\n","    print(f\"\\n\\ninference time: {end - start:.5f} sec \\n\\n\")\n","    return assistant_response"]},{"cell_type":"code","execution_count":190,"metadata":{"id":"bc6e7991-8b1f-4e08-9a8e-39402ad5966f","executionInfo":{"status":"ok","timestamp":1744444840304,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["import threading\n","\n","def worker(query, search_result, req_num_result, include_raw, req_topic):\n","    print(f\"Thread: {query}\")\n","    search_result.append(\n","        tavily_client.search(\n","            query,\n","            max_results= req_num_result,\n","            include_raw_content= include_raw,\n","            topic= req_topic\n","        )\n","    )"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"37f4b17b-86e5-45ab-9885-89404c578304","executionInfo":{"status":"ok","timestamp":1744444840579,"user_tz":-540,"elapsed":4,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def ask_tavily(search_queries, search_tasks, req_num_result, include_raw, req_topic):\n","    start_time = time.time()\n","    threads = []\n","\n","    for query in search_queries:\n","        t = threading.Thread(target=worker, args=(query, search_tasks, req_num_result, include_raw, req_topic))\n","        threads.append(t)\n","        t.start()\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    end_time = time.time()\n","    execution_time = end_time - start_time\n","\n","    print(f\"\\nask_tavily task running time: {execution_time:.2f}초 \\n\")"]},{"cell_type":"code","execution_count":192,"metadata":{"id":"9bc19754-1bb4-44fa-bd34-40781b75e958","executionInfo":{"status":"ok","timestamp":1744444840854,"user_tz":-540,"elapsed":7,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def ask_plan_query_writer(topic, content):\n","    llm_prompt = \"\"\"You are an expert technical writer crafting a section that synthesizes information\n","<section topic>\n","\"\"\" + topic + \"\"\"\n","</section topic>\n","\n","<section organization>\n","\"\"\" + content + \"\"\"\n","</section organization>\n","\n","<Task>\n","Your goal is to generate 3 web search queries that will help gather information for planning the sections.\n","\n","The queries should:\n","\n","1. Be related to the section topic\n","2. Help satisfy the requirements specified in the section organization\n","\n","Make the queries specific enough to find high-quality, relevant sources while covering the breadth needed for the section structure.\n","\n","Note1. that today's date is \"\"\"+time.strftime(\"%Y-%m-%d\")+\"\"\".\n","Note2. Output your response in JSON format, with the following structure: { \"queries\": [ \"query1\", \"query2\", \"query3\" ] }\n","</Task>\"\"\"\n","\n","    return llm_prompt"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"fea43e2f-f69f-4481-86be-031acbfda0d8","executionInfo":{"status":"ok","timestamp":1744444841098,"user_tz":-540,"elapsed":27,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def ask_final_writer_instructions(topic, content, search_tasks):\n","    final_section_writer=\"\"\"You are an expert technical writer.\n","\n","<Section name>\n","\"\"\" + content + \"\"\"\n","</Section name>\n","\n","<Section topic>\n","\"\"\" + topic + \"\"\"\n","</Section topic>\n","\n","<Available Website Search Content>\n","\"\"\" + deduplicate_and_format_sources(search_tasks, max_tokens_per_source=4000, include_raw_content=True) + \"\"\"\n","</Available Website Search Content>\n","\n","<Task>\n","1. Section-Specific Approach:\n","\n","For Introduction:\n","- Use # for Website Search title (Markdown format)\n","- Write in simple and clear language\n","- Focus on the core motivation for the Section in 1-2 paragraphs\n","- Use a clear narrative arc to introduce the Section\n","- Include NO structural elements (no lists or tables)\n","- No sources section needed\n","\n","For Conclusion/Summary:\n","- Use ## for Conclusion/Summary title (Markdown format)\n","- For comparative Conclusion/Summary:\n","    * Must include a focused comparison table using Markdown table syntax\n","    * Table should distill insights from the Section\n","    * Keep table entries clear and concise\n","- For non-comparative Conclusion/Summary:\n","    * Only use ONE structural element IF it helps distill the points made in the Section:\n","    * Either a focused table comparing items present in the Section (using Markdown table syntax)\n","    * Or a short list using proper Markdown list syntax:\n","      - Use `*` or `-` for unordered lists\n","      - Use `1.` for ordered lists\n","      - Ensure proper indentation and spacing\n","- Sources and url section needed. (especially when expressing a URL, please provide the entire URL exactly as given in the content without abbreviating it.)\n","- End with specific next steps or implications\n","\n","2. Writing Approach:\n","- Use concrete details over general statements\n","- Make every word count\n","- Focus on your single most important point\n","</Task>\n","\n","<Quality Checks>\n","- Verify that EVERY claim is grounded in the provided Source material\n","- Confirm each URL appears ONLY ONCE in the Source list\n","- For introduction: # for Website Search title, no structural elements, no sources section\n","- For conclusion: ## for Conclusion/Summary title, only ONE structural element at most, add sources and url section\n","- Markdown format\n","- Do not include word count or any preamble in your response\n","</Quality Checks>\n","\n","Please note that respond in Korean always.\"\"\"\n","\n","    return final_section_writer"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"31b7c569-67dd-450b-8657-b8f7197bd980","outputId":"00a3e641-2b87-4ab3-9d8d-c2102eb473a4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744444848347,"user_tz":-540,"elapsed":6904,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","{\n","  \"queries\": [\n","    \"MCP 프로토콜 A2A 에이전트 비교 분석 2024-2025\",\n","    \"모델 컨텍스트 프로토콜(MCP) 기술 동향 및 활용 사례\",\n","    \"A2A (Agent to Agent) 통신 프로토콜 최신 동향 및 MCP 와의 차이점\"\n","  ]\n","}\n","```\n","\n","inference time: 6.89265 sec \n","\n","\n"]}],"source":["system_prompt = \"You are a helpful assistant. And Answers must be in Korean.\"\n","\n","topic = \"기술동향\"\n","content = \"MCP(model context protocol) 과 A2A(Agent to Agent) 는 어떤 차이가 있는것인지 알려줘.\"\n","\n","messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": ask_plan_query_writer(topic, content)},\n","    ]\n","\n","response_query = generate_response(messages)"]},{"cell_type":"code","execution_count":195,"metadata":{"id":"6dbc2f92-e585-4fcc-b959-a2265785db2c","outputId":"9b2f4f6a-b9cd-4df1-99f2-34377c5ca8bd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744444851326,"user_tz":-540,"elapsed":2976,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["사용자 발화 기반으로 추출한 web query 문장 3건:\n","['MCP 프로토콜 A2A 에이전트 비교 분석 2024-2025', '모델 컨텍스트 프로토콜(MCP) 기술 동향 및 활용 사례', 'A2A (Agent to Agent) 통신 프로토콜 최신 동향 및 MCP 와의 차이점']\n","Thread: MCP 프로토콜 A2A 에이전트 비교 분석 2024-2025\n","Thread: 모델 컨텍스트 프로토콜(MCP) 기술 동향 및 활용 사례\n","Thread: A2A (Agent to Agent) 통신 프로토콜 최신 동향 및 MCP 와의 차이점\n","\n","ask_tavily task running time: 2.91초 \n","\n","[{'query': 'MCP 프로토콜 A2A 에이전트 비교 분석 2024-2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Mcp 시장 지도 완벽 분석: 2025년 모델 컨텍스트 프로토콜 생태계 총정리', 'url': 'https://dma-ai.kr/81', 'content': 'MCP(Model Context Protocol)는 AI 모델과 에이전트가 다양한 도구 및 서비스와 원활하게 상호작용할 수 있도록 하는 표준화된 프로토콜입니다. 1. Top MCP Clients (주요 MCP 클라이언트) 2. Top MCP Servers (주요 MCP 서버) 특징: MCP 애플리케이션의 자원 관리 및 최적화를 위한 도구입니다. 특징: MCP 생태계 내의 통합 및 커뮤니케이션을 관리하는 도구입니다. 주요 MCP 생태계 참여자 클라이언트 | Cursor | https://www.cursor.com | AI 기반 코드 에디터 | Anthropic | https://www.anthropic.com | AI 안전 및 연구 회사 | OpenTools | https://opentools.com | AI 도구 디렉토리 MCP 시장 동향 및 전망 주요 MCP 클라이언트 주요 MCP 서버 주요 MCP 클라이언트, 서버, 마켓플레이스 및 도구들의 총정리와 미래 전망 제시. 순차적 사고(Sequential Thinking) MCP 서버 설치 및 활용 가이드(0) | 2025.03.25', 'score': 0.7036111, 'raw_content': None}, {'title': '[A2a] Ai 에이전트의 공용 언어, A2a가 여는 협업의 미래', 'url': 'https://infogalaxy.co.kr/entry/A2A-Agent2agent-AI-에이전트', 'content': \"기존에 있던 MCP가 AI 모델 간 맥락 공유에 집중했다면, A2A는 실제 '행동 단위'의 상호작용에 초점을 맞춘다. 이 개념은 2024년 구글이 발표한 공식 기술 제안에서 처음 공개되었으며, 이를 통해 구글은 에이전트 기술의 다음 단계로 ' 상호운용성 '을 지목하고 있다.\", 'score': 0.54346967, 'raw_content': None}, {'title': 'A2A vs MCP: 새로운 에이전트 생태계를 위한 두 개의 보완적 프로토콜 · Logto 블로그', 'url': 'https://blog.logto.io/ko/a2a-mcp', 'content': 'MCP 호스트는 AI 앱 자체로 생각할 수 있습니다 — Claude Desktop이나 코딩 도우미와 같은. 적절한 MCP 서버 세트를 통해, 사용자는 모든 MCP 클라이언트를 \"모든 것을 할 수 있는 앱\"으로 변환할 수 있습니다. 개발자 역할 | 엔드포인트를 통해 작업 및 아티팩트를 노출하는 에이전트 구축 | 모델이 사용할 수 있는 구조화된 도구와 컨텍스트 정의 함께 사용될 때, 이는 구성 가능한 다중 에이전트 시스템을 지원하여, 확장 가능하고 상호 운용할 수 있습니다. MCP + A2A 기반 인프라가 에이전트 제품 마켓플레이스의 미래를 어떻게 형성할 수 있는가# 우리는 점점 더 많은 MCP 서버가 등장하는 것을 확인할 수 있습니다. 에이전트(예: MCP 클라이언트로)가 이러한 서버 여러 개를 한 번에 연결할 수 있다면 — 이전에는 맞춤형 통합이나 밀접\\x00하게 연결된 앱이 필요했던 워크플로를 잠금 해제할 수 있습니다.', 'score': 0.5417246, 'raw_content': None}], 'response_time': 0.86}, {'query': 'A2A (Agent to Agent) 통신 프로토콜 최신 동향 및 MCP 와의 차이점', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'A2A vs MCP: 새로운 에이전트 생태계를 위한 두 개의 보완적 프로토콜 · Logto 블로그', 'url': 'https://blog.logto.io/ko/a2a-mcp', 'content': 'MCP 호스트는 AI 앱 자체로 생각할 수 있습니다 — Claude Desktop이나 코딩 도우미와 같은. 적절한 MCP 서버 세트를 통해, 사용자는 모든 MCP 클라이언트를 \"모든 것을 할 수 있는 앱\"으로 변환할 수 있습니다. 개발자 역할 | 엔드포인트를 통해 작업 및 아티팩트를 노출하는 에이전트 구축 | 모델이 사용할 수 있는 구조화된 도구와 컨텍스트 정의 함께 사용될 때, 이는 구성 가능한 다중 에이전트 시스템을 지원하여, 확장 가능하고 상호 운용할 수 있습니다. MCP + A2A 기반 인프라가 에이전트 제품 마켓플레이스의 미래를 어떻게 형성할 수 있는가# 우리는 점점 더 많은 MCP 서버가 등장하는 것을 확인할 수 있습니다. 에이전트(예: MCP 클라이언트로)가 이러한 서버 여러 개를 한 번에 연결할 수 있다면 — 이전에는 맞춤형 통합이나 밀접\\x00하게 연결된 앱이 필요했던 워크플로를 잠금 해제할 수 있습니다.', 'score': 0.7062922, 'raw_content': None}, {'title': '구글의 A2a, Ai 에이전트 시대의 새로운 표준이 될까? - Mcp와의 비교부터 실제 사용 예시까지 한눈에 정리', 'url': 'https://digitalbourgeois.tistory.com/1039', 'content': '구글의 A2A, AI 에이전트 시대의 새로운 표준이 될까? 구글의 A2A, AI 에이전트 시대의 새로운 표준이 될까? 🛠 실제 사용 예시 – A2A 에이전트 체험기 주 용도에이전트 간 협업LLM 기능 확장 및 도구 연결아키텍처클라이언트-서버호스트-LLM-리소스인터페이스JSON, Agent CardJSON-RPC 2.0, 리소스/툴 구성통신 방식HTTP, JSON-RPCstdio 기반 JSON-RPC, SSE특징다중 에이전트 관리, 태스크 중심, 멀티모달 지원모듈성, 툴 재사용, 캐싱생태계구글 중심, 초기 단계이미 대규모 도입 진행 중 멀티 에이전트 기반 AI 시스템이 점점 확산되는 지금, A2A는 이를 구축하는 데 있어 실용적인 대안이 될 수 있습니다. https://hackernoon.com/google-a2a-a-first-look-at-another-agent-agent-protocol Google A2A - a First Look at Another Agent-agent Protocol | HackerNoon Google A2A - a first look at another agent-agent protocol and compared to Anthropic’s MCP. “에이전트가 개발을 대신해준다?” Google Cloud Next ‘25, 개발의 미래를 엿보다\\xa0\\xa0(0)2025.04.11“AI의 판을 다시 짠다” – 구글 클라우드 Next 25에서 공개된 차세대 AI 기술 총정리\\xa0\\xa0(0)2025.04.11딥시크-R1을 넘어선 새로운 강자?', 'score': 0.6780931, 'raw_content': None}, {'title': 'Meet Google A2A: The Protocol That will Revolutionize Multi-Agent AI ...', 'url': 'https://medium.com/@the_manoj_desai/meet-google-a2a-the-protocol-that-will-revolutionize-multi-agent-ai-systems-80d55a4583ed', 'content': 'Meet Google A2A: The Protocol That will Revolutionize Multi-Agent AI Systems | by Manoj Desai | Apr, 2025 | Medium Meet Google A2A: The Protocol That will Revolutionize Multi-Agent AI Systems I had five different AI services that needed to talk to each other — an OpenAI agent handling user queries, a vector search service retrieving documents, a tool-using agent running calculations, a Claude-powered summarizer, and a custom Python agent with domain-specific logic. Before we dive into Python A2A, let me show you what a difference standardized agent communication makes with a real-world example. print(f\"Agent says: {response.content.text}\") Python A2A gives you a simple, production-ready way to implement Google’s A2A protocol and start building more modular, interoperable agent systems today.', 'score': 0.30191797, 'raw_content': None}], 'response_time': 1.6}, {'query': '모델 컨텍스트 프로토콜(MCP) 기술 동향 및 활용 사례', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'AI 실시간 통합을 위한 핵심 기술, MCP(Model Context Protocol) 완벽 가이드', 'url': 'https://the-see.tistory.com/195', 'content': '양방향 연결,\\xa0보안 통신,\\xa0다양한 도구 통합\\xa0등의 특징을 갖추고 있어, AI 활용의 지평을 넓히는 핵심 기술로 주목받고 있습니다. 또한, 양방향 통신 구조를 통해 AI 모델이 데이터 소스와 실시간으로 정보를 주고받으며 컨텍스트를 유지할 수 있습니다. API, 파일 시스템 등 다양한 소스를 단일 프로토콜로 통합할 수 있는 범용성과, HTTPS/TLS 기반의 보안 구조를 통한 안전한 정보 전달 역시 MCP의 강력한 특징 중 하나입니다. 이처럼 MCP는 업무 자동화뿐 아니라, AI 비서, 코드 편집기, 일정 관리 시스템 등 다양한 분야에서 그 가능성을 입증하고 있습니다. MCP(Model Context Protocol)는 AI 모델의 생산성과 실시간 응답 능력을 극대화할 수 있는 개방형 통합 프로토콜입니다. 기존 API 방식보다 통합과 확장성이 뛰어나며, 다양한 도구와 시스템에 쉽게 적용할 수 있습니다. 실시간 데이터 연동, 업무 자동화, 동적 도구 탐색 등 다양한 기능을 활용해, 귀사의 AI 프로젝트 성능을 획기적으로 개선해보세요.', 'score': 0.7931224, 'raw_content': None}, {'title': 'Mcp 시장 지도 완벽 분석: 2025년 모델 컨텍스트 프로토콜 생태계 총정리', 'url': 'https://dma-ai.kr/81', 'content': 'MCP(Model Context Protocol)는 AI 모델과 에이전트가 다양한 도구 및 서비스와 원활하게 상호작용할 수 있도록 하는 표준화된 프로토콜입니다. 1. Top MCP Clients (주요 MCP 클라이언트) 2. Top MCP Servers (주요 MCP 서버) 특징: MCP 애플리케이션의 자원 관리 및 최적화를 위한 도구입니다. 특징: MCP 생태계 내의 통합 및 커뮤니케이션을 관리하는 도구입니다. 주요 MCP 생태계 참여자 클라이언트 | Cursor | https://www.cursor.com | AI 기반 코드 에디터 | Anthropic | https://www.anthropic.com | AI 안전 및 연구 회사 | OpenTools | https://opentools.com | AI 도구 디렉토리 MCP 시장 동향 및 전망 주요 MCP 클라이언트 주요 MCP 서버 주요 MCP 클라이언트, 서버, 마켓플레이스 및 도구들의 총정리와 미래 전망 제시. 순차적 사고(Sequential Thinking) MCP 서버 설치 및 활용 가이드(0) | 2025.03.25', 'score': 0.78512776, 'raw_content': None}, {'title': 'Model Context Protocol(MCP) 완벽 가이드: AI 애플리케이션 개발 표준화하기', 'url': 'https://dma-ai.kr/77', 'content': 'MCP 호스트사용자가 상호작용하는 프론트엔드 애플리케이션 (Claude Desktop, IDE, AI 도구 등)MCP 클라이언트MCP 서버와 1:1 연결을 유지하며 통신을 중개하는 역할MCP 서버특정 기능을 노출하는 경량 프로그램으로, 애플리케이션과 LLM 간 연결 담당로컬 데이터 소스사용자의 컴퓨터에 있는 파일, 데이터베이스, 서비스원격 서비스인터넷을 통해 접근 가능한 외부 시스템 (API 등) Claude.app 및 claude.ai MCP 통합과 관련된 문제는 웹사이트에 제공된 연락처 정보를 통해 지원받을 수 있습니다. 블렌더-mcp 사용 가이드: Claude AI와 블렌더 연동으로 3D 제작 혁신하기\\xa0\\xa0(0)2025.03.21Claude 데스크톱 앱을 위한 MCP-인스톨러 완벽 가이드: 설치부터 문제해결까지\\xa0\\xa0(0)2025.03.21【완벽 가이드】Claude for Desktop에서 파일 시스템 접근 기능 설정하기\\xa0\\xa0(1)2025.03.20Unreal Engine 5와 AI 통합 가이드: MCP 서버 활용법\\xa0\\xa0(2)2025.03.20', 'score': 0.5911811, 'raw_content': None}], 'response_time': 2.14}]\n"]}],"source":["if \"```json\" in response_query:\n","    response_query = response_query.split(\"```json\")[1].strip()\n","    response_query = response_query.split(\"```\")[0].strip()\n","json_data = json.loads(response_query)\n","queries = json_data['queries']\n","\n","print(\"사용자 발화 기반으로 추출한 web query 문장 3건:\")\n","print(queries)\n","\n","search_tasks = []\n","req_topic = 'general' # news   gerneral 과 news 중 선택\n","req_num_result = 3    # 각 web query 에 대해 리턴할 site 개수\n","include_raw = False    # site 의 원본 컨텐츠 리턴 유무\n","\n","ask_tavily(queries, search_tasks, req_num_result, include_raw, req_topic)\n","print(search_tasks)"]},{"cell_type":"code","execution_count":196,"metadata":{"id":"c6543581-a4e6-4d00-af86-71ae7a0f92ba","outputId":"cb9d10d5-57eb-4585-83f5-94012adb453d","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1744444927751,"user_tz":-540,"elapsed":76423,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","=================================================================\n","\n","Warning: No raw_content found for source https://dma-ai.kr/81\n","Warning: No raw_content found for source https://infogalaxy.co.kr/entry/A2A-Agent2agent-AI-에이전트\n","Warning: No raw_content found for source https://blog.logto.io/ko/a2a-mcp\n","Warning: No raw_content found for source https://digitalbourgeois.tistory.com/1039\n","Warning: No raw_content found for source https://medium.com/@the_manoj_desai/meet-google-a2a-the-protocol-that-will-revolutionize-multi-agent-ai-systems-80d55a4583ed\n","Warning: No raw_content found for source https://the-see.tistory.com/195\n","Warning: No raw_content found for source https://dma-ai.kr/77\n","# MCP와 A2A: 차세대 AI 에이전트 생태계를 위한 핵심 프로토콜\n","\n","최근 AI 기술 발전과 함께 에이전트 간의 협업 및 정보 교환의 중요성이 부각되고 있습니다. 이러한 요구에 발맞춰 MCP(Model Context Protocol)와 A2A(Agent to Agent)라는 두 가지 프로토콜이 등장하며 AI 에이전트 생태계를 혁신하고 있습니다. MCP는 AI 모델과 도구 간의 연결을 표준화하여 효율적인 상호작용을 가능하게 하는 반면, A2A는 에이전트들이 직접 '행동 단위'로 상호작용할 수 있는 환경을 제공합니다. 본 섹션에서는 MCP와 A2A의 차이점을 심층적으로 분석하고, 각 프로토콜의 특징과 미래 전망을 살펴봅니다.\n","\n","## 결론/요약\n","\n","MCP와 A2A는 모두 AI 에이전트 생태계 발전에 기여하는 중요한 프로토콜이지만, 접근 방식과 목표에서 뚜렷한 차이를 보입니다. MCP는 AI 모델과 도구 간의 맥락 공유 및 자원 관리에 초점을 맞추는 반면, A2A는 에이전트 간의 직접적인 협업과 상호운용성을 강조합니다.\n","\n","| 특징 | MCP (Model Context Protocol) | A2A (Agent to Agent) |\n","|---|---|---|\n","| **초점** | 맥락 공유, 자원 관리 | 에이전트 간 협업, 상호운용성 |\n","| **통신 방식** | 클라이언트-서버 모델 | 다중 에이전트 관리, 태스크 중심 |\n","| **주요 특징** | 통합 및 커뮤니케이션 관리 | 모듈성, 툴 재사용, 캐싱 |\n","| **생태계** | 다양한 클라이언트와 서버 존재 | 구글 중심, 초기 단계 |\n","| **활용 분야** | 코드 에디터, AI 안전 및 연구, AI 도구 디렉토리 | 에이전트 간 협업, LLM 기능 확장, 도구 연결 |\n","\n","두 프로토콜은 상호 보완적인 관계를 가질 수 있습니다. MCP를 통해 AI 모델과 도구를 효율적으로 연결하고 관리하며, A2A를 통해 에이전트들이 서로 협력하여 복잡한 작업을 수행할 수 있습니다. 미래에는 MCP와 A2A 기반 인프라가 결합된 에이전트 제품 마켓플레이스가 등장하여 더욱 강력하고 유연한 AI 시스템을 구축할 수 있을 것으로 기대됩니다.\n","\n","**출처:**\n","\n","*   [Mcp 시장 지도 완벽 분석: 2025년 모델 컨텍스트 프로토콜 생태계 총정리](https://dma-ai.kr/81)\n","*   [A2a] Ai 에이전트의 공용 언어, A2a가 여는 협업의 미래](https://infogalaxy.co.kr/entry/A2A-Agent2agent-AI-에이전트)\n","*   [A2A vs MCP: 새로운 에이전트 생태계를 위한 두 개의 보완적 프로토콜 · Logto 블로그](https://blog.logto.io/ko/a2a-mcp)\n","*   [구글의 A2a, Ai 에이전트 시대의 새로운 표준이 될까? - Mcp와의 비교부터 실제 사용 예시까지 한눈에 정리](https://digitalbourgeois.tistory.com/1039)\n","*   [Meet Google A2A: The Protocol That will Revolutionize Multi-Agent AI ...](https://medium.com/@the_manoj_desai/meet-google-a2a-the-protocol-that-will-revolutionize-multi-agent-ai-systems-80d55a4583ed)\n","*   [AI 실시간 통합을 위한 핵심 기술, MCP(Model Context Protocol) 완벽 가이드](https://the-see.tistory.com/195)\n","*   [Model Context Protocol(MCP) 완벽 가이드: AI 애플리케이션 개발 표준화하기](https://dma-ai.kr/77)\n","\n","**다음 단계:**\n","\n","AI 개발자는 MCP와 A2A의 특징을 이해하고 자신의 프로젝트에 적합한 프로토콜을 선택해야 합니다. 또한, 두 프로토콜을 결합하여 더욱 강력하고 유연한 AI 시스템을 구축하는 방안을 모색해야 할 것입니다.\n","\n","inference time: 76.43123 sec \n","\n","\n","\n","\n","=========================  Search Report  ========================================\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"# MCP와 A2A: 차세대 AI 에이전트 생태계를 위한 핵심 프로토콜\n\n최근 AI 기술 발전과 함께 에이전트 간의 협업 및 정보 교환의 중요성이 부각되고 있습니다. 이러한 요구에 발맞춰 MCP(Model Context Protocol)와 A2A(Agent to Agent)라는 두 가지 프로토콜이 등장하며 AI 에이전트 생태계를 혁신하고 있습니다. MCP는 AI 모델과 도구 간의 연결을 표준화하여 효율적인 상호작용을 가능하게 하는 반면, A2A는 에이전트들이 직접 '행동 단위'로 상호작용할 수 있는 환경을 제공합니다. 본 섹션에서는 MCP와 A2A의 차이점을 심층적으로 분석하고, 각 프로토콜의 특징과 미래 전망을 살펴봅니다.\n\n## 결론/요약\n\nMCP와 A2A는 모두 AI 에이전트 생태계 발전에 기여하는 중요한 프로토콜이지만, 접근 방식과 목표에서 뚜렷한 차이를 보입니다. MCP는 AI 모델과 도구 간의 맥락 공유 및 자원 관리에 초점을 맞추는 반면, A2A는 에이전트 간의 직접적인 협업과 상호운용성을 강조합니다.\n\n| 특징 | MCP (Model Context Protocol) | A2A (Agent to Agent) |\n|---|---|---|\n| **초점** | 맥락 공유, 자원 관리 | 에이전트 간 협업, 상호운용성 |\n| **통신 방식** | 클라이언트-서버 모델 | 다중 에이전트 관리, 태스크 중심 |\n| **주요 특징** | 통합 및 커뮤니케이션 관리 | 모듈성, 툴 재사용, 캐싱 |\n| **생태계** | 다양한 클라이언트와 서버 존재 | 구글 중심, 초기 단계 |\n| **활용 분야** | 코드 에디터, AI 안전 및 연구, AI 도구 디렉토리 | 에이전트 간 협업, LLM 기능 확장, 도구 연결 |\n\n두 프로토콜은 상호 보완적인 관계를 가질 수 있습니다. MCP를 통해 AI 모델과 도구를 효율적으로 연결하고 관리하며, A2A를 통해 에이전트들이 서로 협력하여 복잡한 작업을 수행할 수 있습니다. 미래에는 MCP와 A2A 기반 인프라가 결합된 에이전트 제품 마켓플레이스가 등장하여 더욱 강력하고 유연한 AI 시스템을 구축할 수 있을 것으로 기대됩니다.\n\n**출처:**\n\n*   [Mcp 시장 지도 완벽 분석: 2025년 모델 컨텍스트 프로토콜 생태계 총정리](https://dma-ai.kr/81)\n*   [A2a] Ai 에이전트의 공용 언어, A2a가 여는 협업의 미래](https://infogalaxy.co.kr/entry/A2A-Agent2agent-AI-에이전트)\n*   [A2A vs MCP: 새로운 에이전트 생태계를 위한 두 개의 보완적 프로토콜 · Logto 블로그](https://blog.logto.io/ko/a2a-mcp)\n*   [구글의 A2a, Ai 에이전트 시대의 새로운 표준이 될까? - Mcp와의 비교부터 실제 사용 예시까지 한눈에 정리](https://digitalbourgeois.tistory.com/1039)\n*   [Meet Google A2A: The Protocol That will Revolutionize Multi-Agent AI ...](https://medium.com/@the_manoj_desai/meet-google-a2a-the-protocol-that-will-revolutionize-multi-agent-ai-systems-80d55a4583ed)\n*   [AI 실시간 통합을 위한 핵심 기술, MCP(Model Context Protocol) 완벽 가이드](https://the-see.tistory.com/195)\n*   [Model Context Protocol(MCP) 완벽 가이드: AI 애플리케이션 개발 표준화하기](https://dma-ai.kr/77)\n\n**다음 단계:**\n\nAI 개발자는 MCP와 A2A의 특징을 이해하고 자신의 프로젝트에 적합한 프로토콜을 선택해야 합니다. 또한, 두 프로토콜을 결합하여 더욱 강력하고 유연한 AI 시스템을 구축하는 방안을 모색해야 할 것입니다."},"metadata":{}}],"source":["print(\"\\n\\n=================================================================\\n\")\n","messages.append(\n","    {\"role\": \"assistant\", \"content\": \" \".join(queries)})\n","messages.append(\n","    {\"role\": \"user\", \"content\": ask_final_writer_instructions(topic, content, search_tasks)}\n",")\n","response_query = generate_response(messages)\n","\n","print(\"\\n\\n=========================  Search Report  ========================================\\n\")\n","display(Markdown(response_query))"]},{"cell_type":"markdown","source":["# Web RAG"],"metadata":{"id":"Xjvk0QMXSMlI"}},{"cell_type":"code","execution_count":72,"metadata":{"id":"96d12b3f-1667-425d-916f-f47d7f7f5a28","executionInfo":{"status":"ok","timestamp":1744442854631,"user_tz":-540,"elapsed":66,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from openai import OpenAI\n","import math\n","import time\n","import json\n","\n","client = OpenAI(\n","    base_url=\"http://localhost:8877/v1\",\n","    api_key=\"token-abc123\",\n",")"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"6cacd48f-181d-4056-b80a-14f76565b114","executionInfo":{"status":"ok","timestamp":1744443167736,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def generate_response(message_list):\n","    completion = client.chat.completions.create(\n","        model = \"ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\",\n","        messages = message_list,\n","        max_tokens=1024,\n","        frequency_penalty=0.3,\n","        temperature=0.6,\n","        stream=True,\n","    )\n","\n","    final_answer = []\n","    assistant_response = \"\"\n","\n","    start = time.time()\n","\n","    # 스트림 모드에서는 completion.choices 를 반복문으로 순회\n","    for chunk in completion:\n","        chunk_content = chunk.choices[0].delta.content\n","\n","        if isinstance(chunk_content, str):\n","            final_answer.append(chunk_content)\n","            # 토큰 단위로 실시간 답변 출력\n","            print(chunk_content, end=\"\")\n","            assistant_response += chunk_content\n","\n","    end = time.time()\n","    print(f\"\\n\\ninference time: {end - start:.5f} sec \\n\\n\")\n","    return assistant_response"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"e1bcc01f-f613-41d4-9809-773d8d1740b2","outputId":"7cd0435c-36d7-465c-ed34-5c00d57e8ced","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744443178865,"user_tz":-540,"elapsed":10872,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["USER > ㅂㅈㅇㅂㅈ\n","죄송합니다만, \"ㅂㅈㅇㅂㅈ\"라는 표현이 무슨 의미인지 정확히 파악하지 못했습니다. 혹시 오타가 있거나 다른 의도로 사용하신 표현일까요? \n","\n","만약 특정 주제나 질문이 있으시다면, 좀 더 자세하게 설명해주시거나 다른 단어로 표현해주시면 제가 최대한 정확하고 풍부한 정보를 전달해 드리도록 노력하겠습니다. \n","\n","또한, 부적절하거나 불쾌감을 줄 수 있는 표현은 사용하지 않도록 주의 부탁드립니다.\n","\n","inference time: 7.76852 sec \n","\n","\n","USER > quit\n"]}],"source":["message_list = [{\"role\": \"system\", \"content\": \"당신은 유저의 질문에 최대한 정확하고 풍부한 정보를 전달하는 assistant 이다. 답변은 항상 한국어로 공손하게 답변해줘.\"}]\n","\n","while True:\n","    user_prompt = input(\"USER > \")\n","    if user_prompt.lower() == \"quit\":\n","        break\n","    message_list.append({\"role\": \"user\", \"content\": user_prompt})\n","\n","    assistant = generate_response(message_list)\n","    message_list.append({\"role\": \"assistant\", \"content\": assistant})"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"849c0000-551b-41ad-9054-d5b70317bba5","executionInfo":{"status":"ok","timestamp":1744443182084,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["system_prompt = \"You are a helpful assistant. And Answers must be in Korean.\"\n","user_prompt = \"\"\"프로 테니스 대회에서 테니스 공은 한번에 6개를 사용합니다. 이 6개의 공을 처음에는 게임 수의 합이 7게임, 다음부터는 9게임마다 새 공으로 교체를 합니다.\n","만일 3세트 경기가 6:5 3:6 6:4 로 진행됐다고 하면 총 몇 개의 공을 사용했을까요?\n","답:\n","각 세트마다 게임 수를 더하면 11+9+10 = 30 으로 총 30게임이 진행됐습니다.\n","테니스 공은 7번째 교체 후 9번째 게임마다 교체되니 7,16,25 게임에 총 3회에 교체 됩니다.\n","최종적으로 경기시작 시 사용한 공 6개 + 교체 시 마다 6개의 새 공으로 교체 했으니 6 + (6 * 3) = 24, 사용된 공은 총 24개 입니다.\n","\n","질문:\n","아마추어 테니스 대회에서는 테니스공을 한번에 2개 사용합니다. 그리고 이 2개의 공을 처음에는 게임 수의 합이 7게임, 다음부터는 9게임마다 새공으로 교체를 합니다.\n","만일 3세트 경기가 6:5 5:7 6:7 로 진행됐다고 하면 총 몇 개의 공을 사용했을까요?\n","\"\"\""]},{"cell_type":"code","execution_count":89,"metadata":{"id":"712e56a8-7d68-46cb-bb52-8c8955fb8629","outputId":"ebf82517-45b1-4fd2-f53b-29b75f9482e3","colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"status":"ok","timestamp":1744443192294,"user_tz":-540,"elapsed":10018,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["답:\n","\n","각 세트마다 게임 수를 더하면 11+12+13 = 36 으로 총 36게임이 진행되었습니다.\n","테니스 공은 7번째 교체 후 9번째 게임마다 교체되니 7, 16, 25, 34 게임에 총 4회에 교체 됩니다.\n","최초 경기 시작 시 사용한 공 2개 + 교체 시 마다 2개의 새 공으로 교체 했으니 2 + (2 * 4) = 10, 사용된 공은 총 10개 입니다.\n","\n","\n","inference time: 9.94419 sec \n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'답:\\n\\n각 세트마다 게임 수를 더하면 11+12+13 = 36 으로 총 36게임이 진행되었습니다.\\n테니스 공은 7번째 교체 후 9번째 게임마다 교체되니 7, 16, 25, 34 게임에 총 4회에 교체 됩니다.\\n최초 경기 시작 시 사용한 공 2개 + 교체 시 마다 2개의 새 공으로 교체 했으니 2 + (2 * 4) = 10, 사용된 공은 총 10개 입니다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":89}],"source":["messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_prompt},\n","    ]\n","generate_response(messages)"]},{"cell_type":"markdown","source":["# Deep Search"],"metadata":{"id":"t99gIVQzSXpf"}},{"cell_type":"code","execution_count":90,"metadata":{"id":"af317be9-b6f6-4986-9b05-ee8a25de06e6","executionInfo":{"status":"ok","timestamp":1744443192308,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from tavily import TavilyClient\n","import asyncio, os, requests, time, json\n","import threading, queue\n","from IPython.display import display, Markdown, Latex\n","\n","tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"09e7b2d7-3d6f-4012-8d9e-99e8e8a66d34","executionInfo":{"status":"ok","timestamp":1744443226922,"user_tz":-540,"elapsed":55,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from openai import OpenAI\n","import math\n","import time\n","import json\n","\n","client = OpenAI(\n","    base_url=\"http://localhost:8877/v1\",\n","    api_key=\"token-abc123\",\n",")"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"802ea473-ad78-43fe-b096-84fcccecd767","executionInfo":{"status":"ok","timestamp":1744443227091,"user_tz":-540,"elapsed":42,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["from pydantic import BaseModel, Field\n","import operator\n","\n","class Section(BaseModel):\n","    name: str = Field(\n","        description=\"Name for this section of the report.\",\n","    )\n","    description: str = Field(\n","        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n","    )\n","    research: bool = Field(\n","        description=\"Whether to perform web research for this section of the report.\"\n","    )\n","    content: str = Field(\n","        description=\"The content of the section.\"\n","    )\n","    search_query: str = Field(None, description=\"Query for web search.\")\n","    query_content: str = Field(None, description=\"Content of web search.\")\n","    section_content: str = Field(None, description=\"Content of section.\")"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"7187e4d1-633c-4d42-a63a-07185476fa36","executionInfo":{"status":"ok","timestamp":1744443227277,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def generate_response(message_list):\n","    completion = client.chat.completions.create(\n","        model = \"ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\",\n","        messages = message_list,\n","        max_tokens=2048,\n","        frequency_penalty=0.3,\n","        temperature=0.6,\n","        stream=True,\n","    )\n","\n","    final_answer = []\n","    assistant_response = \"\"\n","\n","    start = time.time()\n","\n","    # 스트림 모드에서는 completion.choices 를 반복문으로 순회\n","    for chunk in completion:\n","        chunk_content = chunk.choices[0].delta.content\n","\n","        if isinstance(chunk_content, str):\n","            final_answer.append(chunk_content)\n","            # 토큰 단위로 실시간 답변 출력\n","            print(chunk_content, end=\"\")\n","            assistant_response += chunk_content\n","\n","    end = time.time()\n","    print(f\"\\n\\ninference time: {end - start:.5f} sec \\n\\n\")\n","    return assistant_response"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"8e3ec1bb-cf1c-421c-bb6e-3ac5d2559fad","executionInfo":{"status":"ok","timestamp":1744443227525,"user_tz":-540,"elapsed":98,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def report_planner_instructions(topic, report_organization, context, feedback):\n","    planner_writer=\"\"\"You are performing research for a report.\n","<Report topic>\n","\"\"\" + topic + \"\"\"\n","</Report topic>\n","\n","<Report organization>\n","\"\"\" + report_organization + \"\"\"\n","</Report organization>\n","\n","<Context>\n","Here is context to use to plan the sections of the report:\n","\"\"\" + context + \"\"\"\n","</Context>\n","\n","<Task>\n","Generate a list of sections for the report. Your plan should be tight and focused with NO overlapping sections or unnecessary filler.\n","\n","For example, a good report structure might look like:\n","1/ intro\n","2/ overview of topic A\n","3/ overview of topic B\n","4/ comparison between A and B\n","5/ conclusion\n","\n","Each section should have the fields:\n","\n","- Name - Name for this section of the report.\n","- Description - Brief overview of the main topics covered in this section.\n","- Research - Whether to perform web research for this section of the report.\n","- Content - The content of the section, which you will leave blank for now.\n","\n","Integration guidelines:\n","- Include examples and implementation details within main topic sections, not as separate sections\n","- Ensure each section has a distinct purpose with no content overlap\n","- Combine related concepts rather than separating them\n","\n","Before submitting, review your structure to ensure it has no redundant sections and follows a logical flow.\n","</Task>\n","\n","<Feedback>\n","Here is feedback on the report structure from review (if any):\n","\"\"\" + feedback + \"\"\"\n","</Feedback>\n","\n","Note1. that today's date is \"\"\"+time.strftime(\"%Y-%m-%d\")+\"\"\".\n","Note2. Output your response in JSON format, with the following structure: { \"sections\": [ \"section1\", \"section2\", \"section3\" ] }\n","Only output in JSON format when generating responses. Never include additional phrases such as \"here is content in JSON format\".\n","\"\"\"\n","\n","    return planner_writer"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"a5dd2eaf-4a7c-479c-9f86-955e1bfbc2c3","executionInfo":{"status":"ok","timestamp":1744443227582,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def report_query_writer(topic, report_organization, num_queries):\n","    llm_prompt = \"\"\"You are performing research for a report.\n","\n","<Report topic>\n","\"\"\" + topic + \"\"\"\n","</Report topic>\n","\n","<Report organization>\n","\"\"\" + report_organization + \"\"\"\n","</Report organization>\n","\n","<Task>\n","Your goal is to generate \"\"\" + num_queries + \"\"\" web search queries that will help gather information for planning the report sections.\n","\n","The queries should:\n","\n","1. Be related to the Report topic\n","2. Help satisfy the requirements specified in the report organization\n","\n","Make the queries specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\n","\n","Note1. that today's date is \"\"\"+time.strftime(\"%Y-%m-%d\")+\"\"\".\n","Note2. Output your response in JSON format, with the following structure: { \"queries\": [ \"query1\", \"query2\", \"query3\" ] }\n","Only output in JSON format when generating responses. Never include additional phrases such as \"here is content in JSON format\".\n","</Task>\n","\"\"\"\n","\n","    return llm_prompt"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"15a539f5-c50a-4337-b78a-71bfdab4bdab","executionInfo":{"status":"ok","timestamp":1744443227919,"user_tz":-540,"elapsed":172,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def section_writer_inputs(topic, section_name, section_topic, context):\n","    section_writer_prompt=\"\"\"\n","<Report topic>\n","\"\"\" + topic + \"\"\"\n","</Report topic>\n","\n","<Section name>\n","\"\"\" + section_name + \"\"\"\n","</Section name>\n","\n","<Section topic>\n","\"\"\" + section_topic + \"\"\"\n","</Section topic>\n","\n","<Source material>\n","\"\"\" + context + \"\"\"\n","</Source material>\n","\"\"\"\n","    return section_writer_prompt"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"cb095563-f701-4681-99cd-fa10ea5a4613","executionInfo":{"status":"ok","timestamp":1744443227938,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def final_section_writer_instructions(topic, section_name, section_topic, context):\n","    final_writer_prompt=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n","\n","<Report topic>\n","\"\"\" + topic + \"\"\"\n","</Report topic>\n","\n","<Section name>\n","\"\"\" + section_name + \"\"\"\n","</Section name>\n","\n","<Section topic>\n","\"\"\" + section_topic + \"\"\"\n","</Section topic>\n","\n","<Available report content>\n","\"\"\" + context + \"\"\"\n","</Available report content>\n","\n","<Task>\n","1. Section-Specific Approach:\n","\n","For Introduction:\n","- Use # for report title (Markdown format)\n","- 50-100 word limit\n","- Write in simple and clear language\n","- Focus on the core motivation for the report in 1-2 paragraphs\n","- Use a clear narrative arc to introduce the report\n","- Include NO structural elements (no lists or tables)\n","- No sources section needed\n","\n","For Conclusion:\n","- Use ## for section title (Markdown format)\n","- 200-300 word limit\n","- For comparative reports:\n","    * Must include a focused comparison table using Markdown table syntax\n","    * Table should distill insights from the report\n","    * Keep table entries clear and concise\n","- For non-comparative reports:\n","    * Only use ONE structural element IF it helps distill the points made in the report:\n","    * Either a focused table comparing items present in the report (using Markdown table syntax)\n","    * Or a short list using proper Markdown list syntax:\n","      - Use `*` or `-` for unordered lists\n","      - Use `1.` for ordered lists\n","      - Ensure proper indentation and spacing\n","- End with specific next steps or implications\n","- No sources section needed\n","\n","2. Writing Approach:\n","- Use concrete details over general statements\n","- Make every word count\n","- Focus on your single most important point\n","</Task>\n","\n","<Quality Checks>\n","- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n","- For conclusion: 200-300 word limit, ## for section title, only ONE structural element at most, no sources section\n","- Markdown format\n","- Do not include word count or any preamble in your response\n","</Quality Checks>\n","\n","Please note that respond in Korean always.\"\"\"\n","\n","    return final_writer_prompt"]},{"cell_type":"code","execution_count":114,"metadata":{"id":"5a562baf-69b4-4a79-a840-699b407551bd","executionInfo":{"status":"ok","timestamp":1744443228251,"user_tz":-540,"elapsed":143,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["report_organization = \"\"\"Use this structure to create a report on the user-provided topic:\n","\n","1. Introduction (no research needed)\n","   - Brief overview of the topic area\n","\n","2. Main Body Sections:\n","   - Each section should focus on a sub-topic of the user-provided topic\n","\n","3. Conclusion\n","   - Aim for 1 structural element (either a list of table) that distills the main body sections\n","   - Provide a concise summary of the report\"\"\""]},{"cell_type":"code","execution_count":166,"metadata":{"id":"31db4c38-0935-4715-9274-7b40d975f14b","executionInfo":{"status":"ok","timestamp":1744444675166,"user_tz":-540,"elapsed":38,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["section_writer_instructions = \"\"\"Write one section of a research report.\n","\n","<Task>\n","1. Review the report topic, section name, and section topic carefully.\n","2. If present, review any existing section content.\n","3. Then, look at the provided Source material.\n","4. Decide the sources that you will use it to write a report section.\n","5. Write the report section and list your sources.\n","</Task>\n","\n","<Writing Guidelines>\n","- If existing section content is not populated, write from scratch\n","- If existing section content is populated, synthesize it with the source material\n","- Strict 150-200 word limit\n","- Use simple, clear language\n","- Use short paragraphs (2-3 sentences max)\n","- Use ## for section title (Markdown format)\n","</Writing Guidelines>\n","\n","<Citation Rules>\n","- Assign each unique URL a single citation number in your text\n","- End with ### Sources that lists each source with corresponding numbers\n","- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose\n","- Example format:\n","  [1] Source Title: URL\n","  [2] Source Title: URL\n","</Citation Rules>\n","\n","<Final Check>\n","1. Verify that EVERY claim is grounded in the provided Source material\n","2. Confirm each URL appears ONLY ONCE in the Source list\n","3. Verify that sources are numbered sequentially (1,2,3...) without any gaps\n","</Final Check>\n","\"\"\"\n"]},{"cell_type":"code","execution_count":167,"metadata":{"id":"f915b96a-a771-49e2-8056-91d4d435cd4b","executionInfo":{"status":"ok","timestamp":1744444675369,"user_tz":-540,"elapsed":4,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def worker(query, search_result, req_num_result, include_raw, req_topic):\n","    print(f\"Thread: {query}\")\n","    search_result.append(\n","        tavily_client.search(\n","            query,\n","            max_results= req_num_result,\n","            include_raw_content= include_raw,\n","            topic= req_topic\n","        )\n","    )"]},{"cell_type":"code","execution_count":168,"metadata":{"id":"f27acce1-9f64-46af-8fb6-752fdb5f69b8","executionInfo":{"status":"ok","timestamp":1744444675568,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def ask_tavily(search_queries, search_tasks, req_num_result, include_raw, req_topic, opt_print=True):\n","    if opt_print:\n","        print(\"\\nRun ask_tavily task: \\n\")\n","\n","    threads = []\n","    start_time = time.time()\n","\n","    for query in search_queries:\n","        t = threading.Thread(target=worker, args=(query, search_tasks, req_num_result, include_raw, req_topic))\n","        threads.append(t)\n","        t.start()\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    end_time = time.time()\n","    execution_time = end_time - start_time\n","\n","    if opt_print:\n","        print(f\"\\nask_tavily task running time: {execution_time:.2f}초 \\n\")"]},{"cell_type":"code","execution_count":169,"metadata":{"id":"fb72fc1e-d5eb-44ae-966e-7aeb00c21bb4","executionInfo":{"status":"ok","timestamp":1744444675886,"user_tz":-540,"elapsed":112,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n","     # Collect all results\n","    sources_list = []\n","    for response in search_response:\n","        sources_list.extend(response['results'])\n","\n","    # Deduplicate by URL\n","    unique_sources = {source['url']: source for source in sources_list}\n","\n","    # Format output\n","    formatted_text = \"Content from sources:\\n\"\n","    for i, source in enumerate(unique_sources.values(), 1):\n","        formatted_text += f\"{'='*80}\\n\"  # Clear section separator\n","        formatted_text += f\"Source: {source['title']}\\n\"\n","        formatted_text += f\"{'-'*80}\\n\"  # Subsection separator\n","        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n","        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n","        if include_raw_content:\n","            # Using rough estimate of 4 characters per token\n","            char_limit = max_tokens_per_source * 2\n","            # Handle None raw_content\n","            raw_content = source.get('raw_content', '')\n","            if raw_content is None:\n","                raw_content = ''\n","                print(f\"Warning: No raw_content found for source {source['url']}\")\n","            if len(raw_content) > char_limit:\n","                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n","            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n","        formatted_text += f\"{'='*80}\\n\\n\" # End section separator\n","\n","    return formatted_text.strip()"]},{"cell_type":"code","execution_count":170,"metadata":{"id":"f96e90b0-74fd-4ea5-be28-2544492488a5","executionInfo":{"status":"ok","timestamp":1744444675914,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def web_search_worker(section: Section, opt_print=False):\n","    print(f\"Thread: {section}\")\n","\n","    if section.research:\n","        section_query_prompt = report_query_writer(section.name, section.description, \"3\")\n","\n","        messages = [\n","            {\"role\": \"system\", \"content\": section_query_prompt},\n","            {\"role\": \"user\", \"content\": \"Generate search queries on the provided topic.\"},\n","        ]\n","\n","        response_section_queries = generate_response(messages)\n","\n","        json_data = json.loads(response_section_queries)\n","        queries = json_data['queries']\n","\n","        section.search_query = queries\n","\n","        search_tasks = []\n","        req_topic = 'general' # news   gerneral 과 news 중 선택\n","        req_num_result = 2    # 각 web query 에 대해 리턴할 site 개수\n","        include_raw = True    # site 의 원본 컨텐츠 리턴 유무\n","\n","        ask_tavily(queries, search_tasks, req_num_result, include_raw, req_topic, opt_print)\n","        source_str = deduplicate_and_format_sources(search_tasks, max_tokens_per_source=2000, include_raw_content=True)\n","        section.query_content = source_str\n","\n","        messages = [\n","            {\"role\": \"system\", \"content\": section_writer_instructions},\n","            {\"role\": \"user\", \"content\": section_writer_inputs(topic, section.name, section.description, source_str)},\n","        ]\n","        section.section_content = generate_response(messages)\n"]},{"cell_type":"code","execution_count":210,"metadata":{"id":"82f26101-f94f-49af-b222-a070d04aa20d","executionInfo":{"status":"ok","timestamp":1744445184756,"user_tz":-540,"elapsed":46,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["def final_section_writer_worker(section: Section, opt_print=True):\n","    user_prompt = \"Generate a report section based on the provided sources.\"\n","    final_section_writer_instructions(topic, section.name, section.description, source_str)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": final_section_writer_instructions},\n","        {\"role\": \"user\", \"content\": user_prompt}\n","    ]\n","\n","    section.section_content = generate_response(messages)"]},{"cell_type":"code","execution_count":211,"metadata":{"id":"afa5a365-3ad0-4745-8735-b2fc15dbc765","executionInfo":{"status":"ok","timestamp":1744445184904,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["topic = \"MCP(model context protocol) 과 A2A(Agent to Agent) 는 어떤 차이가 있는것인지 알려줘.\"\n","num_queries = \"3\"\n","model_id = 102\n","report_planner_query_prompt = report_query_writer(topic, report_organization, num_queries)"]},{"cell_type":"code","execution_count":212,"metadata":{"id":"7ea948e2-f58a-4175-b808-88fe0a9412fb","outputId":"12f6cac2-a896-4e8b-f002-759d6f29c283","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744445190615,"user_tz":-540,"elapsed":5514,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","{\n","  \"queries\": [\n","    \"MCP (Model Context Protocol) vs A2A (Agent to Agent) technical comparison 2024-2025\",\n","    \"A2A Agent to Agent communication frameworks and use cases\",\n","    \"MCP protocol implementation details and limitations agent interaction\"\n","  ]\n","}\n","```\n","\n","inference time: 5.48270 sec \n","\n","\n","사용자 발화 기반으로 추출한 web query 문장 3건:\n","['MCP (Model Context Protocol) vs A2A (Agent to Agent) technical comparison 2024-2025', 'A2A Agent to Agent communication frameworks and use cases', 'MCP protocol implementation details and limitations agent interaction']\n"]}],"source":["user_prompt = \"Generate search queries that will help with planning the sections of the report.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": report_planner_query_prompt},\n","    {\"role\": \"user\", \"content\": user_prompt}\n","]\n","\n","response_query = generate_response(messages)\n","if \"```json\" in response_query:\n","    response_query = response_query.split(\"```json\")[1].strip()\n","    response_query = response_query.split(\"```\")[0].strip()\n","json_data = json.loads(response_query)\n","queries = json_data['queries']\n","\n","print(\"사용자 발화 기반으로 추출한 web query 문장 3건:\")\n","print(queries)"]},{"cell_type":"code","execution_count":228,"metadata":{"id":"affbae24-f9c8-4957-a3a5-89e9ba61b4f5","outputId":"700b5ab8-9b1a-4a77-9ff5-a9a3a9d25e26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744445629280,"user_tz":-540,"elapsed":2734,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Thread: MCP (Model Context Protocol) vs A2A (Agent to Agent) technical comparison 2024-2025\n","Thread: A2A Agent to Agent communication frameworks and use cases\n","Thread: MCP protocol implementation details and limitations agent interaction\n","\n","ask_tavily task running time: 2.69초 \n","\n"]}],"source":["search_tasks = []\n","req_topic = 'general' # news   gerneral 과 news 중 선택\n","req_num_result = 2    # 각 web query 에 대해 리턴할 site 개수\n","include_raw = False    # site 의 원본 컨텐츠 리턴 유무\n","\n","ask_tavily(queries, search_tasks, req_num_result, include_raw, req_topic)\n","source_str = deduplicate_and_format_sources(search_tasks, max_tokens_per_source=2000, include_raw_content=False)\n"]},{"cell_type":"code","execution_count":229,"metadata":{"id":"7f263d81-0014-4360-a51d-13c50fea7007","outputId":"36d5b160-9679-4f06-a1d5-ce58b06304bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744445629293,"user_tz":-540,"elapsed":11,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["You are performing research for a report.\n","<Report topic>\n","MCP(model context protocol) 과 A2A(Agent to Agent) 는 어떤 차이가 있는것인지 알려줘.\n","</Report topic>\n","\n","<Report organization>\n","Use this structure to create a report on the user-provided topic:\n","\n","1. Introduction (no research needed)\n","   - Brief overview of the topic area\n","\n","2. Main Body Sections:\n","   - Each section should focus on a sub-topic of the user-provided topic\n","\n","3. Conclusion\n","   - Aim for 1 structural element (either a list of table) that distills the main body sections\n","   - Provide a concise summary of the report\n","</Report organization>\n","\n","<Context>\n","Here is context to use to plan the sections of the report:\n","Content from sources:\n","================================================================================\n","Source: MCP Agents: The Open Standard Revolutionizing Context-Aware AI\n","--------------------------------------------------------------------------------\n","URL: https://www.luseratech.com/ai/mcp-agents-the-open-standard-revolutionizing-context-aware-ai\n","===\n","Most relevant content from source: The Model Context Protocol (MCP) Agents represent a transformative leap in artificial intelligence, enabling systems to dynamically interact with external tools, data sources, and services through a standardized framework. By bridging the gap between large language models (LLMs) and real-world applications, MCP Agents transcend static data limitations, allowing AI to access real-time information, execute complex tasks, and make context-aware decisions autonomously. MCP’s architecture resolves these through a client-server model where AI agents (host applications) interact with MCP servers that provide standardized access to tools, databases, APIs, and prompts. By standardizing how AI systems access tools and data, MCP enables the creation of dynamic, context-aware agents capable of autonomous operation across industries.\n","===\n","================================================================================\n","\n","================================================================================\n","Source: Model Context Protocol (MCP): The Key To Agentic AI\n","--------------------------------------------------------------------------------\n","URL: https://medium.com/@jalajagr/model-context-protocol-mcp-the-key-to-agentic-ai-a26eaf19050e\n","===\n","Most relevant content from source: Stories Member-only story Model Context Protocol (MCP): The Key To Agentic AI Jalaj Agrawal MCP, or Model Context Protocol, is a framework that allows AI models to retrieve and process external context — from databases to files and APIs. The name itself reveals its core purpose: If you ask an AI to verify the sum of orders in an SQLite database and match it with a PowerPoint presentation, the model alone can’t do it — it needs access to those files and databases. MCP solves this problem by acting as a middleware layer, allowing AI models to: This means richer responses, deeper integrations, and more intelligent AI-powered applications. An AI enthusiast exploring intersection of ML, LLM, personalized experiences, human-centric design. No responses yet\n","===\n","================================================================================\n","\n","================================================================================\n","Source: pab1it0/awesome-a2a: A collection of A2A servers and clients. - GitHub\n","--------------------------------------------------------------------------------\n","URL: https://github.com/pab1it0/awesome-a2a\n","===\n","Most relevant content from source: By use case. DevSecOps DevOps CI/CD View all use cases By industry. Healthcare ... 🏬 Communication Services. A2A servers for messaging, email, and other communication tools. ... Google Agent Development Kit (ADK) 👖‍ 🐍 - Google's framework for building A2A-compliant agents. LangGraph 🐍 - A framework for building stateful, multi\n","===\n","================================================================================\n","\n","================================================================================\n","Source: GitHub - google/A2A: An open protocol enabling communication and ...\n","--------------------------------------------------------------------------------\n","URL: https://github.com/google/A2A\n","===\n","Most relevant content from source: That’s why we created an open Agent2Agent (A2A) protocol, a collaborative way to help agents across different ecosystems communicate with each other. Google is driving this open protocol initiative for the industry because we believe this protocol will be critical to support multi-agent communication by giving your agents a common language – irrespective of the framework or vendor they are built on. The Agent2Agent (A2A) protocol facilitates communication between independent AI agents. A2A Server: An agent exposing an HTTP endpoint that implements the A2A protocol methods (defined in the json specification). A2A Protocol is an open source project run by Google LLC, under License and open to contributions from the entire community.\n","===\n","================================================================================\n","\n","================================================================================\n","Source: The future of AI tooling is Interoperable: MCP and Agent2Agent\n","--------------------------------------------------------------------------------\n","URL: https://www.vccafe.com/2025/04/10/the-future-of-ai-tooling-is-interoperable-mcp-and-agent2agent/\n","===\n","Most relevant content from source: The future of AI tooling is Interoperable: MCP and Agent2Agent The future of AI tooling is Interoperable: MCP and Agent2Agent Two recent developments are poised to revolutionise this: Anthropic’s Model Context Protocol (MCP) and Google’s Agent2Agent (A2A) protocol. Enter the Model Context Protocol (MCP): A Universal Remote for AI Introduced in November 2024, MCP is an open protocol that provides a generalisable way for systems to offer context to AI models. MCP is an open protocol that allows systems to provide context to AI models in a manner that’s generalizable across integrations Building on the momentum of agentic AI, Google announced yesterday their Agent2Agent (A2A) protocol, an open interoperability protocol designed for seamless collaboration between AI agents across diverse frameworks and vendors.\n","===\n","================================================================================\n","\n","================================================================================\n","Source: A2A vs MCP: key difference - by fulliron\n","--------------------------------------------------------------------------------\n","URL: https://fulliron.substack.com/p/a2a-vs-mcp-key-difference\n","===\n","Most relevant content from source: Full Iron Substack A2A vs MCP: key difference Full Iron Substack A2A vs MCP: key difference The key differences between **A2A (Agent-to-Agent Protocol)** and **MCP (Model Context Protocol)** lie in their focus and functionality: In summary, A2A focuses on agent collaboration, while MCP standardizes how models interact with external resources. [4] Make Your Agentic Applications More Powerful With MCP (Model Context Protocol) https://www.linkedin.com/pulse/make-your-agentic-applications-more-powerful-mcp-model-belagatti-ll7cc [5] AI Spotlight: MCP (Model Context Protocol) and Agentic AI systems https://www.gravitee.io/blog/mcp-model-context-protocol-agentic-ai https://ai.stackexchange.com/questions/26289/what-are-the-differences-between-an-agent-and-a-model [7] The Dawn of Agentic DevOps: Understanding Model Context Protocol (MCP) https://www.linkedin.com/pulse/dawn-agentic-devops-understanding-model-context-protocol-gourav-shah-wjqjc [8] PydanticAI Agents vs Model Context Protocol (MCP) Subscribe to Full Iron Substack Full Iron Substack A2A vs MCP: key difference Click the link we sent to , or click here to sign in.\n","===\n","================================================================================\n","</Context>\n","\n","<Task>\n","Generate a list of sections for the report. Your plan should be tight and focused with NO overlapping sections or unnecessary filler.\n","\n","For example, a good report structure might look like:\n","1/ intro\n","2/ overview of topic A\n","3/ overview of topic B\n","4/ comparison between A and B\n","5/ conclusion\n","\n","Each section should have the fields:\n","\n","- Name - Name for this section of the report.\n","- Description - Brief overview of the main topics covered in this section.\n","- Research - Whether to perform web research for this section of the report.\n","- Content - The content of the section, which you will leave blank for now.\n","\n","Integration guidelines:\n","- Include examples and implementation details within main topic sections, not as separate sections\n","- Ensure each section has a distinct purpose with no content overlap\n","- Combine related concepts rather than separating them\n","\n","Before submitting, review your structure to ensure it has no redundant sections and follows a logical flow.\n","</Task>\n","\n","<Feedback>\n","Here is feedback on the report structure from review (if any):\n","\n","</Feedback>\n","\n","Note1. that today's date is 2025-04-12.\n","Note2. Output your response in JSON format, with the following structure: { \"sections\": [ \"section1\", \"section2\", \"section3\" ] }\n","Only output in JSON format when generating responses. Never include additional phrases such as \"here is content in JSON format\".\n","\n"]}],"source":["feedback = \"\"\n","planner_writer_prompt = report_planner_instructions(topic, report_organization, source_str, feedback)\n","print(planner_writer_prompt)"]},{"cell_type":"code","execution_count":230,"metadata":{"id":"67c70fe7-d9b1-456c-90d3-13dddefa94f9","outputId":"2807fba7-b51f-473d-cf18-de5cb2cf25f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744446331099,"user_tz":-540,"elapsed":71724,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","{\n","  \"sections\": [\n","    {\n","      \"name\": \"Introduction\",\n","      \"description\": \"A brief overview of Model Context Protocol (MCP) and Agent to Agent (A2A) protocols, highlighting their emergence in the field of AI tooling and their significance for interoperability.\",\n","      \"research\": false,\n","      \"content\": \"The rapid development of agentic AI necessitates standardized communication and context provision. Two key protocols addressing this need are Model Context Protocol (MCP) and Agent to Agent (A2A). MCP focuses on enabling AI models to access and utilize external context, while A2A focuses on facilitating communication between independent AI agents. Both aim to resolve limitations in current AI systems, but approach the problem from different angles. This report will detail each protocol and highlight their key differences.\"\n","    },\n","    {\n","      \"name\": \"Model Context Protocol (MCP) – Enabling Context-Aware AI\",\n","      \"description\": \"Detailed explanation of MCP's architecture, functionality, and purpose. Focus on how it bridges the gap between LLMs and external resources.\",\n","      \"research\": false,\n","      \"content\": \"MCP is an open protocol designed to provide a standardized way for systems to offer context to AI models. It acts as a middleware layer, allowing models to retrieve and process external information from databases, files, and APIs. Unlike simply providing static data, MCP enables dynamic interaction with these resources. This is achieved through a client-server model where AI agents (clients) interact with MCP servers providing access to tools and data. The core purpose of MCP is to enable richer responses, deeper integrations, and more intelligent applications by providing models with real-time information and the ability to execute complex tasks autonomously.\"\n","    },\n","    {\n","      \"name\": \"Agent to Agent (A2A) – Facilitating Agent Collaboration\",\n","      \"description\": \"Detailed explanation of A2A's architecture, functionality, and purpose. Focus on how it enables communication between different AI agents.\",\n","      \"research\": false,\n","      \"content\": \"A2A is an open protocol developed by Google intended to enable seamless collaboration between AI agents across diverse frameworks and vendors. It establishes a common language for agents irrespective of their underlying technology. A2A operates via an HTTP endpoint implemented by A2A Servers which expose methods defined in a JSON specification. The key goal is interoperability – allowing agents built with different frameworks (e.g., LangGraph, Google ADK) to communicate effectively without requiring custom integrations or understanding of each other's internal workings.\"\n","    },\n","    {\n","      \"name\": \"MCP vs. A2A: Key Differences & Synergies\",\n","      \"description\": \"Direct comparison of MCP and A2A protocols highlighting their distinct focuses: resource access vs. agent communication.\",\n","      \"research\": false,\n","      \"content\": \"While both protocols contribute to advancing agentic AI capabilities, they address different aspects of the challenge. **MCP** primarily concerns itself with *how* a model accesses external resources – standardizing the interface for tools, databases & APIs. It equips an agent with the ability to *utilize* information effectively. **A2A**, on the other hand, focuses on *who* an agent communicates with – establishing a common language for agent-to-agent interactions across different ecosystems. Essentially, A2A enables agents to *collaborate*, while MCP empowers them with contextual awareness. They aren’t mutually exclusive; in fact they can be complementary – an agent utilizing MCP for data access could then use A2A to share insights gained with other agents.\"\n","    },\n","    {\n","      \"name\": \"Conclusion\",\n","      \"description\": \"Summary of the key distinctions between MCP and A2A along with a table summarizing their core features.\",\n","      \"research\": false,\n","      \"content\": \"In conclusion, both MCP and A2A represent significant steps toward more interoperable and powerful AI systems.\\n\\n| Feature | Model Context Protocol (MCP) | Agent to Agent (A2A) |\\n|---|---|---|\\n| **Primary Focus** | Standardized access to external context | Inter-agent communication |\\n| **Functionality** | Middleware layer for tool/data interaction | Protocol for agent collaboration |\\n| **Key Benefit** | Context-aware AI applications | Seamless interoperability between agents |\\n| **Developed By** | Anthropic | Google |\\n\\nThe future of AI tooling likely involves integration of both protocols—leveraging MCP for robust context provision within individual agents alongside A2A for effective collaboration between them.\"\n","    }\n","  ]\n","}\n","```\n","\n","inference time: 71.73124 sec \n","\n","\n"]}],"source":["plan_user_prompt = \"\"\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections.\n","                      Each section must have: name, description, research and contentfields.\n","                      Content must filled, not would be None.\n","                      You must not add anything other than these fields under any circumstances.\"\"\"\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": planner_writer_prompt},\n","    {\"role\": \"user\", \"content\": plan_user_prompt}\n","]\n","\n","response_planner = generate_response(messages)\n","if \"```json\" in response_planner:\n","    response_planner = response_planner.split(\"```json\")[1].strip()\n","    response_planner = response_planner.split(\"```\")[0].strip()\n","json_planner_data = json.loads(response_planner)"]},{"cell_type":"code","execution_count":231,"metadata":{"id":"7fad2b6a-0b32-4b75-8f52-26a0bb20be55","outputId":"4b6a94f1-f4c7-4bfe-bfa6-8681f5093a6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744446348630,"user_tz":-540,"elapsed":13,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[\n","    {\n","        \"name\": \"Introduction\",\n","        \"description\": \"A brief overview of Model Context Protocol (MCP) and Agent to Agent (A2A) protocols, highlighting their emergence in the field of AI tooling and their significance for interoperability.\",\n","        \"research\": false,\n","        \"content\": \"The rapid development of agentic AI necessitates standardized communication and context provision. Two key protocols addressing this need are Model Context Protocol (MCP) and Agent to Agent (A2A). MCP focuses on enabling AI models to access and utilize external context, while A2A focuses on facilitating communication between independent AI agents. Both aim to resolve limitations in current AI systems, but approach the problem from different angles. This report will detail each protocol and highlight their key differences.\"\n","    },\n","    {\n","        \"name\": \"Model Context Protocol (MCP) \\u2013 Enabling Context-Aware AI\",\n","        \"description\": \"Detailed explanation of MCP's architecture, functionality, and purpose. Focus on how it bridges the gap between LLMs and external resources.\",\n","        \"research\": false,\n","        \"content\": \"MCP is an open protocol designed to provide a standardized way for systems to offer context to AI models. It acts as a middleware layer, allowing models to retrieve and process external information from databases, files, and APIs. Unlike simply providing static data, MCP enables dynamic interaction with these resources. This is achieved through a client-server model where AI agents (clients) interact with MCP servers providing access to tools and data. The core purpose of MCP is to enable richer responses, deeper integrations, and more intelligent applications by providing models with real-time information and the ability to execute complex tasks autonomously.\"\n","    },\n","    {\n","        \"name\": \"Agent to Agent (A2A) \\u2013 Facilitating Agent Collaboration\",\n","        \"description\": \"Detailed explanation of A2A's architecture, functionality, and purpose. Focus on how it enables communication between different AI agents.\",\n","        \"research\": false,\n","        \"content\": \"A2A is an open protocol developed by Google intended to enable seamless collaboration between AI agents across diverse frameworks and vendors. It establishes a common language for agents irrespective of their underlying technology. A2A operates via an HTTP endpoint implemented by A2A Servers which expose methods defined in a JSON specification. The key goal is interoperability \\u2013 allowing agents built with different frameworks (e.g., LangGraph, Google ADK) to communicate effectively without requiring custom integrations or understanding of each other's internal workings.\"\n","    },\n","    {\n","        \"name\": \"MCP vs. A2A: Key Differences & Synergies\",\n","        \"description\": \"Direct comparison of MCP and A2A protocols highlighting their distinct focuses: resource access vs. agent communication.\",\n","        \"research\": false,\n","        \"content\": \"While both protocols contribute to advancing agentic AI capabilities, they address different aspects of the challenge. **MCP** primarily concerns itself with *how* a model accesses external resources \\u2013 standardizing the interface for tools, databases & APIs. It equips an agent with the ability to *utilize* information effectively. **A2A**, on the other hand, focuses on *who* an agent communicates with \\u2013 establishing a common language for agent-to-agent interactions across different ecosystems. Essentially, A2A enables agents to *collaborate*, while MCP empowers them with contextual awareness. They aren\\u2019t mutually exclusive; in fact they can be complementary \\u2013 an agent utilizing MCP for data access could then use A2A to share insights gained with other agents.\"\n","    },\n","    {\n","        \"name\": \"Conclusion\",\n","        \"description\": \"Summary of the key distinctions between MCP and A2A along with a table summarizing their core features.\",\n","        \"research\": false,\n","        \"content\": \"In conclusion, both MCP and A2A represent significant steps toward more interoperable and powerful AI systems.\\n\\n| Feature | Model Context Protocol (MCP) | Agent to Agent (A2A) |\\n|---|---|---|\\n| **Primary Focus** | Standardized access to external context | Inter-agent communication |\\n| **Functionality** | Middleware layer for tool/data interaction | Protocol for agent collaboration |\\n| **Key Benefit** | Context-aware AI applications | Seamless interoperability between agents |\\n| **Developed By** | Anthropic | Google |\\n\\nThe future of AI tooling likely involves integration of both protocols\\u2014leveraging MCP for robust context provision within individual agents alongside A2A for effective collaboration between them.\"\n","    }\n","]\n"]}],"source":["plan_from_llm = json_planner_data['sections']\n","print(json.dumps(plan_from_llm, indent=4))"]},{"cell_type":"code","execution_count":232,"metadata":{"id":"fd78a78e-265c-49a8-973a-03d934e31082","executionInfo":{"status":"ok","timestamp":1744446349493,"user_tz":-540,"elapsed":4,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["report_sections = []\n","\n","for part in plan_from_llm:\n","    section = Section(\n","        name=part['name'],\n","        description=part['description'],\n","        content=part['content'],\n","        research=part['research']\n","    )\n","    report_sections.append(section)"]},{"cell_type":"code","execution_count":233,"metadata":{"id":"cef6d502-0bc6-4484-b238-525c3c221e17","outputId":"0ca88d25-fffe-4280-b2c8-0b79990a1b05","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744446349998,"user_tz":-540,"elapsed":16,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["name='Introduction' description='A brief overview of Model Context Protocol (MCP) and Agent to Agent (A2A) protocols, highlighting their emergence in the field of AI tooling and their significance for interoperability.' research=False content='The rapid development of agentic AI necessitates standardized communication and context provision. Two key protocols addressing this need are Model Context Protocol (MCP) and Agent to Agent (A2A). MCP focuses on enabling AI models to access and utilize external context, while A2A focuses on facilitating communication between independent AI agents. Both aim to resolve limitations in current AI systems, but approach the problem from different angles. This report will detail each protocol and highlight their key differences.' search_query=None query_content=None section_content=None \n","\n","name='Model Context Protocol (MCP) – Enabling Context-Aware AI' description=\"Detailed explanation of MCP's architecture, functionality, and purpose. Focus on how it bridges the gap between LLMs and external resources.\" research=False content='MCP is an open protocol designed to provide a standardized way for systems to offer context to AI models. It acts as a middleware layer, allowing models to retrieve and process external information from databases, files, and APIs. Unlike simply providing static data, MCP enables dynamic interaction with these resources. This is achieved through a client-server model where AI agents (clients) interact with MCP servers providing access to tools and data. The core purpose of MCP is to enable richer responses, deeper integrations, and more intelligent applications by providing models with real-time information and the ability to execute complex tasks autonomously.' search_query=None query_content=None section_content=None \n","\n","name='Agent to Agent (A2A) – Facilitating Agent Collaboration' description=\"Detailed explanation of A2A's architecture, functionality, and purpose. Focus on how it enables communication between different AI agents.\" research=False content=\"A2A is an open protocol developed by Google intended to enable seamless collaboration between AI agents across diverse frameworks and vendors. It establishes a common language for agents irrespective of their underlying technology. A2A operates via an HTTP endpoint implemented by A2A Servers which expose methods defined in a JSON specification. The key goal is interoperability – allowing agents built with different frameworks (e.g., LangGraph, Google ADK) to communicate effectively without requiring custom integrations or understanding of each other's internal workings.\" search_query=None query_content=None section_content=None \n","\n","name='MCP vs. A2A: Key Differences & Synergies' description='Direct comparison of MCP and A2A protocols highlighting their distinct focuses: resource access vs. agent communication.' research=False content='While both protocols contribute to advancing agentic AI capabilities, they address different aspects of the challenge. **MCP** primarily concerns itself with *how* a model accesses external resources – standardizing the interface for tools, databases & APIs. It equips an agent with the ability to *utilize* information effectively. **A2A**, on the other hand, focuses on *who* an agent communicates with – establishing a common language for agent-to-agent interactions across different ecosystems. Essentially, A2A enables agents to *collaborate*, while MCP empowers them with contextual awareness. They aren’t mutually exclusive; in fact they can be complementary – an agent utilizing MCP for data access could then use A2A to share insights gained with other agents.' search_query=None query_content=None section_content=None \n","\n","name='Conclusion' description='Summary of the key distinctions between MCP and A2A along with a table summarizing their core features.' research=False content='In conclusion, both MCP and A2A represent significant steps toward more interoperable and powerful AI systems.\\n\\n| Feature | Model Context Protocol (MCP) | Agent to Agent (A2A) |\\n|---|---|---|\\n| **Primary Focus** | Standardized access to external context | Inter-agent communication |\\n| **Functionality** | Middleware layer for tool/data interaction | Protocol for agent collaboration |\\n| **Key Benefit** | Context-aware AI applications | Seamless interoperability between agents |\\n| **Developed By** | Anthropic | Google |\\n\\nThe future of AI tooling likely involves integration of both protocols—leveraging MCP for robust context provision within individual agents alongside A2A for effective collaboration between them.' search_query=None query_content=None section_content=None \n","\n"]}],"source":["for section in report_sections:\n","    print(f'{section} \\n')"]},{"cell_type":"code","execution_count":234,"metadata":{"id":"00feeb50-0ffd-4a3a-9d53-ea2e192cf094","outputId":"492b931f-e5ae-4866-e45c-1991173db94b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744446350446,"user_tz":-540,"elapsed":11,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Thread: name='Introduction' description='A brief overview of Model Context Protocol (MCP) and Agent to Agent (A2A) protocols, highlighting their emergence in the field of AI tooling and their significance for interoperability.' research=False content='The rapid development of agentic AI necessitates standardized communication and context provision. Two key protocols addressing this need are Model Context Protocol (MCP) and Agent to Agent (A2A). MCP focuses on enabling AI models to access and utilize external context, while A2A focuses on facilitating communication between independent AI agents. Both aim to resolve limitations in current AI systems, but approach the problem from different angles. This report will detail each protocol and highlight their key differences.' search_query=None query_content=None section_content=None\n","Thread: name='Model Context Protocol (MCP) – Enabling Context-Aware AI' description=\"Detailed explanation of MCP's architecture, functionality, and purpose. Focus on how it bridges the gap between LLMs and external resources.\" research=False content='MCP is an open protocol designed to provide a standardized way for systems to offer context to AI models. It acts as a middleware layer, allowing models to retrieve and process external information from databases, files, and APIs. Unlike simply providing static data, MCP enables dynamic interaction with these resources. This is achieved through a client-server model where AI agents (clients) interact with MCP servers providing access to tools and data. The core purpose of MCP is to enable richer responses, deeper integrations, and more intelligent applications by providing models with real-time information and the ability to execute complex tasks autonomously.' search_query=None query_content=None section_content=None\n","Thread: name='Agent to Agent (A2A) – Facilitating Agent Collaboration' description=\"Detailed explanation of A2A's architecture, functionality, and purpose. Focus on how it enables communication between different AI agents.\" research=False content=\"A2A is an open protocol developed by Google intended to enable seamless collaboration between AI agents across diverse frameworks and vendors. It establishes a common language for agents irrespective of their underlying technology. A2A operates via an HTTP endpoint implemented by A2A Servers which expose methods defined in a JSON specification. The key goal is interoperability – allowing agents built with different frameworks (e.g., LangGraph, Google ADK) to communicate effectively without requiring custom integrations or understanding of each other's internal workings.\" search_query=None query_content=None section_content=None\n","Thread: name='MCP vs. A2A: Key Differences & Synergies' description='Direct comparison of MCP and A2A protocols highlighting their distinct focuses: resource access vs. agent communication.' research=False content='While both protocols contribute to advancing agentic AI capabilities, they address different aspects of the challenge. **MCP** primarily concerns itself with *how* a model accesses external resources – standardizing the interface for tools, databases & APIs. It equips an agent with the ability to *utilize* information effectively. **A2A**, on the other hand, focuses on *who* an agent communicates with – establishing a common language for agent-to-agent interactions across different ecosystems. Essentially, A2A enables agents to *collaborate*, while MCP empowers them with contextual awareness. They aren’t mutually exclusive; in fact they can be complementary – an agent utilizing MCP for data access could then use A2A to share insights gained with other agents.' search_query=None query_content=None section_content=None\n","Thread: name='Conclusion' description='Summary of the key distinctions between MCP and A2A along with a table summarizing their core features.' research=False content='In conclusion, both MCP and A2A represent significant steps toward more interoperable and powerful AI systems.\\n\\n| Feature | Model Context Protocol (MCP) | Agent to Agent (A2A) |\\n|---|---|---|\\n| **Primary Focus** | Standardized access to external context | Inter-agent communication |\\n| **Functionality** | Middleware layer for tool/data interaction | Protocol for agent collaboration |\\n| **Key Benefit** | Context-aware AI applications | Seamless interoperability between agents |\\n| **Developed By** | Anthropic | Google |\\n\\nThe future of AI tooling likely involves integration of both protocols—leveraging MCP for robust context provision within individual agents alongside A2A for effective collaboration between them.' search_query=None query_content=None section_content=None\n","실행 시간: 0.00초\n"]}],"source":["start_time = time.time()\n","threads = []\n","\n","for section in report_sections:\n","    t = threading.Thread(target=web_search_worker, args=(section, True,))\n","    threads.append(t)\n","    t.start()\n","\n","for thread in threads:\n","    thread.join()\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","\n","print(f\"실행 시간: {execution_time:.2f}초\")"]},{"cell_type":"code","execution_count":235,"metadata":{"id":"2c4cc1aa-e849-4cdd-9b7a-96fcc7ac5918","outputId":"5f81971c-4605-4eb7-bdd5-def2287ef911","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744446351001,"user_tz":-540,"elapsed":10,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["section.name: Introduction\n","section.description: A brief overview of Model Context Protocol (MCP) and Agent to Agent (A2A) protocols, highlighting their emergence in the field of AI tooling and their significance for interoperability.\n","section.search_query: \n","None\n","section.section_content: \n","None\n","====================================\n","section.name: Model Context Protocol (MCP) – Enabling Context-Aware AI\n","section.description: Detailed explanation of MCP's architecture, functionality, and purpose. Focus on how it bridges the gap between LLMs and external resources.\n","section.search_query: \n","None\n","section.section_content: \n","None\n","====================================\n","section.name: Agent to Agent (A2A) – Facilitating Agent Collaboration\n","section.description: Detailed explanation of A2A's architecture, functionality, and purpose. Focus on how it enables communication between different AI agents.\n","section.search_query: \n","None\n","section.section_content: \n","None\n","====================================\n","section.name: MCP vs. A2A: Key Differences & Synergies\n","section.description: Direct comparison of MCP and A2A protocols highlighting their distinct focuses: resource access vs. agent communication.\n","section.search_query: \n","None\n","section.section_content: \n","None\n","====================================\n","section.name: Conclusion\n","section.description: Summary of the key distinctions between MCP and A2A along with a table summarizing their core features.\n","section.search_query: \n","None\n","section.section_content: \n","None\n","====================================\n"]}],"source":["for section in report_sections:\n","    print(\"section.name: \" + section.name)\n","    print(\"section.description: \" + section.description)\n","    print(\"section.search_query: \")\n","    print(section.search_query)\n","    print(\"section.section_content: \")\n","    print(section.section_content)\n","    print(\"====================================\")"]},{"cell_type":"code","execution_count":244,"metadata":{"id":"b5e3bd5d-e11d-4d66-ad4a-e865e5cc66a4","outputId":"e587abb9-3cb6-4a8d-efda-8cf4f9d3da89","colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"status":"ok","timestamp":1744446417368,"user_tz":-540,"elapsed":71,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<p>The rapid development of agentic AI necessitates standardized communication and context provision. Two key protocols addressing this need are Model Context Protocol (MCP) and Agent to Agent (A2A). MCP focuses on enabling AI models to access and utilize external context, while A2A focuses on facilitating communication between independent AI agents. Both aim to resolve limitations in current AI systems, but approach the problem from different angles. This report will detail each protocol and highlight their key differences.</p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<p>MCP is an open protocol designed to provide a standardized way for systems to offer context to AI models. It acts as a middleware layer, allowing models to retrieve and process external information from databases, files, and APIs. Unlike simply providing static data, MCP enables dynamic interaction with these resources. This is achieved through a client-server model where AI agents (clients) interact with MCP servers providing access to tools and data. The core purpose of MCP is to enable richer responses, deeper integrations, and more intelligent applications by providing models with real-time information and the ability to execute complex tasks autonomously.</p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<p>A2A is an open protocol developed by Google intended to enable seamless collaboration between AI agents across diverse frameworks and vendors. It establishes a common language for agents irrespective of their underlying technology. A2A operates via an HTTP endpoint implemented by A2A Servers which expose methods defined in a JSON specification. The key goal is interoperability – allowing agents built with different frameworks (e.g., LangGraph, Google ADK) to communicate effectively without requiring custom integrations or understanding of each other's internal workings.</p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<p>While both protocols contribute to advancing agentic AI capabilities, they address different aspects of the challenge. <strong>MCP</strong> primarily concerns itself with <em>how</em> a model accesses external resources – standardizing the interface for tools, databases &amp; APIs. It equips an agent with the ability to <em>utilize</em> information effectively. <strong>A2A</strong>, on the other hand, focuses on <em>who</em> an agent communicates with – establishing a common language for agent-to-agent interactions across different ecosystems. Essentially, A2A enables agents to <em>collaborate</em>, while MCP empowers them with contextual awareness. They aren’t mutually exclusive; in fact they can be complementary – an agent utilizing MCP for data access could then use A2A to share insights gained with other agents.</p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<p>In conclusion, both MCP and A2A represent significant steps toward more interoperable and powerful AI systems.</p>\n<p>| Feature | Model Context Protocol (MCP) | Agent to Agent (A2A) |\n|---|---|---|\n| <strong>Primary Focus</strong> | Standardized access to external context | Inter-agent communication |\n| <strong>Functionality</strong> | Middleware layer for tool/data interaction | Protocol for agent collaboration |\n| <strong>Key Benefit</strong> | Context-aware AI applications | Seamless interoperability between agents |\n| <strong>Developed By</strong> | Anthropic | Google |</p>\n<p>The future of AI tooling likely involves integration of both protocols—leveraging MCP for robust context provision within individual agents alongside A2A for effective collaboration between them.</p>"},"metadata":{}}],"source":["import markdown\n","from IPython.display import display, HTML, Markdown\n","\n","for section in report_sections:\n","    if section.content:\n","        display(Markdown(markdown.markdown(section.content)))\n","    # display(Markdown(section.section_content))"]},{"cell_type":"code","source":[],"metadata":{"id":"TZ4kxBmg_yW-"},"execution_count":null,"outputs":[]}]}