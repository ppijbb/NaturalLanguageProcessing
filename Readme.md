# 자연어 처리 노트북
---
<b>Directories</b>
* gpt generations
  - gpt-3 text generation
* Text Analysis
  - 기본적인 텍스트 분석 기법
* Transformers
  - pytorch Transformer 구조 모델 연구
* rlhl
  - gpt 강화학습 연구
* LLM parallelism
  - 병렬처리를 통한 LLM 서비스 연구
* LLM Benchmark
  - LLM task 별 평가 방법 연구
* PEFT
  - LLM Finetuning Method 연구

---

> Reading Pappers \
[DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)\
[UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPLICIT REWARD FUNCTION](https://openreview.net/pdf?id=xoXn62FzD0)\
[SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/pdf/2405.14734)\
[Coconut: Chain of Continuous Thought](https://arxiv.org/pdf/2412.06769v1)\
[DAPO (Dynamic Accuracy-based Proximal Optimization)](https://arxiv.org/pdf/2503.14476)\
[Param∆ FOR DIRECT WEIGHT MIXING: POST-TRAIN LARGE LANGUAGE MODEL AT ZERO COST](https://arxiv.org/pdf/2504.21023)\
[Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/pdf/2504.12329)\
[Evaluation of Architecture, Training/Tuning, and Inference Efficiency](https://arxiv.org/pdf/2505.13840)\
[Evaluation Metrics for Various Models (AAR, AEM, ARR)](https://aclanthology.org/2022.findings-acl.48.pdf)\
[Latent Typing and Reconstruction Results Analysis of Input Tokens](https://arxiv.org/pdf/2210.12582)\
[Experimental Details and Evaluation Metrics for LLM Training](https://arxiv.org/pdf/2306.08543)\
[Document Excerpt (Content Unclear)](https://arxiv.org/pdf/2403.07816)\
[Analysis of Mixture-of-Experts (MoE) Models](https://arxiv.org/pdf/2409.12136)\
[3D Shape Reconstruction with Various Architectures and Activations](https://arxiv.org/pdf/2410.21643)\
[Reference List Excerpt](https://arxiv.org/pdf/2502.03387)\
[SPCT Pipeline and GRM Derivation](https://arxiv.org/pdf/2504.02495)\
[TTRL Method and Math Model Performance Comparison](https://arxiv.org/pdf/2504.16084)\
[Continuous Thought Machines](https://arxiv.org/pdf/2505.05522)

> Read Pappers \
[CoMPM: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation](https://arxiv.org/pdf/2108.11626.pdf) \
[Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods](https://arxiv.org/pdf/2207.13919.pdf)\
[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf)\
[Few-Shot Parameter-Efficient Fine-Tuning is Betterand Cheaper than In-Context Learning](https://arxiv.org/pdf/2205.05638.pdf)\
[Few-shot Natural Language Generation for Task-Oriented Dialog](https://aclanthology.org/2020.findings-emnlp.17.pdf)\
[Towards a Human-like Open-Domain Chatbot](https://arxiv.org/pdf/2001.09977.pdf)\
[Implicit Unlikelihood Training: Improving Neural Text Generation withReinforcement Learning](https://arxiv.org/pdf/2101.04229.pdf)\
[Few-Shot Text Generation with Pattern-Exploiting Training](https://aclanthology.org/2021.emnlp-main.32.pdf)\
[Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295.pdf)\
[Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods](https://arxiv.org/pdf/2207.13919.pdf)\
[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)\
[Generative Language Models for Paragraph-Level Question Generation](https://arxiv.org/pdf/2210.03992v3.pdf)\
[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593v2.pdf) \
[GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL](https://arxiv.org/pdf/2210.02414v1.pdf) \
[Ankh : Optimized Protein Language Model Unlocks General-Purpose Modelling](https://arxiv.org/ftp/arxiv/papers/2301/2301.06568.pdf)\
[HuaTuo (华驼): Tuning LLaMA Model with Chinese Medical Knowledge](https://arxiv.org/pdf/2304.06975v1.pdf)\
[Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/pdf/2403.13187)\
[AVATAR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval](https://arxiv.org/pdf/2406.11200v2)\
[MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/pdf/2402.14905)\
[Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)
