{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfahULH_8nv3"
   },
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCMEDZSiqGyr",
    "outputId": "a61cd107-6b82-4902-d4bf-e572e1a31812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/bionic.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... gpg: WARNING: unsafe ownership on homedir '/home/ubuntu/.gnupg'\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 188 not upgraded.\n",
      "Need to get 6,800 kB of archives.\n",
      "After this operation, 15.3 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 3.1.2 [6,800 kB]\n",
      "Fetched 6,800 kB in 1s (8,723 kB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 121191 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.1.2_amd64.deb ...\n",
      "Unpacking git-lfs (3.1.2) ...\n",
      "Setting up git-lfs (3.1.2) ...\n",
      "Git LFS initialized.\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# Install\n",
    "!sudo apt-get install git-lfs\n",
    "!git lfs install\n",
    "!git config --global user.email \"ppijbb@gmail.com\"\n",
    "!git config --global user.name \"gunulhona\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9XUxVUmUdY_"
   },
   "outputs": [],
   "source": [
    "# # kowiki 데이터 가져오기\n",
    "!python -m wikiextractor.WikiExtractor /data/kevin.jung/kowiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "# # 나무위키 데이터 mediafire 링크에서 받아온 주소\n",
    "!py7zr x /data/kevin.jung/37d51b7a67b1ab2e3ebf93eae75ee90a.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ijson in ./anaconda3/envs/nlp_env/lib/python3.8/site-packages (3.1.4)\n"
     ]
    }
   ],
   "source": [
    "!anaconda3/envs/nlp_env/bin/pip install ijson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5M6I0ySvfsO"
   },
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "hello world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efbb001a1501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "20f1b708e6a94e08b54d474da294769c",
      "8a3e1e1673fd49a5b19e73076f6caf86",
      "7735a15195c54e4f89d3c295e8142d3d",
      "fe5e3980462b4607911e8809ab834d99",
      "eaaa639063f8480eaa025ff4edabf69c",
      "a7c43d92ac51449b97f3dad47d6df86b",
      "31f2935ffebd457dba150bb6e5aab07d",
      "f7f6317cc3864040a7300a21193a3d34",
      "103522cfa1604fad8059fc4fc93179d8",
      "52e37c34bb2f454eab217e8c06c153e5",
      "8b7663502e7e4c87901cb5edf3221118"
     ]
    },
    "id": "iW1It1TJU_Ce",
    "outputId": "d03e9615-bb07-4137-be16-68b05f9753ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `kcbert` is already installed at /content/kcbert/kcbert-train.tar.gzaa\n",
      "[Korpora] Corpus `kcbert` is already installed at /content/kcbert/kcbert-train.tar.gzab\n",
      "[Korpora] Corpus `kcbert` is already installed at /content/kcbert/kcbert-train.tar.gzac\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f1b708e6a94e08b54d474da294769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title KcBERT 댓글 데이터\n",
    "#@markdown 바꾸는데 너무 오래걸림\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "Korpora.fetch(\"kcbert\",\n",
    "              root_dir=\"/content/\")\n",
    "\n",
    "dummy = pd.DataFrame(data=None,columns=[\"pattern\",\"label\"])\n",
    "with open(\"/content/kcbert/20190101_20200611_v2.txt\",\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        dummy = dummy.append({\"pattern\":\"\",\"label\":line},ignore_index=True)\n",
    "dummy.to_csv(\"kcbert.tsv\",index=False,sep=\"\\t\")\n",
    "del dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DFXMHwY8sEo0"
   },
   "outputs": [],
   "source": [
    "#@title 나무위키 데이터\n",
    "import pandas as pd\n",
    "import ijson\n",
    "from multiprocessing import Pool\n",
    "from namuwiki.extractor import extract_text\n",
    "\n",
    "capture_values = [\n",
    "    (\"item.namespace\", \"string\"),\n",
    "    (\"item.title\", \"string\"),\n",
    "    (\"item.text\", \"string\")\n",
    "]\n",
    "\n",
    "def parse_namuwiki_json(limit = -1, debug=False):\n",
    "  i = 0\n",
    "  doc = {}\n",
    "  with open(\"/data/kevin.jung/namuwiki_20210301.json\",\"r\") as f:\n",
    "    for prefix, event, value in ijson.parse(f):\n",
    "      if debug:\n",
    "        print(prefix, event, value)\n",
    "      if (prefix, event) in capture_values:\n",
    "        doc[prefix[5:]] = value\n",
    "      if (prefix, event, value) == (\"item\", \"end_map\", None):\n",
    "        yield doc    \n",
    "        doc = {}\n",
    "        i += 1\n",
    "        if limit > 0 and i >= limit:\n",
    "          break\n",
    "\n",
    "temp = [[],[]]          \n",
    "for doc in parse_namuwiki_json(debug=False):\n",
    "    temp[0]+=[doc[\"title\"]]\n",
    "    temp[1]+=[extract_text(doc['text'])]\n",
    "\n",
    "pd.DataFrame({\"pattern\":temp[0],\"label\":temp[1]}).to_csv(\"/data/kevin.jung/namuwiki.tsv\",index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOmTBiaXsCt6",
    "outputId": "96e70847-859c-435f-dcd2-7ef2412a065d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]<ipython-input-10-e2c075650f1b>:17: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  for document in root.getchildren():\n",
      "100%|██████████| 9/9 [02:15<00:00, 15.03s/it]\n"
     ]
    }
   ],
   "source": [
    "#@title ko Wiki 데이터\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "filepath = \"/data/kevin.jung/text/\"\n",
    "file_list = list(map(lambda x: filepath + x, os.listdir(filepath)))\n",
    "document_items=[[],[]]\n",
    "for path in tqdm(file_list):\n",
    "    for i in [file for file in os.listdir(path) if file.startswith('wiki')]:\n",
    "        with open(f\"{path}/{i}\",'r') as f:\n",
    "            root = ElementTree.fromstringlist(\n",
    "                    \"<root>\"+\"\".join(f.readlines())+\"</root>\"\n",
    "                )\n",
    "            for document in root.getchildren():\n",
    "                document_items[0] += [document.attrib[\"title\"]]\n",
    "                document_items[1] += [document.text.replace(\"\\n\\n\",\"\\n\")[1:-1]]\n",
    "pd.DataFrame({\"pattern\":document_items[0], \"label\":document_items[1]}).to_csv(\"/data/kevin.jung/kowiki.tsv\",index=False,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5Ov6O5TvhpJ",
    "outputId": "ea4aed7b-6dc7-4562-d686-249d53aacd0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 길이 : 5252850\n",
      "Train.csv 파일 크기 : 1.5175 GB\n",
      "Test.csv 파일 크기 : 0.3789 GB\n",
      "Dev.csv 파일 크기 : 0.0191 GB\n",
      "Dev_s.csv 파일 크기 : 0.0019 GB\n"
     ]
    }
   ],
   "source": [
    "#@title 데이터 통합\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DEFAULT_PATH = \"/data/kevin.jung\"\n",
    "filepath = DEFAULT_PATH+\"/PET data/\"\n",
    "file_list = os.listdir(filepath)\n",
    "try:\n",
    "    df = pd.concat([pd.read_csv(\"kowiki.tsv\",sep=\"\\t\").dropna(),\n",
    "                pd.read_csv(\"namuwiki.tsv\",sep=\"\\t\").dropna()],\n",
    "               ignore_index=True)\n",
    "except:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "for i in [file for file in file_list if file.endswith('.csv')]:\n",
    "    try:\n",
    "        data = pd.read_csv(filepath + i)\n",
    "    except:\n",
    "        continue\n",
    "    df = pd.concat([df,data])\n",
    "data = df.dropna().reset_index(drop = True)               \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(f\"전체 길이 : {len(data)}\")\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "_,dev =train_test_split(data, test_size=0.01)\n",
    "_,dev_small =train_test_split(dev, test_size=0.1)\n",
    "train.to_csv(\"/data/kevin.jung/Train.csv\",index=None)\n",
    "print('Train.csv 파일 크기 : %.4f GB'%(os.path.getsize(\"/data/kevin.jung/Train.csv\")/(1024.0 * 1024.0 * 1000.0)))\n",
    "test.to_csv(\"/data/kevin.jung/Test.csv\",index=None)\n",
    "print('Test.csv 파일 크기 : %.4f GB'%(os.path.getsize(\"/data/kevin.jung/Test.csv\")/(1024.0 * 1024.0 * 1000.0)))\n",
    "dev.to_csv(\"/data/kevin.jung/Dev.csv\",index=None)\n",
    "print('Dev.csv 파일 크기 : %.4f GB'%(os.path.getsize(\"/data/kevin.jung/Dev.csv\")/(1024.0 * 1024.0 * 1000.0)))\n",
    "dev_small.to_csv(\"/data/kevin.jung/Dev_s.csv\",index=None)\n",
    "print('Dev_s.csv 파일 크기 : %.4f GB'%(os.path.getsize(\"/data/kevin.jung/Dev_s.csv\")/(1024.0 * 1024.0 * 1000.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6pV_73CrXUS"
   },
   "source": [
    "# GPU\n",
    "---\n",
    "tesla t4(16gb) * 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/config.json\n",
    "{ \n",
    "  \"activation_dropout\": 0.0,\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 2,\n",
    "  \"classifier_dropout\": 0.0,\n",
    "  \"d_model\": 768,\n",
    "  \"decoder_attention_heads\": 12,\n",
    "  \"decoder_ffn_dim\": 4096,\n",
    "  \"decoder_layerdrop\": 0.0,\n",
    "  \"decoder_layers\": 6,\n",
    "  \"decoder_start_token_id\": 2,\n",
    "  \"dropout\": 0.1,\n",
    "  \"encoder_attention_heads\": 24,\n",
    "  \"encoder_ffn_dim\": 4096,\n",
    "  \"encoder_layerdrop\": 0.0,\n",
    "  \"encoder_layers\": 3,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"forced_eos_token_id\": 2,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\",\n",
    "    \"2\": \"LABEL_2\"\n",
    "  },\n",
    "  \"init_std\": 0.02,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "    \"LABEL_2\": 2\n",
    "  },\n",
    "  \"max_position_embeddings\": 1026,\n",
    "  \"model_type\": \"bart\",\n",
    "  \"pad_token_id\": 3,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"scale_embedding\": false,\n",
    "  \"transformers_version\": \"4.14.0\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bart Model Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuAXLwuI1Mfp",
    "outputId": "a81171a1-ae17-4cf5-c229-aa4cf8eb90a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/bart_custom.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/bart_custom.py\n",
    "# -*- coding: utf-8 -*- \n",
    "#@title Custom Bart 파일 저장\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.file_utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_end_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    MaskedLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqQuestionAnsweringModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "from transformers.models.bart.modeling_bart import *\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bart.configuration_bart import BartConfig\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"facebook/bart-large\"\n",
    "_CONFIG_FOR_DOC = \"BartConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n",
    ")\n",
    "class BartForMaskedLM(BartPretrainedModel):\n",
    "    base_model_prefix = \"model\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = BartModel(config)\n",
    "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.model.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
    "        self._resize_final_logits_bias(new_num_tokens)\n",
    "        return new_embeddings\n",
    "\n",
    "    def _resize_final_logits_bias(self, new_num_tokens: int)->None:\n",
    "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
    "        if new_num_tokens <= old_num_tokens:\n",
    "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
    "        else:\n",
    "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
    "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
    "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bart CausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AyUCVYEPCxH",
    "outputId": "189bccd5-8d6e-41da-831c-4b7794577660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/kobart_mlm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/kobart_mlm.py\n",
    "# -*- coding: utf-8 -*- \n",
    "#@title kobart_mlm (causal mlm) 파일 저장\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (BartForConditionalGeneration,\n",
    "                          BartForCausalLM,\n",
    "                          BartModel, BartConfig,\n",
    "                          BartForSequenceClassification,\n",
    "                          PreTrainedTokenizerFast)\n",
    "from transformers.optimization import (AdamW, get_cosine_schedule_with_warmup,\n",
    "                                       get_cosine_with_hard_restarts_schedule_with_warmup)\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader\n",
    "\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='KoBART Chit-Chat')\n",
    "\n",
    "parser.add_argument('--checkpoint_path',\n",
    "                    type=str,\n",
    "                    help='checkpoint path')\n",
    "\n",
    "parser.add_argument('--chat',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='response generation on given user input')\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class ArgsBase():\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--train_file',\n",
    "                            type=str,\n",
    "                            default='/dev_t.csv',\n",
    "                            help='train file')\n",
    "\n",
    "        parser.add_argument('--val_file',\n",
    "                            type=str,\n",
    "                            default='/dev_v.csv',\n",
    "                            help='val file')\n",
    "\n",
    "        parser.add_argument('--test_file',\n",
    "                            type=str,\n",
    "                            default='/dev_v.csv',\n",
    "                            help='test file')\n",
    "\n",
    "        parser.add_argument('--tokenizer_path',\n",
    "                            type=str,\n",
    "                            default='tokenizer',\n",
    "                            help='tokenizer')\n",
    "        \n",
    "        parser.add_argument('--batch_size',\n",
    "                            type=int,\n",
    "                            default=4,\n",
    "                            help='')\n",
    "        parser.add_argument('--max_seq_len',\n",
    "                            type=int,\n",
    "                            default=1024,\n",
    "                            help='max seq len')\n",
    "        return parser\n",
    "\n",
    "\n",
    "class Pet_Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 file_root_path=None,\n",
    "                 tokenizer_path=None,\n",
    "                 max_seq_len=512):\n",
    "        self.filepath = file_root_path\n",
    "        self.data = pd.read_csv(self.filepath).dropna()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "        self.masking_start, self.masking_end =self.tokenizer.encode(\"[]\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.__len__()\n",
    "\n",
    "    def _encode(self, text):\n",
    "        tokens = [self.tokenizer.bos_token] + self.tokenizer.tokenize(text) + [self.tokenizer.eos_token]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1]*len(input_ids)\n",
    "        if len(input_ids) < self.max_seq_len:\n",
    "            while len(input_ids)<self.max_seq_len:\n",
    "                input_ids+=[self.tokenizer.pad_token_id]\n",
    "                attention_mask+=[0]\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_seq_len-1]+[self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def _labeling(self,label):\n",
    "        tokens = self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n",
    "        label_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(label_ids) < self.max_seq_len:\n",
    "            while len(label_ids)<self.max_seq_len:\n",
    "                label_ids+=[-100]\n",
    "        else:\n",
    "            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n",
    "        return label_ids\n",
    "\n",
    "    def _masking(self, tokens):\n",
    "        masked = tokens\n",
    "        mask_idx=[]\n",
    "        while self.masking_start in masked and self.masking_end in masked:\n",
    "            start_idx = masked.index(self.masking_start)\n",
    "            end_idx = masked.index(self.masking_end)+1\n",
    "            if start_idx < end_idx:\n",
    "                mask_idx +=[[start_idx,end_idx]]\n",
    "                masked[start_idx:end_idx] = [self.tokenizer.mask_token_id]*len(masked[start_idx:end_idx])\n",
    "            else: break\n",
    "        return masked, mask_idx\n",
    "\n",
    "    def _label_masking(self, tokens, idx):\n",
    "        masked = np.array(tokens)\n",
    "        index,position =[],[]\n",
    "        for i in idx:\n",
    "            index+=range(i[0],i[1])\n",
    "        for n,x in enumerate(tokens):\n",
    "            position += [n not in index]\n",
    "        masked[position] = [-100]\n",
    "        return masked#.tolist()\n",
    "\n",
    "class Masked_Dataset(Pet_Dataset):\n",
    "    def _random_masking(self,input, mask=None, ratio=0.15):\n",
    "        if mask is None:\n",
    "            mask = self.tokenizer.mask_token_id\n",
    "        input = np.array(input)\n",
    "        rand = np.random.rand(input.size)\n",
    "        mask_arr = (rand < ratio) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n",
    "        input[mask_arr.nonzero()] = mask\n",
    "        return input#.tolist()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        record = self.data.iloc[index]\n",
    "        pattern, label = record[\"pattern\"], record[\"label\"]\n",
    "        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n",
    "        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n",
    "        labels = self._labeling(pattern+label)\n",
    "        encoder_input_ids,mask_idx = self._masking(encoder_input_ids)\n",
    "        encoder_input_ids = self._random_masking(encoder_input_ids)\n",
    "\n",
    "        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n",
    "                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n",
    "                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n",
    "                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n",
    "                \"labels\":np.array(labels,dtype=np.int_)}\n",
    "\n",
    "class Permutation_Dataset(Pet_Dataset):\n",
    "    def _random_rotation(self, input):\n",
    "        input = np.array(input)\n",
    "        start_idx = np.where(input==self.tokenizer.bos_token_id)[0][0]\n",
    "        end_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n",
    "        np.random.shuffle(input[start_idx:end_idx])\n",
    "        return input#.tolist()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        record = self.data.iloc[index]\n",
    "        pattern, label = record[\"pattern\"], record[\"label\"]\n",
    "        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n",
    "        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n",
    "        labels = self._labeling(pattern+label)\n",
    "        encoder_input_ids = self._random_rotation(encoder_input_ids)\n",
    "\n",
    "        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n",
    "                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n",
    "                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n",
    "                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n",
    "                \"labels\":np.array(labels,dtype=np.int_)}\n",
    "\n",
    "class Deletion_Dataset(Pet_Dataset):\n",
    "    def _random_deletion(self,input, ratio=0.15):\n",
    "        input = np.array(input)\n",
    "        eos_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n",
    "        rand = np.random.rand(input[:eos_idx].size)\n",
    "        rand = np.append(rand,[1.]*input[eos_idx:].size)\n",
    "        del_arr = (rand < ratio) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n",
    "        input = np.delete(input, del_arr.nonzero())\n",
    "        return np.int_(np.append(input,[self.tokenizer.pad_token_id]*del_arr.nonzero()[0].size))#.tolist()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        record = self.data.iloc[index]\n",
    "        pattern, label = record[\"pattern\"], record[\"label\"]\n",
    "        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n",
    "        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n",
    "        labels = self._labeling(pattern+label)\n",
    "        encoder_input_ids = self._random_deletion(encoder_input_ids)\n",
    "\n",
    "        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n",
    "                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n",
    "                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n",
    "                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n",
    "                \"labels\":np.array(labels,dtype=np.int_)}\n",
    "\n",
    "class Infilling_Dataset(Pet_Dataset):\n",
    "    def _random_infilling(self,input, ratio=0.15, l=3):\n",
    "        input = np.array(input)\n",
    "        eos_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n",
    "        text_range = input[1:eos_idx]\n",
    "        poi = poisson(l).pmf(text_range) > ratio\n",
    "        infill = np.where(poi, np.array([self.tokenizer.mask_token_id]+[-100]*(poi.size-1)), text_range)\n",
    "        infill = np.append(input[0], np.delete(infill,np.where(infill == -100)))\n",
    "        infill = np.append(infill, [self.tokenizer.pad_token_id]*(np.count_nonzero(poi)-1))\n",
    "        infill = np.append(infill, input[eos_idx:])\n",
    "        if poi.max() == poi.min():\n",
    "            if not len(infill) > len(input):\n",
    "                infill = np.insert(infill, np.random.choice(len(text_range)), \n",
    "                                   self.tokenizer.mask_token_id)[:len(input)]                    \n",
    "            else:\n",
    "                _size = int(len(text_range)*ratio)\n",
    "                infill = np.insert(infill, np.random.choice(len(text_range),\n",
    "                                                            size=_size), self.tokenizer.mask_token_id)[:len(input)-1]\n",
    "                infill = np.append(infill, [self.tokenizer.eos_token_id])\n",
    "        if infill.size == (self.max_seq_len-1):\n",
    "            infill = np.append(infill, input[-1])\n",
    "        return np.int_(infill)#.tolist()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        record = self.data.iloc[index]\n",
    "        pattern, label = record[\"pattern\"], record[\"label\"]\n",
    "        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n",
    "        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n",
    "        labels = self._labeling(pattern+label)\n",
    "        encoder_input_ids = self._random_infilling(encoder_input_ids)\n",
    "\n",
    "        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n",
    "                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n",
    "                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n",
    "                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n",
    "                \"labels\":np.array(labels,dtype=np.int_)}\n",
    "\n",
    "class ChatDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 train_file,\n",
    "                 val_file,\n",
    "                 test_file, \n",
    "                 tokenizer_path,\n",
    "                 max_seq_len=1024,\n",
    "                 batch_size=4,\n",
    "                 num_workers=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.train_file_path = train_file\n",
    "        self.val_file_path = val_file\n",
    "        self.test_file_path = test_file\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--num_workers',\n",
    "                            type=int,\n",
    "                            default=8,\n",
    "                            help='num of worker for dataloader')\n",
    "        return parser\n",
    "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
    "\n",
    "    def _load_multiple(self, file_path, shuffle):\n",
    "        return {\n",
    "            \"Deletion Data\":DataLoader(Deletion_Dataset(file_path, self.tokenizer_path, self.max_seq_len),\n",
    "                          pin_memory=True,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=shuffle),\n",
    "            \"Permutation Data\":DataLoader(Permutation_Dataset(file_path, self.tokenizer_path, self.max_seq_len),\n",
    "                          pin_memory=True,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=shuffle),\n",
    "            \"Masked Data\":DataLoader(Masked_Dataset(file_path, self.tokenizer_path, self.max_seq_len),\n",
    "                          pin_memory=True,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=shuffle),\n",
    "            \"Infilling Data\":DataLoader(Infilling_Dataset(file_path, self.tokenizer_path, self.max_seq_len),\n",
    "                          pin_memory=True,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, \n",
    "                          shuffle=shuffle)\n",
    "        }\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # split dataset\n",
    "        # self.train = Deletion_Dataset(self.train_file_path, self.tokenizer_path, self.max_seq_len)\n",
    "\n",
    "        # self.val = Deletion_Dataset(self.val_file_path, self.tokenizer_path, self.max_seq_len)\n",
    "\n",
    "        # self.test = Deletion_Dataset(self.test_file_path, self.tokenizer_path, self.max_seq_len)\n",
    "\n",
    "        self.train = self._load_multiple(file_path=self.train_file_path, shuffle=True)\n",
    "\n",
    "        self.val = self._load_multiple(file_path=self.val_file_path, shuffle=False)\n",
    "\n",
    "        self.test = self._load_multiple(file_path=self.test_file_path, shuffle=False)\n",
    "\n",
    "    # def train_dataloader(self):\n",
    "    #     return DataLoader(self.train,\n",
    "    #                        batch_size=self.batch_size,\n",
    "    #                        num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.test,\n",
    "    #                      batch_size=self.batch_size,\n",
    "    #                      num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test,\n",
    "    #                       batch_size=self.batch_size,\n",
    "    #                       num_workers=self.num_workers, shuffle=False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return CombinedLoader(self.train, mode=\"max_size_cycle\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return CombinedLoader(self.val, mode=\"max_size_cycle\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return CombinedLoader(self.test, mode=\"max_size_cycle\")\n",
    "\n",
    "class Base(pl.LightningModule):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super(Base, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        # add model specific args\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "\n",
    "        parser.add_argument('--batch-size',\n",
    "                            type=int,\n",
    "                            default=128,\n",
    "                            help='batch size for training (default: 4)')\n",
    "\n",
    "        parser.add_argument('--lr',\n",
    "                            type=float,\n",
    "                            default=5e-8,\n",
    "                            help='The initial learning rate')\n",
    "\n",
    "        parser.add_argument('--warmup_ratio',\n",
    "                            type=float,\n",
    "                            default=0.1,\n",
    "                            help='warmup ratio')\n",
    "\n",
    "        parser.add_argument('--model_path',\n",
    "                            type=str,\n",
    "                            default=None,\n",
    "                            help='kobart model path')\n",
    "        return parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.lr, correct_bias=False)\n",
    "        # warm up lr\n",
    "        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * \\\n",
    "            (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n",
    "        data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n",
    "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
    "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
    "        logging.info(f'num_train_steps : {num_train_steps}')\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
    "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
    "        scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_train_steps,\n",
    "            num_cycles=1)\n",
    "        lr_scheduler = {'scheduler': scheduler, \n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "class KoBARTConditionalGeneration(Base):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super(KoBARTConditionalGeneration, self).__init__(hparams, **kwargs)\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.hparams.tokenizer_path)\n",
    "        # self.model = BartForConditionalGeneration.from_pretrained(self.hparams.model_path) if self.hparams.model_path \\\n",
    "        #      else BartForConditionalGeneration(BartConfig.from_json_file(\"trainer/config.json\"))\n",
    "        self.model = BartForConditionalGeneration(BartConfig.from_json_file(\"trainer/config.json\"))\n",
    "        # self._resize_token_embeddings()\n",
    "        # self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        # self.model.share_memory().train()\n",
    "        self.model.config.eos_token_id = 1\n",
    "    @staticmethod\n",
    "    def save_hf_repo(tokenizer, model):\n",
    "        MODEL_SAVE_REPO = 'Gunulhona/tb_pretrained'\n",
    "        HUGGINGFACE_AUTO_TOKEN = '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' \n",
    "        tokenizer.push_to_hub(MODEL_SAVE_REPO, \n",
    "        \t\t\t\t\t  use_temp_dir=True, \n",
    "        \t\t\t\t\t  use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
    "        model.cpu().push_to_hub(MODEL_SAVE_REPO, \n",
    "\t\t\t\t                use_temp_dir=True, \n",
    "\t\t\t\t                use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
    "\n",
    "    def _resize_token_embeddings(self):\n",
    "        tokens = {\"additional_special_tokens\":[\"<P01>\",\"<P02>\",\"<P03>\",\"<P04>\",\"<P05>\",\"<P06>\",\"<P07>\",\"<P08>\",\"<P09>\"]}\n",
    "        self.tokenizer.add_special_tokens(tokens)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outs = self.model(input_ids=inputs['input_ids'],\n",
    "                         attention_mask=inputs['attention_mask'],\n",
    "                         decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                         decoder_attention_mask=inputs['decoder_attention_mask'],\n",
    "                         labels=inputs['labels'],\n",
    "                         return_dict=True)\n",
    "        return outs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # outs = self(batch)\n",
    "        # loss = outs.loss\n",
    "        loss_list= torch.empty(0,device=self.model.device)\n",
    "        for loader in batch:\n",
    "            outputs = self(batch[loader])\n",
    "            loss_list = torch.cat((loss_list, outputs.loss.view(-1)))\n",
    "        loss = loss_list.sum()        \n",
    "        with torch.no_grad():\n",
    "            self.log('train_loss', loss.detach(), prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # outs = self(batch)\n",
    "        # loss = outs.loss\n",
    "        loss_list= torch.empty(0,device=self.model.device) #self.register_buffer(\"loss_list\", torch.empty(0))\n",
    "        for loader in batch:\n",
    "            outputs = self(batch[loader])\n",
    "            loss_list = torch.cat((loss_list, outputs.loss.view(-1)))\n",
    "        loss = loss_list.sum()\n",
    "        with torch.no_grad():     \n",
    "            self.log('val_loss', loss.detach(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # def training_epoch_end(self, training_step_outputs):\n",
    "        # all_preds = torch.stack(training_step_outputs)\n",
    "        # self.save_hf_repo(self.tokenizer, self.model)\n",
    "\n",
    "    def chat(self, text):\n",
    "        input_ids =  [self.tokenizer.bos_token_id] + self.tokenizer.encode(text) + [self.tokenizer.eos_token_id]\n",
    "        res_ids = self.model.generate(torch.tensor([input_ids]),\n",
    "                                      max_length=self.hparams.max_seq_len,\n",
    "                                      # num_beams=1,\n",
    "                                      # max_new_tokens=3,\n",
    "                                      # top_p=0.80,\n",
    "                                      top_k=200,\n",
    "                                      temperature=0.88,\n",
    "                                      do_sample=True,\n",
    "                                      length_penalty = 1.0,\n",
    "                                      repetition_penalty=1.0,\n",
    "                                      no_repeat_ngram_size=2,)\n",
    "        a = self.tokenizer.batch_decode(res_ids.tolist())\n",
    "        for i, x in enumerate(a):\n",
    "            print(f\"생성된 문장 {i+1} : \",x)\n",
    "        return a[0].replace('<s>', '').replace('</s>', '').replace('<usr>','').replace('<sys>','')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = Base.add_model_specific_args(parser)\n",
    "    parser = ArgsBase.add_model_specific_args(parser)\n",
    "    parser = ChatDataModule.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\")]).add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    logging.info(args)\n",
    "\n",
    "    model = KoBARTConditionalGeneration(args)\n",
    "\n",
    "    dm = ChatDataModule(args.train_file,\n",
    "                        args.val_file,\n",
    "                        args.test_file,\n",
    "                        args.tokenizer_path,\n",
    "                        max_seq_len=args.max_seq_len,\n",
    "                        num_workers=args.num_workers)\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                                       dirpath=args.default_root_dir,\n",
    "                                                       filename=\"best-checkpoint\",\n",
    "                                                       verbose=True,\n",
    "                                                       save_last=True,\n",
    "                                                       mode='min',\n",
    "                                                       save_top_k=1,)\n",
    "\n",
    "    # logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir, 'tb_logs'))\n",
    "    logger = pl_loggers.WandbLogger(name=\"test_pretraining\", project=\"pytorchlightning-test\", log_model=\"all\",)\n",
    "    if not args.chat and args.devices != 1:\n",
    "        wandb.require(\"service\")\n",
    "        wandb.setup()\n",
    "    lr_logger = pl.callbacks.LearningRateMonitor()\n",
    "    trainer = pl.Trainer.from_argparse_args(args, logger=logger,\n",
    "                                            callbacks=[checkpoint_callback, lr_logger],)\n",
    "    # trainer.fit(model, dm, )                                         \n",
    "    try:\n",
    "        trainer.fit(model, dm, )\n",
    "        if args.chat:\n",
    "            model.model.eval()\n",
    "            while 1:\n",
    "                q = input('prompt > ').strip()\n",
    "                if q == 'quit':\n",
    "                    break\n",
    "                elif q == 'save':\n",
    "                    save_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/\"\n",
    "                    torch.save(model.model.state_dict(),save_path)\n",
    "                    print(f'kobart-chat model.pth has saved at {save_path}')\n",
    "                result=model.chat(q)\n",
    "                print(\"Result > {}\".format(result))\n",
    "        else:\n",
    "            model.save_hf_repo(tokenizer=model.tokenizer, model=model.model)\n",
    "    except:\n",
    "        print(\"@ Training Done Save Model to huggingface repository... @\")\n",
    "        model.save_hf_repo(tokenizer=model.tokenizer, model=model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3GQoWMK4dcW",
    "outputId": "eed23ec8-8322-477e-c1d1-a131586702e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ubuntu/kevin.jung/training\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/training\n",
    "export CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\n",
    "export TOKENIZERS_PARALLELISM=\"0\"\n",
    "export PL_TORCH_DISTRIBUTED_BACKEND=\"nccl\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "export NCCL_IB_DISABLE=\"1\"\n",
    "export NCCL_P2P_DISABLE=\"1\"\n",
    "export MALLOC_MMAP_THRESHOLD_=\"331172164\"\n",
    "\n",
    "EXECUTEFILE=\"trainer/kobart_mlm.py\"\n",
    "BATCHSIZE=128\n",
    "LEARNINGRATE=5e-5\n",
    "EPOCHS=10\n",
    "TRAINFILE=\"/data/kevin.jung/Train.csv\"\n",
    "TESTFILE=\"/data/kevin.jung/Dev_s.csv\"\n",
    "VALFILE=\"/data/kevin.jung/Test.csv\"\n",
    "MODELPATH=\"Gunulhona/tb_pretrained\"\n",
    "TOKENIZERPATH=\"Gunulhona/tbbarttokenizer\"\n",
    "MAXSEQUENCELEN=300\n",
    "STRATEGY=\"DDP\"\n",
    "ACCELERATOR=\"gpu\"\n",
    "\n",
    "python $EXECUTEFILE\\\n",
    "  --gradient_clip_val 1.0\\\n",
    "  --max_epochs $EPOCHS\\\n",
    "  --lr $LEARNINGRATE\\\n",
    "  --precision 16\\\n",
    "  --batch-size $BATCHSIZE\\\n",
    "  --default_root_dir logs\\\n",
    "  --train_file $TRAINFILE\\\n",
    "  --test_file $TESTFILE\\\n",
    "  --val_file $TESTFILE\\\n",
    "  --model_path $MODELPATH\\\n",
    "  --tokenizer_path $TOKENIZERPATH\\\n",
    "  --max_seq_len $MAXSEQUENCELEN\\\n",
    "  --accelerator $ACCELERATOR\\\n",
    "  --devices 4\\\n",
    "  --num_workers 4\\\n",
    "  --strategy $STRATEGY\\\n",
    "  --progress_bar_refresh_rate 500\\\n",
    "  --terminate_on_nan True\\\n",
    "  --sync_batchnorm True\\\n",
    "  --profiler simple\\\n",
    "  --num_sanity_val_steps 5\\\n",
    "  --resume_from_checkpoint logs/last.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%system\n",
    "kill -9 `ps -ef | grep trainer/kobart_mlm.py | awk '{print $2}'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```{r, engine='bash', count_lines}\n",
    "nohup sh training > trainer.log&\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs_0420’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs_0420\n",
    "!touch logs_0420/best-checkpoint.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1_TtMnr1Hemp"
   },
   "outputs": [],
   "source": [
    "!mkdir logs\n",
    "!touch logs/best-checkpoint.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjC1OKhURE7_",
    "outputId": "9a44d551-aad1-425a-c465-1f27f5570646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"trainer/kobart_mlm.py\", line 26, in <module>\n",
      "    import wandb\n",
      "ModuleNotFoundError: No module named 'wandb'\n"
     ]
    }
   ],
   "source": [
    "#@title Training with TPU\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] =\"0\"\n",
    "\n",
    "!sh training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29-CShctCMWP",
    "outputId": "574f0253-bbe2-42b0-ec77-6bc191af25ca"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-62e1d7069125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title generate test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofiler_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python \"/content/kobart_mlm.py\"   --gradient_clip_val 1.0   --max_epochs 1   --lr 5e-7   --batch_size 512   --default_root_dir logs   --train_file Dev.csv   --val_file Dev.csv   --test_file Dev.csv   --model_path \"Gunulhona/tb_pretrained\"   --tokenizer_path \"Gunulhona/tbbarttokenizer\"   --progress_bar_refresh_rate 50   --max_seq_len 768   --num_sanity_val_steps 0   --chat    --resume_from_checkpoint \"logs/last.ckpt\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#@title generate test\n",
    "import os\n",
    "\n",
    "!python \"/content/kobart_mlm.py\"\\\n",
    "  --gradient_clip_val 1.0\\\n",
    "  --max_epochs 1\\\n",
    "  --lr 5e-7\\\n",
    "  --batch_size 512\\\n",
    "  --default_root_dir logs\\\n",
    "  --train_file Dev.csv\\\n",
    "  --val_file Dev.csv\\\n",
    "  --test_file Dev.csv\\\n",
    "  --model_path \"Gunulhona/tb_pretrained\"\\\n",
    "  --tokenizer_path \"Gunulhona/tbbarttokenizer\"\\\n",
    "  --progress_bar_refresh_rate 50\\\n",
    "  --max_seq_len 768\\\n",
    "  --num_sanity_val_steps 0\\\n",
    "  --move_metrics_to_cpu 1\\\n",
    "  --chat \\\n",
    "  --resume_from_checkpoint \"logs/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b3_Vjipd2YnK"
   },
   "outputs": [],
   "source": [
    " !rm -rdf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rdf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2ea9cafa9161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkobart_mlm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKoBARTConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mKoBARTConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kevin.jung/trainer/kobart_mlm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from trainer.kobart_mlm import KoBARTConditionalGeneration\n",
    "\n",
    "KoBARTConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532189223864ad392338d37f037e8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=992.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a6ae7fe274412ea1c4ca66fdf73ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=599565821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Gunulhona/tb_pretrained were not used when initializing BartModel: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartModel(\n",
       "  (shared): Embedding(30000, 768, padding_idx=3)\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartModel\n",
    "from torch.nn import functional as F\n",
    "model_name = 'Gunulhona/tb_pretrained'\n",
    "BartModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "XfahULH_8nv3",
    "rhOX8jeBi8AI",
    "cpWMzdN42Clh"
   ],
   "name": "[Turingbio Dev]Pretraining test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "103522cfa1604fad8059fc4fc93179d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20f1b708e6a94e08b54d474da294769c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a3e1e1673fd49a5b19e73076f6caf86",
       "IPY_MODEL_7735a15195c54e4f89d3c295e8142d3d",
       "IPY_MODEL_fe5e3980462b4607911e8809ab834d99"
      ],
      "layout": "IPY_MODEL_eaaa639063f8480eaa025ff4edabf69c"
     }
    },
    "31f2935ffebd457dba150bb6e5aab07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52e37c34bb2f454eab217e8c06c153e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7735a15195c54e4f89d3c295e8142d3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7f6317cc3864040a7300a21193a3d34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_103522cfa1604fad8059fc4fc93179d8",
      "value": 1
     }
    },
    "8a3e1e1673fd49a5b19e73076f6caf86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7c43d92ac51449b97f3dad47d6df86b",
      "placeholder": "​",
      "style": "IPY_MODEL_31f2935ffebd457dba150bb6e5aab07d",
      "value": ""
     }
    },
    "8b7663502e7e4c87901cb5edf3221118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7c43d92ac51449b97f3dad47d6df86b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaaa639063f8480eaa025ff4edabf69c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7f6317cc3864040a7300a21193a3d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "fe5e3980462b4607911e8809ab834d99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52e37c34bb2f454eab217e8c06c153e5",
      "placeholder": "​",
      "style": "IPY_MODEL_8b7663502e7e4c87901cb5edf3221118",
      "value": " 338/? [00:03&lt;00:00, 127.25it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
