{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfahULH_8nv3"
      },
      "source": [
        "# installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Br4fQjam8APU"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall setuptools -y\n",
        "# !pip install -q setuptools==59.6.0\n",
        "\n",
        "# import setuptools\n",
        "# import os\n",
        "\n",
        "# setuptools.distutils.version\n",
        "\n",
        "# def restart_runtime():\n",
        "#   os.kill(os.getpid(), 9)\n",
        "\n",
        "# restart_runtime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1tHvu9S0bky",
        "outputId": "34e93f31-238f-4503-cecf-37650888ec75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.5/266.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.2/766.2 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.5/553.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping page https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl because the HEAD request got Content-Type: application/octet-stream. The only supported Content-Types are application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html, and text/html\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchaudio' candidate (version 2.0.0 at https://files.pythonhosted.org/packages/24/bb/c34894cde118a6d2dcfc29cd79d15d2bd3b5f18266cd6cb59d4270e23802/torchaudio-2.0.0-cp39-cp39-manylinux1_x86_64.whl (from https://pypi.org/simple/torchaudio/))\n",
            "Reason for being yanked: Contains an incorrect torch dependency\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 설치하고 세션 다시시작 해주고 설치해야 정상적으로 인스톨 됨==1.5.10\n",
        "import IPython\n",
        "# !!pip install -q cloud-tpu-client==0.10\\\n",
        "#  torch==1.9.0\\\n",
        "#  torchtext==0.10.0\\\n",
        "#  torchvision==0.10.0\\\n",
        "#  https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# !pip install -q cloud-tpu-client==0.10 \\\n",
        "#     torch==1.11.0 \\\n",
        "#     torchvision torchtext\\\n",
        "#     https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "!sudo pip install -q -U pip\n",
        "!pip uninstall -q  earthengine-api -y\n",
        "# !sudo pip install -q libtpu-nightly -f https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/wheels/libtpu-nightly/libtpu_nightly-0.1.dev20230313-py3-none-any.whl\n",
        "# https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip uninstall -q torch torchvision torchaudio torchtext torchdata -y\n",
        "!pip install -U jsonargparse[signatures]>=4.17.0\n",
        "!pip install -U -q \\\n",
        "    lightning lightning-utilities \\\n",
        "    pytorch-ignite \\\n",
        "    transformers \\\n",
        "    tensorboard-plugin-profile \\\n",
        "    deepspeed \\\n",
        "    torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.0 torchtext==0.15.1 torchdata==0.6.0\\\n",
        "    cloud-tpu-client==0.10 \\\n",
        "    torch_xla -f https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC8PZ8A7CxS0",
        "outputId": "04a05d2a-684e-4f23-8cab-0d2dfcbad67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- ------------\n",
            "absl-py                       1.4.0\n",
            "aiohttp                       3.8.4\n",
            "aiosignal                     1.3.1\n",
            "alabaster                     0.7.13\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.2\n",
            "anyio                         3.6.2\n",
            "appdirs                       1.4.4\n",
            "argon2-cffi                   21.3.0\n",
            "argon2-cffi-bindings          21.2.0\n",
            "arrow                         1.2.3\n",
            "arviz                         0.15.1\n",
            "astropy                       5.2.2\n",
            "astunparse                    1.6.3\n",
            "async-timeout                 4.0.2\n",
            "attrs                         23.1.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.12.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.11.2\n",
            "bleach                        6.0.0\n",
            "blessed                       1.20.0\n",
            "blis                          0.7.9\n",
            "blosc2                        2.0.0\n",
            "bokeh                         2.4.3\n",
            "branca                        0.6.0\n",
            "CacheControl                  0.12.11\n",
            "cached-property               1.5.2\n",
            "cachetools                    5.3.0\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.12.7\n",
            "cffi                          1.15.1\n",
            "chardet                       4.0.0\n",
            "charset-normalizer            2.0.12\n",
            "chex                          0.1.7\n",
            "click                         8.1.3\n",
            "cloud-tpu-client              0.10\n",
            "cloudpickle                   2.2.1\n",
            "cmake                         3.25.2\n",
            "cmdstanpy                     1.1.0\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.4\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.6.0.post1\n",
            "contourpy                     1.0.7\n",
            "convertdate                   2.4.0\n",
            "croniter                      1.3.14\n",
            "cryptography                  40.0.2\n",
            "cufflinks                     0.17.3\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.3.1\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.34\n",
            "dask                          2022.12.1\n",
            "datascience                   0.17.6\n",
            "dateutils                     0.6.12\n",
            "db-dtypes                     1.1.1\n",
            "dbus-python                   1.2.16\n",
            "debugpy                       1.6.6\n",
            "decorator                     4.4.2\n",
            "deepdiff                      6.3.0\n",
            "deepspeed                     0.9.1\n",
            "defusedxml                    0.7.1\n",
            "distributed                   2022.12.1\n",
            "dlib                          19.24.1\n",
            "dm-tree                       0.1.8\n",
            "docstring-parser              0.15\n",
            "docutils                      0.16\n",
            "dopamine-rl                   4.0.6\n",
            "duckdb                        0.7.1\n",
            "easydict                      1.10\n",
            "ecos                          2.0.12\n",
            "editdistance                  0.6.2\n",
            "en-core-web-sm                3.5.0\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.4\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         1.2.0\n",
            "etuples                       0.3.8\n",
            "exceptiongroup                1.1.1\n",
            "fastai                        2.7.12\n",
            "fastapi                       0.88.0\n",
            "fastcore                      1.5.29\n",
            "fastdownload                  0.0.7\n",
            "fastjsonschema                2.16.3\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "filelock                      3.11.0\n",
            "firebase-admin                5.3.0\n",
            "Flask                         2.2.3\n",
            "flatbuffers                   23.3.3\n",
            "flax                          0.6.8\n",
            "folium                        0.14.0\n",
            "fonttools                     4.39.3\n",
            "frozendict                    2.3.7\n",
            "frozenlist                    1.3.3\n",
            "fsspec                        2023.4.0\n",
            "future                        0.18.3\n",
            "gast                          0.4.0\n",
            "GDAL                          3.3.2\n",
            "gdown                         4.6.6\n",
            "gensim                        4.3.1\n",
            "geographiclib                 2.0\n",
            "geopy                         2.3.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               1.34.0\n",
            "google-api-python-client      1.8.0\n",
            "google-auth                   2.17.3\n",
            "google-auth-httplib2          0.1.0\n",
            "google-auth-oauthlib          1.0.0\n",
            "google-cloud-bigquery         3.9.0\n",
            "google-cloud-bigquery-storage 2.19.1\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.15.1\n",
            "google-cloud-firestore        2.11.0\n",
            "google-cloud-language         2.9.1\n",
            "google-cloud-storage          2.8.0\n",
            "google-cloud-translate        3.11.1\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.4.1\n",
            "googleapis-common-protos      1.59.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.20.1\n",
            "greenlet                      2.0.2\n",
            "grpcio                        1.53.0\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gviz-api                      1.10.0\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h11                           0.14.0\n",
            "h5netcdf                      1.1.0\n",
            "h5py                          3.8.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.4\n",
            "hjson                         3.1.0\n",
            "holidays                      0.22\n",
            "holoviews                     1.15.4\n",
            "html5lib                      1.1\n",
            "httpimport                    1.3.0\n",
            "httplib2                      0.21.0\n",
            "huggingface-hub               0.13.4\n",
            "humanize                      4.6.0\n",
            "hyperopt                      0.2.7\n",
            "idna                          3.4\n",
            "imageio                       2.25.1\n",
            "imageio-ffmpeg                0.4.8\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.10.1\n",
            "imgaug                        0.4.0\n",
            "importlib-metadata            6.4.1\n",
            "importlib-resources           5.12.0\n",
            "imutils                       0.5.4\n",
            "inflect                       6.0.4\n",
            "iniconfig                     2.0.0\n",
            "inquirer                      3.1.3\n",
            "intel-openmp                  2023.1.0\n",
            "ipykernel                     5.5.6\n",
            "ipython                       7.34.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.4.1\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  2.1.2\n",
            "jax                           0.3.25\n",
            "jaxlib                        0.3.25\n",
            "jieba                         0.42.1\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.2.0\n",
            "jsonargparse                  4.21.0\n",
            "jsonpickle                    3.0.1\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter_core                  5.3.0\n",
            "jupyter-server                1.24.0\n",
            "jupyterlab-pygments           0.2.2\n",
            "jupyterlab-widgets            3.0.7\n",
            "kaggle                        1.5.13\n",
            "keras                         2.12.0\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "lazy_loader                   0.2\n",
            "libclang                      16.0.0\n",
            "librosa                       0.10.0.post2\n",
            "lightgbm                      3.3.5\n",
            "lightning                     2.0.1.post0\n",
            "lightning-cloud               0.5.33\n",
            "lightning-utilities           0.8.0\n",
            "lit                           16.0.1\n",
            "llvmlite                      0.39.1\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.2\n",
            "Markdown                      3.4.3\n",
            "markdown-it-py                2.2.0\n",
            "MarkupSafe                    2.1.2\n",
            "matplotlib                    3.7.1\n",
            "matplotlib-inline             0.1.6\n",
            "matplotlib-venn               0.11.9\n",
            "mdurl                         0.1.2\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.2\n",
            "mistune                       0.8.4\n",
            "mizani                        0.8.1\n",
            "mkl                           2019.0\n",
            "ml-dtypes                     0.1.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.1.0\n",
            "moviepy                       1.0.3\n",
            "mpmath                        1.3.0\n",
            "msgpack                       1.0.5\n",
            "multidict                     6.0.4\n",
            "multipledispatch              0.6.0\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       8.1.0\n",
            "natsort                       8.3.1\n",
            "nbclient                      0.7.3\n",
            "nbconvert                     6.5.4\n",
            "nbformat                      5.8.0\n",
            "nest-asyncio                  1.5.6\n",
            "networkx                      3.1\n",
            "nibabel                       3.0.2\n",
            "ninja                         1.11.1\n",
            "nltk                          3.8.1\n",
            "notebook                      6.4.8\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.24.3\n",
            "nvidia-cublas-cu11            11.10.3.66\n",
            "nvidia-cuda-cupti-cu11        11.7.101\n",
            "nvidia-cuda-nvrtc-cu11        11.7.99\n",
            "nvidia-cuda-runtime-cu11      11.7.99\n",
            "nvidia-cudnn-cu11             8.5.0.96\n",
            "nvidia-cufft-cu11             10.9.0.58\n",
            "nvidia-curand-cu11            10.2.10.91\n",
            "nvidia-cusolver-cu11          11.4.0.1\n",
            "nvidia-cusparse-cu11          11.7.4.91\n",
            "nvidia-nccl-cu11              2.14.3\n",
            "nvidia-nvtx-cu11              11.7.91\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "opencv-contrib-python         4.7.0.72\n",
            "opencv-python                 4.7.0.72\n",
            "opencv-python-headless        4.7.0.72\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "optax                         0.1.4\n",
            "orbax                         0.1.7\n",
            "ordered-set                   4.1.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     23.1\n",
            "palettable                    3.3.2\n",
            "pandas                        1.5.3\n",
            "pandas-datareader             0.10.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.14.4\n",
            "param                         1.13.0\n",
            "parso                         0.8.3\n",
            "partd                         1.4.0\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.1\n",
            "patsy                         0.5.3\n",
            "pep517                        0.13.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        8.4.0\n",
            "pip                           23.1.1\n",
            "pip-tools                     6.6.2\n",
            "platformdirs                  3.2.0\n",
            "plotly                        5.13.1\n",
            "plotnine                      0.10.1\n",
            "pluggy                        1.0.0\n",
            "polars                        0.17.3\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   0.7.2\n",
            "proglog                       0.1.10\n",
            "progressbar2                  4.2.0\n",
            "prometheus-client             0.16.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                3.0.38\n",
            "prophet                       1.1.2\n",
            "proto-plus                    1.22.2\n",
            "protobuf                      3.20.3\n",
            "psutil                        5.9.5\n",
            "psycopg2                      2.9.6\n",
            "ptyprocess                    0.7.0\n",
            "py-cpuinfo                    9.0.0\n",
            "py4j                          0.10.9.7\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.5.0\n",
            "pydantic                      1.10.7\n",
            "pydata-google-auth            1.7.0\n",
            "pydot                         1.4.2\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyerfa                        2.0.0.3\n",
            "pygame                        2.3.0\n",
            "Pygments                      2.14.0\n",
            "PyGObject                     3.36.0\n",
            "PyJWT                         2.6.0\n",
            "pymc                          5.1.2\n",
            "PyMeeus                       0.5.12\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyrsistent                    0.19.3\n",
            "PySocks                       1.7.1\n",
            "pytensor                      2.10.1\n",
            "pytest                        7.2.2\n",
            "python-apt                    0.0.0\n",
            "python-dateutil               2.8.2\n",
            "python-editor                 1.0.4\n",
            "python-louvain                0.16\n",
            "python-multipart              0.0.6\n",
            "python-slugify                8.0.1\n",
            "python-utils                  3.5.2\n",
            "pytorch-ignite                0.4.11\n",
            "pytorch-lightning             2.0.1.post0\n",
            "pytz                          2022.7.1\n",
            "pytz-deprecation-shim         0.1.0.post0\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.7\n",
            "qudida                        0.0.4\n",
            "readchar                      4.0.5\n",
            "regex                         2022.10.31\n",
            "requests                      2.27.1\n",
            "requests-oauthlib             1.3.1\n",
            "requests-unixsocket           0.2.0\n",
            "rich                          13.3.4\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.19.3\n",
            "scikit-learn                  1.2.2\n",
            "scipy                         1.10.1\n",
            "scs                           3.2.3\n",
            "seaborn                       0.12.2\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    67.6.1\n",
            "shapely                       2.0.1\n",
            "six                           1.16.0\n",
            "sklearn-pandas                2.2.0\n",
            "smart-open                    6.3.0\n",
            "sniffio                       1.3.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.12.1\n",
            "soupsieve                     2.4.1\n",
            "soxr                          0.3.5\n",
            "spacy                         3.5.2\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.4\n",
            "Sphinx                        3.5.4\n",
            "sphinxcontrib-applehelp       1.0.4\n",
            "sphinxcontrib-devhelp         1.0.2\n",
            "sphinxcontrib-htmlhelp        2.0.1\n",
            "sphinxcontrib-jsmath          1.0.1\n",
            "sphinxcontrib-qthelp          1.0.3\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "SQLAlchemy                    2.0.9\n",
            "sqlparse                      0.4.3\n",
            "srsly                         2.4.6\n",
            "starlette                     0.22.0\n",
            "starsessions                  1.3.0\n",
            "statsmodels                   0.13.5\n",
            "sympy                         1.11.1\n",
            "tables                        3.8.0\n",
            "tabulate                      0.8.10\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.2.2\n",
            "tensorboard                   2.12.2\n",
            "tensorboard-data-server       0.7.0\n",
            "tensorboard-plugin-profile    2.11.2\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.12.0\n",
            "tensorflow-datasets           4.8.3\n",
            "tensorflow-estimator          2.12.0\n",
            "tensorflow-gcs-config         2.12.0\n",
            "tensorflow-hub                0.13.0\n",
            "tensorflow-io-gcs-filesystem  0.32.0\n",
            "tensorflow-metadata           1.13.1\n",
            "tensorflow-probability        0.19.0\n",
            "tensorstore                   0.1.35\n",
            "termcolor                     2.2.0\n",
            "terminado                     0.17.1\n",
            "text-unidecode                1.3\n",
            "textblob                      0.17.1\n",
            "tf-slim                       1.1.0\n",
            "thinc                         8.1.9\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2023.4.12\n",
            "tinycss2                      1.2.1\n",
            "tokenizers                    0.13.3\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         2.0.0\n",
            "torch-xla                     2.0\n",
            "torchaudio                    2.0.0\n",
            "torchdata                     0.6.0\n",
            "torchmetrics                  0.11.4\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.1\n",
            "torchvision                   0.15.1\n",
            "tornado                       6.2\n",
            "tqdm                          4.65.0\n",
            "traitlets                     5.7.1\n",
            "transformers                  4.28.1\n",
            "triton                        2.0.0\n",
            "tweepy                        4.13.0\n",
            "typer                         0.7.0\n",
            "typeshed-client               2.2.0\n",
            "typing_extensions             4.5.0\n",
            "tzdata                        2023.3\n",
            "tzlocal                       4.3\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.26.15\n",
            "uvicorn                       0.21.1\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        1.1.1\n",
            "wcwidth                       0.2.6\n",
            "webcolors                     1.13\n",
            "webencodings                  0.5.1\n",
            "websocket-client              1.5.1\n",
            "websockets                    11.0.2\n",
            "Werkzeug                      2.2.3\n",
            "wheel                         0.40.0\n",
            "widgetsnbextension            3.6.4\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        2022.12.0\n",
            "xarray-einstats               0.5.1\n",
            "xgboost                       1.7.5\n",
            "xlrd                          2.0.1\n",
            "yarl                          1.9.1\n",
            "yellowbrick                   1.5\n",
            "yfinance                      0.2.18\n",
            "zict                          2.2.0\n",
            "zipp                          3.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fbaY8u_8c-1W"
      },
      "outputs": [],
      "source": [
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeJNiJ5jqEH8",
        "outputId": "01ef192c-6ee8-463a-bd72-2d8161160d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/focal.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.0.9\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 7,419 kB of archives.\n",
            "After this operation, 4,936 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.3.0 [7,419 kB]\n",
            "Fetched 7,419 kB in 0s (15.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 122356 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.3.0_amd64.deb ...\n",
            "Unpacking git-lfs (3.3.0) over (2.9.2-1) ...\n",
            "Setting up git-lfs (3.3.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "# Install\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git config --global user.email \"ppijbb@gmail.com\"\n",
        "!git config --global user.name \"gunulhona\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p7ndo7AKOOhE"
      },
      "outputs": [],
      "source": [
        "!unzip -qq -n \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/국립국어원 메신저, 대화/NIKL_MESSENGER_v2.0.zip\" -d ./mes\n",
        "!unzip -qq -n \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/국립국어원 메신저, 대화/NIKL_OM_2021_v1.0.zip\" -d  ./om"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcNx5skb1NwX"
      },
      "source": [
        "Strategies\n",
        "\n",
        "<li>bagua\n",
        "<li>ddp\n",
        "<li>ddp2\n",
        "<li>ddp_find_unused_parameters_false\n",
        "<li>ddp_fully_sharded\n",
        "<li>ddp_sharded\n",
        "<li>ddp_sharded_find_unused_parameters_false\n",
        "<li>ddp_sharded_spawn\n",
        "<li>ddp_sharded_spawn_find_unused_parameters_false\n",
        "<li>ddp_spawn\n",
        "<li>ddp_spawn_find_unused_parameters_false\n",
        "<li>deepspeed\n",
        "<li>deepspeed_stage_1\n",
        "<li>deepspeed_stage_2\n",
        "<li>deepspeed_stage_2_offload\n",
        "<li>deepspeed_stage_3\n",
        "<li>deepspeed_stage_3_offload\n",
        "<li>deepspeed_stage_3_offload_nvme\n",
        "<li>dp\n",
        "<li>fsdp\n",
        "<li>horovod\n",
        "<li>hpu_parallel\n",
        "<li>hpu_single\n",
        "<li>ipu_strategy\n",
        "<li>single_device\n",
        "<li>single_tpu\n",
        "<li>tpu_spawn\n",
        "<li>tpu_spawn_debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiR-2UBBjySy"
      },
      "source": [
        "#DEV note\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "RL7lTXtGgLqL"
      },
      "outputs": [],
      "source": [
        "#@title Data Process\n",
        "#@markdown 한국어 SNS 데이터 목록(120만건)\n",
        "#@markdown <li>개인및관계 <li>미용과건강<li> 상거래(쇼핑)<li> 시사교육<li> 식음료<li> 여가생활<li> 일과직업<li> 주거와생활<li> 행사 </li>\n",
        "#@markdown SNS데이터 <P01> ~ <P09>까지의 턴 대화\n",
        "import json\n",
        "import pandas as pd\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/\"#@param {type:\"string\"}\n",
        "# 한국어 SNS 데이터 처리 function\n",
        "def make_context(json_data,summary=False,multiline=False):\n",
        "  result = [[],[]]\n",
        "  token = \"\\n\" #@param{type:\"string\"}\n",
        "  for i,data in enumerate(json_data['data']):\n",
        "    previous_t,previous_p = None, None\n",
        "    q,a = \"\",\"\"\n",
        "    json_body = data['body']['dialogue'] if summary else data['body']\n",
        "    for body in json_body:\n",
        "      present_p, present_t = body['turnID'], body['participantID']\n",
        "      if previous_p:\n",
        "        if previous_p == present_p and previous_t == present_t:\n",
        "          a += f\"{body['utterance']}\\n\"\n",
        "        else:\n",
        "          result[0].append(q)\n",
        "          result[1].append(a)\n",
        "          q = q+a+token if multiline else a\n",
        "          a = f\"{body['utterance']}\\n\"\n",
        "      else: a = f\"{body['utterance']}\\n\"\n",
        "      previous_p, previous_t = present_p, present_t\n",
        "    result[0].append(token+q if multiline else q)\n",
        "    result[1].append(token+a if multiline else a)\n",
        "  result = pd.DataFrame({\"Q\":result[0], \"A\":result[1]})\n",
        "  result = result.drop(result[result.Q==\"\"].index)\n",
        "  if multiline:\n",
        "    result.Q = result.Q.map(lambda x: x[:-(len(token))])\n",
        "  return  result\n",
        "\n",
        "def make_long_context(json_data,summary=False):\n",
        "  result = [[],[]]\n",
        "  for i,data in enumerate(json_data['data']):\n",
        "    line = [\"\"]\n",
        "    token = \"</s>\"\n",
        "    json_body = data['body']['dialogue'] if summary else data['body']\n",
        "    for body in json_body:\n",
        "      result[0].append(token.join(line[-5:]))\n",
        "      chat = f\"{body['utterance']}{token}\" if body['participantID'] == \"P01\" else f\"{body['utterance']}{token}\"\n",
        "      if line[0]==\"\":\n",
        "          line[0] = chat[:-len(token)]\n",
        "      else:\n",
        "          line += [chat[:-len(token)]]\n",
        "      result[1].append(chat[:-len(token)])\n",
        "  df = pd.DataFrame({\"Q\":result[0], \"A\":result[1]})\n",
        "  return df.drop(df[df['Q']==''].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Av3zZOEJaIK"
      },
      "source": [
        "## make_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu66lNBBseaQ"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/개인및관계.json','r') as f: #이것만 100만건 가량\n",
        "  data = json.load(f,strict=False)\n",
        "  personal = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W98Q9A4WkgW3"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/개인및관계.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  personal_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y5w8p2LsOGD"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/미용과건강.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  beauty = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxxy6j9kk7SK"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/미용과건강.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  beauty_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrScdcRSr6LK"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/상거래(쇼핑).json','r') as f:\n",
        "  data = json.load(f)\n",
        "  commercial = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57v6Hs1k7rM"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/상거래(쇼핑).json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  commercial_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUnnBKsZxRra"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/시사교육.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  passing = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXwnWOF-k8lw"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/시사교육.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  passing_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eNu7hKhleJO"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/식음료.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  food = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh7wFuHYk9DS"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/식음료.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  food_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_Qf5mMnDkv"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/여가생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  leisure = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocV9c8AOk9bC"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/여가생활.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  leisure_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzkyI74fnB1U"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/일과직업.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  job = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EdZjkO5k9ya"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/일과직업.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  job_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJZwbzoiKcg"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/주거와생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  life = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ1HVIdck-JO"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/주거와생활.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  life_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Exmv-RertXD"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/행사.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  event = make_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjn438ok-jb"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/행사.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  event_summary = make_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md5AYWOVJUU6"
      },
      "source": [
        "## make long context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nYEM1ulWJeCe"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/개인및관계.json','r') as f: #이것만 100만건 가량\n",
        "  data = json.load(f,strict=False)\n",
        "  personal = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "66jnb4ptwfLe"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/개인및관계.json','r') as f:\n",
        "  data = json.load(f,strict=False)\n",
        "  personal_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nAYQzQTEJeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/미용과건강.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  beauty = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lKljF_Iswil1"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/미용과건강.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  beauty_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-83xsYY7JeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/상거래(쇼핑).json','r') as f:\n",
        "  data = json.load(f)\n",
        "  commercial = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bSETXeaawmYT"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/상거래(쇼핑).json','r') as f:\n",
        "  data = json.load(f)\n",
        "  commercial_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P39_efu9JeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/시사교육.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  passing = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WsYis6o2wozK"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/시사교육.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  passing_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IzjVVgRLJeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/식음료.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  food = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1qj3ywA1wsIJ"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/식음료.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  food_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GrRZD-lrJeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/여가생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  leisure = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5iAkkdIiwvAJ"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/여가생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  leisure_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mbaD8rBhJeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/일과직업.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  job = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MK30vTVBwzKC"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/일과직업.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  job_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xr2nbhYxJeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/주거와생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  life = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VJH8hGuWw1LZ"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/주거와생활.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  life_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MhMS82h1JeCf"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어SNS/행사.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  event = make_long_context(data)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bf3XYeukw4BK"
      },
      "outputs": [],
      "source": [
        "with open(DATA_PATH+'한국어 대화 요약/Training/행사.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  event_summary = make_long_context(data,summary=True)\n",
        "  del data\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ5ZaJOWJW7C"
      },
      "source": [
        "## tpu chatting train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rp6SvAOq0Z1"
      },
      "source": [
        "### 프로세스 Restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IubbV4Ork2Uo"
      },
      "outputs": [],
      "source": [
        "#@title restart runtime\n",
        "import os\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "restart_runtime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G6m3KHnNdSJ3"
      },
      "outputs": [],
      "source": [
        "#@title kill python process\n",
        "!kill -9 `ps -ef | grep python | awk '{print $2}'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Zhj7rd8Xgh"
      },
      "source": [
        "### Data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "6b4H-HbccXPW"
      },
      "outputs": [],
      "source": [
        "#@title Chatbot_data 디렉토리 지우기\n",
        "!rm -rdf Chatbot_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "-al8lV4Uaw7A"
      },
      "outputs": [],
      "source": [
        "#@title 국립국어원 데이터\n",
        "import os, json\n",
        "\n",
        "def r_string(text):\n",
        "    text = text.replace(\"{emoji}\",\"#@이모티콘#\")\n",
        "    text = text.replace(\"{system: gift}\",\"#@선물하기#\")\n",
        "    text = text.replace(\"{system: call}\",\"#@통화#\")\n",
        "    text = text.replace(\"{system: money}\",\"#@금융#\")\n",
        "    text = text.replace(\"{system: notice}\",\"#@공지#\")\n",
        "    text = text.replace(\"{system: map}\",\"#@위치공유#\")\n",
        "    text = text.replace(\"{system: delete}\",\"#@삭제된 메시지#\")\n",
        "    text = text.replace(\"{share:photo}\",\"#@사진#\")\n",
        "    text = text.replace(\"{share:video}\",\"#@영상#\")\n",
        "    text = text.replace(\"{share:music}\",\"#@음악#\")\n",
        "    text = text.replace(\"{share:file}\",\"#@파일#\")\n",
        "    text = text.replace(\"{share:voice}\",\"#@음성메시지#\")\n",
        "    text = text.replace(\"{share:info}\",\"#@정보#\")\n",
        "    text = text.replace(\"{share:url}\",\"#@URL#\")\n",
        "    text = text.replace(\"&name&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name1&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name2&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name3&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name4&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name5&\",\"#@이름#\")\n",
        "    text = text.replace(\"&name6&\",\"#@이름#\")\n",
        "    text = text.replace(\"&account&\",\"#@계정#\")\n",
        "    text = text.replace(\"&social-security-num&\",\"#@신원#\")\n",
        "    text = text.replace(\"&tel-num&\",\"#@전번#\")\n",
        "    text = text.replace(\"&card-num&\",\"#@금융#\")\n",
        "    text = text.replace(\"&address&\",\"#@주소#\")\n",
        "    text = text.replace(\"&affiliation&\",\"#@신원#\")\n",
        "    text = text.replace(\"&others&\",\"#@기타#\")\n",
        "    return text\n",
        "\n",
        "def json_msg(json_data):\n",
        "    temp = [[],[]]\n",
        "    for doc in json_data[\"document\"]:\n",
        "        q, a, post_text = [\"\"] * 3\n",
        "        post_id = None\n",
        "        for utt in doc[\"utterance\"]:\n",
        "            if post_id and post_id != utt[\"speaker_id\"]:\n",
        "                temp[0] += [q[:]]\n",
        "                temp[1] += [a[:]]\n",
        "                q = a\n",
        "                a = \"\"\n",
        "            a += r_string(utt[\"original_form\"])+\"\\n\"\n",
        "            post_id = utt[\"speaker_id\"]\n",
        "        temp[0] += [q[:-1]]\n",
        "        temp[1] += [a[:-1]]\n",
        "    return temp\n",
        "\n",
        "root_path = \"/content/om/NIKL_OM_2021_v1.0/국립국어원 온라인 대화 말뭉치 2021(버전 1.0)/국립국어원 온라인 대화 말뭉치 2021(버전 1.0)/\"\n",
        "file_list = os.listdir(root_path)\n",
        "to_df = [[],[]]\n",
        "for i in [file for file in file_list if file.endswith('.json')]:\n",
        "    with open(root_path+i,'r') as f:\n",
        "        json_data = json.load(f)\n",
        "        result = json_msg(json_data)\n",
        "        to_df[0] += result[0]\n",
        "        to_df[1] += result[1]\n",
        "        del json_data, result\n",
        "        f.close()\n",
        "\n",
        "root_path = \"/content/mes/NIKL_MESSENGER_v2.0/국립국어원 메신저 말뭉치(버전 2.0)/\"\n",
        "file_list = os.listdir(root_path)\n",
        "for i in [file for file in file_list if file.endswith('.json')]:\n",
        "    with open(root_path+i,'r') as f:\n",
        "        json_data = json.load(f)\n",
        "        result = json_msg(json_data)\n",
        "        to_df[0] += result[0]\n",
        "        to_df[1] += result[1]\n",
        "        del json_data, result\n",
        "        f.close()\n",
        "\n",
        "momal = pd.DataFrame({\"Q\":to_df[0], \"A\":to_df[1]})\n",
        "momal = momal[momal[\"Q\"]!=\"\"]\n",
        "\n",
        "del to_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "id": "UsDn0FpaL1Te"
      },
      "outputs": [],
      "source": [
        "#@title 감성대화말뭉치 qa pair 데이터 + 웰니스 + 챗봇 + 연속대화 + 말티모달 대화 추출 + 주제별 일상대화\n",
        "\n",
        "twt = pd.read_excel(DATA_PATH+\"트위터_대화시나리오DB_2000Set.xlsx\",header=None)\n",
        "twt_list=[[]]\n",
        "for i,x in twt.iterrows():\n",
        "    for text in x:\n",
        "        if type(text)==str:\n",
        "            twt_list[i].append(text)\n",
        "        twt_list+=[[]]\n",
        "twt_l = []\n",
        "for x in twt_list:\n",
        "    if x != []:\n",
        "        for i,line in enumerate(x):\n",
        "            if i > 0:\n",
        "                if len(x)>56:\n",
        "                    twt_l.append({\"Q\":f'{\"</s>\".join(x[-5:i])}',\"A\":f'{x[i]}'})\n",
        "                else:\n",
        "                    twt_l.append({\"Q\":f'{\"</s>\".join(x[:i])}',\"A\":f'{x[i]}'})\n",
        "            twt_l += []\n",
        "twt= pd.DataFrame(twt_l)\n",
        "\n",
        "multi_text = pd.read_excel(DATA_PATH+\"한국어 감정 정보가 포함된 연속적 대화 데이터셋/한국어_연속적_대화_데이터셋.xlsx\",header=1)\n",
        "index = 0\n",
        "li = [[]]\n",
        "text_label = multi_text[multi_text.columns[:3]]\n",
        "for i,x in text_label.iterrows():\n",
        "    if x['dialog #'] ==\"S\" and i != 0:\n",
        "        li += [[]]\n",
        "        index += 1\n",
        "    li[index].append(x['발화'])\n",
        "Li=[]\n",
        "for x in li:\n",
        "    for i,v in enumerate(x):\n",
        "        result = {\"Q\":f'{\"</s>\".join(x[:i])}',\"A\":f'{x[i]}'} if i>0 else None\n",
        "        if result:\n",
        "            Li.append(result)\n",
        "conti_conversation = pd.DataFrame(Li)\n",
        "conti_conversation = conti_conversation.dropna()\n",
        "\n",
        "df_t = pd.read_excel(DATA_PATH+\"감성대화말뭉치(최종데이터)_Training.xlsx\")\n",
        "df_v = pd.read_excel(DATA_PATH+\"감성대화말뭉치(최종데이터)_Validation.xlsx\")\n",
        "df_a = pd.read_excel(DATA_PATH+\"감성대화말뭉치-추가.xlsx\")\n",
        "df = pd.concat([df_t,df_v],ignore_index=True)\n",
        "text_df = pd.concat([df.loc[:,'사람문장1':'시스템응답4'] , df_a.loc[:,\"사람문장1\":\"시스템응답4\"]],ignore_index=True)\n",
        "# t = \"<P01>\"+text_df['사람문장1']+\"<P02>\"+text_df['시스템응답1'] + \\\n",
        "#     \"<P01>\"+text_df['사람문장2']+\"<P02>\"+text_df['시스템응답2'] + \\\n",
        "#     \"<P01>\"+text_df['사람문장3']+\"<P02>\"+text_df['시스템응답3'] + \\\n",
        "#     \"<P01>\"+text_df['사람문장4']+\"<P02>\"+text_df['시스템응답4']\n",
        "result = [[],[]]\n",
        "for row in text_df.iterrows():\n",
        "  result[0].append(f'<mask>{row[1][\"사람문장1\"]}')    #   result[0].append('<usr>'+row[1]['사람문장1'])\n",
        "  result[1].append(f'{row[1][\"시스템응답1\"]}')  #   result[1].append('<sys>'+row[1]['시스템응답1'])\n",
        "#   result[0].append(f'<mask>{row[1][\"시스템응답1\"]}')  #   result[0].append('<sys>'+row[1]['시스템응답1'])\n",
        "#   result[1].append(f'<mask>{row[1][\"사람문장2\"]}')    #   result[1].append('<usr>'+row[1]['사람문장2'])\n",
        "  result[0].append(f'<mask>{row[1][\"사람문장1\"]}</s>{row[1][\"시스템응답1\"]}</s>{row[1][\"사람문장2\"]}')    #   result[0].append('<usr>'+row[1]['사람문장2'])\n",
        "  result[1].append(f'{row[1][\"시스템응답2\"]}')  #   result[1].append('<sys>'+row[1]['시스템응답2'])\n",
        "#   if row[1]['시스템응답2'] != \"\" and row[1]['사람문장3'] != \"\":\n",
        "#     result[0].append(f'<mask>{row[1][\"시스템응답2\"]}')  #   result[0].append('<sys>'+row[1]['시스템응답2'])\n",
        "#     result[1].append(f'<mask>{row[1][\"사람문장3\"]}')  #   result[1].append(f'<usr>{row[1][\"사람문장3\"]}')\n",
        "  if type(row[1]['시스템응답3']) == str and type(row[1]['사람문장3']) == str:\n",
        "    result[0].append(f'<mask>{row[1][\"사람문장1\"]}</s>{row[1][\"시스템응답1\"]}</s>{row[1][\"사람문장2\"]}</s>{row[1][\"사람문장3\"]}')  #   result[0].append(f'<usr>{row[1][\"사람문장3\"]}')\n",
        "    result[1].append(f'{row[1][\"시스템응답3\"]}')    #   result[1].append(f'<sys>{row[1][\"시스템응답3\"]}')\n",
        "#   if row[1]['시스템응답3'] != \"\" and row[1]['사람문장4'] != \"\":\n",
        "#     result[0].append(f'<mask>{row[1][\"시스템응답3\"]}')    #   result[0].append(f'<sys>{row[1][\"시스템응답3\"]}')\n",
        "#     result[1].append(f'<mask>{row[1][\"사람문장4\"]}')  #   result[1].append(f'<usr>{row[1][\"사람문장4\"]}')\n",
        "  if type(row[1]['시스템응답4']) == str and type(row[1]['사람문장4']) == str:\n",
        "    result[0].append(f'<mask>{row[1][\"사람문장1\"]}</s>{row[1][\"시스템응답1\"]}</s>{row[1][\"사람문장2\"]}</s>{row[1][\"사람문장3\"]}</s>{row[1][\"사람문장4\"]}')  #   result[0].append(f'<usr>{row[1][\"사람문장4\"]}')\n",
        "    result[1].append(f'{row[1][\"시스템응답4\"]}')    #   result[1].append(f'<sys>{row[1][\"시스템응답4\"]}')\n",
        "\n",
        "t = pd.DataFrame({'Q':result[0],'A':result[1]}).dropna()\n",
        "del result, df_t, df_v\n",
        "wellness = pd.read_excel(DATA_PATH+\"웰니스 대화 스크립트 데이터셋/웰니스_대화_스크립트_데이터셋_GPT3.xlsx\")\n",
        "response = wellness[[\"구분\",\"챗봇\"]].dropna()\n",
        "wellresponse = [\"저런, 그런일이 있으셨군요...\",\n",
        "                \"그런 일이 있으셨군요.. 계속 말씀해 주세요.\",\n",
        "                \"이해해요. 이야기하다 보면 해결책을 찾을 수도 있을 거예요.\",\n",
        "                \"그렇군요...저와 같이 이야기를 나눠 보아요.\",\n",
        "                \"자세히는 알 수 없지만, 잠시 지나가는 상황일 거 에요. 곧 좋은 일이 생길 거라 믿어요.\",\n",
        "                \"그런 생각을 하셨군요. 제가 힘이 되어드릴 수 있으면 좋겠어요.\" ]\n",
        "for i in range(len(wellness)):\n",
        "  if type(wellness.loc[i,\"챗봇\"])==float:\n",
        "    try:\n",
        "      wellness.loc[i,\"챗봇\"] = response.loc[response[\"구분\"]==wellness.loc[i,\"구분\"],\"챗봇\"].sample().iloc[0]\n",
        "    except:\n",
        "      wellness.loc[i,\"챗봇\"] = pd.Series(wellresponse).sample().iloc[0]\n",
        "wellness.rename(columns={\"유저\":\"Q\",\"챗봇\":\"A\"},inplace=True)\n",
        "wellness[\"Q\"] = \"<mask>\"+wellness[\"Q\"]\n",
        "\n",
        "dialogue = pd.read_csv(DATA_PATH+\"국립국어원 한국어 일상대화/dialogue.csv\")\n",
        "dialogue.rename(columns={\"usr\":\"Q\", \"sys\":\"A\"},inplace=True)\n",
        "# dialogue[\"Q\"] = \"<mask>\"+dialogue [\"Q\"]\n",
        "\n",
        "chitchat = pd.read_csv(DATA_PATH+\"ChatbotData.csv\")\n",
        "speaker_df = pd.read_csv(DATA_PATH+\"aispeaker.csv\")\n",
        "topic_df = pd.read_excel(DATA_PATH+\"topic_dialogue.xlsx\")\n",
        "topic_df = topic_df[(topic_df[\"Q\"] != \"\")&(~topic_df[\"Q\"].isna())]\n",
        "topic_df = topic_df.loc[topic_df.Q.str.contains(\"짜증|고민|불안|걱정|우울|힘들|마음이|고생|초조|불쌍|괜찮|고생|피곤|ㅠㅠ|ㅜㅜ\"),:]\n",
        "platform_df = pd.read_csv(DATA_PATH+\"platform_dialogue.csv\")\n",
        "platform_df = platform_df[(platform_df[\"Q\"] != \"\")&(~platform_df[\"Q\"].isna())]\n",
        "platform_df = platform_df.loc[platform_df.Q.str.contains(\"짜증|고민|불안|걱정|우울|힘들|마음이|고생|초조|불쌍|괜찮|피곤|ㅠㅠ|ㅜㅜ\"),:]\n",
        "t = pd.concat(\n",
        "    [t,\n",
        "     twt,\n",
        "     wellness[[\"Q\",\"A\"]],\n",
        "     chitchat[[\"Q\",\"A\"]],\n",
        "     conti_conversation,\n",
        "     dialogue,\n",
        "     speaker_df,\n",
        "     speaker_df,\n",
        "     topic_df,\n",
        "     platform_df],\n",
        "     ignore_index=True)\n",
        "if \"momal\" in locals() or \"momal\" in globals():\n",
        "    t = pd.concat([t, momal], ignore_index=True)\n",
        "del conti_conversation,twt, wellness, wellresponse,speaker_df, response,chitchat, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "685542cc4ffc4ec6929c6b510f80dde7",
            "dc1fb4ebd3d747eda75f0942bdd1e914",
            "4f59dbf8696f4e34889e6650c27bdf1e",
            "bcd35461dced433380a7c9a66052804c",
            "54143cc64d064c07b17cb86dc5a9eff7",
            "a2f49862b59745f0ac9751138913a4d5",
            "badf81cec039443b87d46e0cba279c5b",
            "718e2ef821cd4c259fd0ba29361a2ac8",
            "a88bd9b4199e44858a0de775c3762b39",
            "f3f24430b08a4009a105d8e3e0f70ad6",
            "ffd3ada2dc5a42aaba3a10152db01299"
          ]
        },
        "id": "7KlyBbpicfeg",
        "outputId": "18438e5a-2b58-4cc9-9910-c324f70d3857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNS+대화요약 : 2474628\n",
            "감성대화+챗봇+웰니스+연속대화+트위터+한국어 일상대화 : 2626438\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "folding data...: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "685542cc4ffc4ec6929c6b510f80dde7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(SNS+대화요약)DEV+감성대화+챗봇+웰니스 :  2626438\n",
            "Train set: 1,979,702\n",
            "Test set : 247,463\n",
            "Dev set : 247,463\n",
            "dev set : 248\n"
          ]
        }
      ],
      "source": [
        "#@title Train, Test 데이터 분리\n",
        "!mkdir Chatbot_data\n",
        "!mkdir Chatbot_data/fold\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "try :\n",
        "    df = pd.concat([personal,personal_summary,beauty,beauty_summary,\\\n",
        "                    commercial,commercial_summary,passing,passing_summary,\\\n",
        "                    food,food_summary,leisure,leisure_summary,job,job_summary,\\\n",
        "                    life,life_summary,event,event_summary],ignore_index=True)\n",
        "    df = df.loc[df.Q.str.contains(\"짜증|고민|불안|걱정|우울|힘들|마음이|고생|초조|불쌍|피곤|ㅠㅠ|ㅜㅜ\"),:]\n",
        "except:\n",
        "    print(\"exception!\")\n",
        "    df = []\n",
        "\n",
        "print(f\"SNS+대화요약 : {len(df)}\")\n",
        "# _, df  = train_test_split(df, test_size=0.01,random_state=42)\n",
        "print(f\"감성대화+챗봇+웰니스+연속대화+트위터+한국어 일상대화 : {len(t)}\")\n",
        "# t = pd.concat([df,t],ignore_index=True)\n",
        "\n",
        "if type(df) == pd.DataFrame:\n",
        "    TRAIN, TEST = train_test_split(df, test_size=0.2,random_state=42)\n",
        "    TEST, DEV  = train_test_split(TEST, test_size=0.5,random_state=42)\n",
        "    SPLITS=2#@param {type:\"integer\"}\n",
        "    kf = KFold(n_splits=SPLITS, shuffle=True)\n",
        "    for index, (train, test) in tqdm(enumerate(kf.split(TRAIN)),\n",
        "                                    \"folding data...\"):\n",
        "        pd.concat([TRAIN.iloc[test],t.sample(int(len(t)*0.4))],ignore_index=True)\\\n",
        "        .to_csv(f\"Chatbot_data/fold/train_{random.randint(0,100000)}_{index}.csv\",index=False)\n",
        "    TRAIN.to_csv(\"Chatbot_data/train.csv\",index=False) # 4GB\n",
        "    TEST.to_csv(\"Chatbot_data/test.csv\",index=False)  # 1GB\n",
        "    DEV.to_csv(\"Chatbot_data/dev.csv\",index=False)  # 1GB\n",
        "    _, dev = train_test_split(DEV,test_size=0.001,random_state=42)\n",
        "    dev.to_csv(\"Chatbot_data/dev_small.csv\",index=False)\n",
        "\n",
        "else:\n",
        "    TRAIN, TEST = train_test_split(t, test_size=0.2,random_state=42)\n",
        "    TEST, DEV  = train_test_split(TEST, test_size=0.5,random_state=42)\n",
        "    SPLITS=5\n",
        "    kf = KFold(n_splits=SPLITS, shuffle=True)\n",
        "    for index, (train, test) in tqdm(enumerate(kf.split(TRAIN)),\n",
        "                                    \"folding data...\"):\n",
        "        pd.concat([TRAIN.iloc[test],],ignore_index=True)\\\n",
        "        .to_csv(f\"Chatbot_data/fold/train_{random.randint(0,100000)}_{index}.csv\",index=False)\n",
        "    TRAIN.to_csv(\"Chatbot_data/train.csv\",index=False) # 4GB\n",
        "    TEST.to_csv(\"Chatbot_data/test.csv\",index=False)  # 1GB\n",
        "    DEV.to_csv(\"Chatbot_data/dev.csv\",index=False)  # 1GB\n",
        "    _, dev = train_test_split(DEV,test_size=0.001,random_state=42)\n",
        "    dev.to_csv(\"Chatbot_data/dev_small.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"(SNS+대화요약)DEV+감성대화+챗봇+웰니스 : \",len(t))\n",
        "print(\"Train set:\",format(len(TRAIN),\",d\"))\n",
        "print(\"Test set :\",format(len(TEST),\",d\"))\n",
        "print(\"Dev set :\",format(len(DEV),\",d\"))\n",
        "print(\"dev set :\",format(len(dev),\",d\"))\n",
        "del df, t, TRAIN, TEST, DEV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9MAhNA6Xujn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "id": "R3PsPzbYnt26"
      },
      "outputs": [],
      "source": [
        "#@title 감성대화말뭉치 qa multiturn 데이터 및 Train, Test 데이터 분리\n",
        "# df_t = pd.read_excel(DATA_PATH+\"감성대화말뭉치(최종데이터)_Training.xlsx\")\n",
        "# df_v = pd.read_excel(DATA_PATH+\"감성대화말뭉치(최종데이터)_Validation.xlsx\")\n",
        "# df = pd.concat([df_t,df_v],ignore_index=True)\n",
        "# text_df = df.loc[:,'사람문장1':'시스템응답4'].fillna(\"\")\n",
        "# # t = \"<P01>\"+text_df['사람문장1']+\"<P02>\"+text_df['시스템응답1'] + \\\n",
        "# #     \"<P01>\"+text_df['사람문장2']+\"<P02>\"+text_df['시스템응답2'] + \\\n",
        "# #     \"<P01>\"+text_df['사람문장3']+\"<P02>\"+text_df['시스템응답3'] + \\\n",
        "# #     \"<P01>\"+text_df['사람문장4']+\"<P02>\"+text_df['시스템응답4']\n",
        "# result = [[],[]]\n",
        "# for row in text_df.iterrows():\n",
        "#   line=\"<mask>\"+row[1]['사람문장1']\n",
        "#   result[0].append(line)\n",
        "#   result[1].append(\"<mask>\"+row[1]['시스템응답1'])\n",
        "#   line += \"<mask>\"+row[1]['시스템응답1']+\"<mask>\"+row[1]['사람문장2']\n",
        "#   result[0].append(line)\n",
        "#   result[1].append(r\"<mask>\"+row[1]['시스템응답2'])\n",
        "#   line += \"<mask>\"+row[1]['시스템응답2']+\"<mask>\"+row[1]['사람문장3']\n",
        "#   result[0].append(line)\n",
        "#   result[1].append(\"<mask>\"+row[1]['시스템응답3'])\n",
        "#   line += \"<mask>\"+row[1]['시스템응답3']+\"<mask>\"+row[1]['사람문장4']\n",
        "#   result[0].append(line)\n",
        "#   result[1].append(\"<mask>\"+row[1]['시스템응답4'])\n",
        "# t = pd.DataFrame({'Q':result[0],'A':result[1]})\n",
        "# del result,df_t,df_v\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# DF = pd.concat([personal,personal_summary,beauty,beauty_summary, \\\n",
        "#               commercial,commercial_summary,passing,passing_summary, \\\n",
        "#               food,food_summary,leisure,leisure_summary,job,job_summary,\\\n",
        "#               life,life_summary,event,event_summary,],ignore_index=True)\n",
        "# print(\"데이터 예시 \\n\",DF.head())\n",
        "# print(\"데이터 예시 \\n\",DF.tail())\n",
        "\n",
        "# TRAIN, TEST = train_test_split(DF, test_size=0.2)\n",
        "# !mkdir Chatbot_data\n",
        "# TRAIN.to_csv(\"./Chatbot_data/train.csv\",index=False) # 4GB\n",
        "# TEST.to_csv(\"./Chatbot_data/test.csv\",index=False)  # 1GB\n",
        "\n",
        "# _,DEV = train_test_split(TEST,test_size=0.015,random_state=42)\n",
        "# DEV = pd.concat([DEV,t],ignore_index=True)\n",
        "# DEV_T,DEV_V = train_test_split(DEV,test_size=0.33,random_state=42)\n",
        "# DEV.to_csv(\"./Chatbot_data/dev.csv\",index=False)\n",
        "# DEV_T.to_csv(\"./Chatbot_data/dev_t.csv\",index=False)\n",
        "# DEV_V.to_csv(\"./Chatbot_data/dev_v.csv\",index=False)\n",
        "# print(\"전체 크기:\",len(DF))\n",
        "# print(\"Train set:\",len(TRAIN))\n",
        "# print(\"Test set :\",len(TEST))\n",
        "# print(\"Dev set  :\",len(DEV))\n",
        "# del DF, t, TRAIN, TEST, DEV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm3kyKtU8OaZ"
      },
      "source": [
        "### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qOAr1QwQrP8w"
      },
      "outputs": [],
      "source": [
        "#@title Sequence 길이 분포\n",
        "import pandas as pd\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")\n",
        "train = pd.read_csv(\"/content/Chatbot_data/train.csv\").dropna()\n",
        "\n",
        "doc_len = [[len(tokenizer.encode(row[\"Q\"])),\n",
        "            len(tokenizer.encode(row[\"A\"]))] for index, row in tqdm(train.iterrows(),\n",
        "                                                                    total=len(train),\n",
        "                                                                    desc=\"시퀀스 길이 계산\")]\n",
        "print(\"Q(min) :\", np.min([doc[0] for doc in doc_len]),np.median([doc[0] for doc in doc_len]))\n",
        "print(\"Q(mean) :\", np.mean([doc[0] for doc in doc_len]),np.median([doc[0] for doc in doc_len]))\n",
        "print(\"Q(max) :\", np.max([doc[0] for doc in doc_len]),np.median([doc[0] for doc in doc_len]))\n",
        "print(\"A(min) :\", np.min([doc[1] for doc in doc_len]),np.median([doc[1] for doc in doc_len]))\n",
        "print(\"A(mean) :\", np.mean([doc[1] for doc in doc_len]),np.median([doc[1] for doc in doc_len]))\n",
        "print(\"A(max) :\", np.max([doc[1] for doc in doc_len]),np.median([doc[1] for doc in doc_len]))\n",
        "\n",
        "sns.distplot([doc[0] for doc in doc_len],label=\"Q\")\n",
        "sns.distplot([doc[1] for doc in doc_len],label=\"A\")\n",
        "plt.xlim(0, 512)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "Sy4h6Pd1hOKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af04154-f7c8-44fb-a7b4-93e5e8a6dd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_config.json\n"
          ]
        }
      ],
      "source": [
        "#@title tbstmodel_v3 - model_config.json file\n",
        "#@markdown activations : relu, gelu, gelu_new, silu\n",
        "%%writefile model_config.json\n",
        "{\n",
        "  \"_name_or_path\": \"Gunulhona/tbstmodel_v3\",\n",
        "  \"activation_dropout\": 0.0,\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"add_bias_logits\": false,\n",
        "  \"add_final_layer_norm\": false,\n",
        "  \"architectures\": [\n",
        "    \"BartForConditionalGeneration\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"author\": \"Kevin Jung(kevin.jung@turingbio.com)\",\n",
        "  \"bos_token_id\": 0,\n",
        "  \"classif_dropout\": 0.1,\n",
        "  \"classifier_dropout\": 0.1,\n",
        "  \"d_model\": 768,\n",
        "  \"decoder_attention_heads\": 16,\n",
        "  \"decoder_ffn_dim\": 3072,\n",
        "  \"decoder_layerdrop\": 0.0,\n",
        "  \"decoder_layers\": 6,\n",
        "  \"decoder_start_token_id\": 1,\n",
        "  \"do_blenderbot_90_layernorm\": true,\n",
        "  \"dropout\": 0.1,\n",
        "  \"encoder_attention_heads\": 16,\n",
        "  \"encoder_ffn_dim\": 3072,\n",
        "  \"encoder_layerdrop\": 0.0,\n",
        "  \"encoder_layers\": 6,\n",
        "  \"eos_token_id\": 1,\n",
        "  \"extra_pos_embeddings\": 2,\n",
        "  \"force_bos_token_to_be_generated\": false,\n",
        "  \"forced_eos_token_id\": 1,\n",
        "  \"gradient_checkpointing\": false,\n",
        "  \"id2label\": {\n",
        "    \"0\": \"Negative\",\n",
        "    \"1\": \"Positive\"\n",
        "  },\n",
        "  \"init_std\": 0.02,\n",
        "  \"is_encoder_decoder\": true,\n",
        "  \"kobart_version\": 1.0,\n",
        "  \"label2id\": {\n",
        "    \"Negative\": 0,\n",
        "    \"Positive\": 1\n",
        "  },\n",
        "  \"max_position_embeddings\": 1026,\n",
        "  \"model_type\": \"bart\",\n",
        "  \"normalize_before\": false,\n",
        "  \"normalize_embedding\": true,\n",
        "  \"num_hidden_layers\": 6,\n",
        "  \"bos_token_id\": 0,\n",
        "  \"eos_token_id\": 1,\n",
        "  \"pad_token_id\": 3,\n",
        "  \"scale_embedding\": false,\n",
        "  \"static_position_embeddings\": false,\n",
        "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
        "  \"torch_dtype\": \"float32\",\n",
        "  \"transformers_version\": \"4.12.5\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 30000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMbiR7TKbDtU",
        "outputId": "349b16f6-d3c5-40cc-f46f-9d634185e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_config.json\n"
          ]
        }
      ],
      "source": [
        "#@title tbstmodel_v4 - model_config.json file\n",
        "%%writefile model_config.json\n",
        "{\n",
        "  \"_name_or_path\": \"Gunulhona/tbstmodel_v4\",\n",
        "  \"activation_dropout\": 0.0,\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"architectures\": [\n",
        "    \"BartForConditionalGeneration\"\n",
        "  ],\n",
        "  \"author\": \"Kevin Jung(kevin.jung@turingbio.com)\",\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 0,\n",
        "  \"classifier_dropout\": 0.0,\n",
        "  \"d_model\": 768,\n",
        "  \"decoder_attention_heads\": 12,\n",
        "  \"decoder_ffn_dim\": 4096,\n",
        "  \"decoder_layerdrop\": 0.0,\n",
        "  \"decoder_layers\": 9,\n",
        "  \"decoder_start_token_id\": 2,\n",
        "  \"dropout\": 0.1,\n",
        "  \"encoder_attention_heads\": 12,\n",
        "  \"encoder_ffn_dim\": 4096,\n",
        "  \"encoder_layerdrop\": 0.0,\n",
        "  \"encoder_layers\": 3,\n",
        "  \"eos_token_id\": 1,\n",
        "  \"forced_eos_token_id\": 2,\n",
        "  \"id2label\": {\n",
        "    \"0\": \"LABEL_0\",\n",
        "    \"1\": \"LABEL_1\"\n",
        "  },\n",
        "  \"init_std\": 0.02,\n",
        "  \"is_encoder_decoder\": true,\n",
        "  \"label2id\": {\n",
        "    \"LABEL_0\": 0,\n",
        "    \"LABEL_1\": 1\n",
        "  },\n",
        "  \"max_position_embeddings\": 1026,\n",
        "  \"model_type\": \"bart\",\n",
        "  \"num_hidden_layers\": 2,\n",
        "  \"pad_token_id\": 3,\n",
        "  \"scale_embedding\": false,\n",
        "  \"torch_dtype\": \"float32\",\n",
        "  \"transformers_version\": \"4.26.0\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 30000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsMKvuQYz0QO",
        "outputId": "c08e8f15-9ce3-4043-82a7-9ab5df401659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting kobart_chit_chat.py\n"
          ]
        }
      ],
      "source": [
        "#@title <b><i>Training 파일 저장<i/></b>\n",
        "%%writefile kobart_chit_chat.py\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "from transformers import (BartForConditionalGeneration,\n",
        "                          BartConfig,\n",
        "                          PreTrainedTokenizerFast)\n",
        "from transformers.optimization import (AdamW, Adafactor,\n",
        "                                       get_cosine_schedule_with_warmup,\n",
        "                                       get_constant_schedule_with_warmup,\n",
        "                                       get_cosine_with_hard_restarts_schedule_with_warmup)\n",
        "from torch import optim\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from lightning.pytorch.callbacks import (TQDMProgressBar, EarlyStopping, LearningRateMonitor,\n",
        "                                         ModelCheckpoint, StochasticWeightAveraging, DeviceStatsMonitor)\n",
        "from lightning.pytorch.cli import LightningCLI, ArgsType\n",
        "# from lightning_colossalai import ColossalAIStrategy\n",
        "from lightning_utilities.core.imports import module_available\n",
        "from torchmetrics.text.bleu import BLEUScore\n",
        "\n",
        "print(f'Can Use Torch_XLA? {module_available(\"torch_xla\")}')\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='KoBART Chit-Chat')\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "CONFIG = BartConfig.from_json_file('model_config.json')\n",
        "\n",
        "def save_hf_repo(model, \n",
        "                 tokenizer):\n",
        "    MODEL_SAVE_REPO = 'Gunulhona/tbstmodel_v4'#@param {type:\"string\"}\n",
        "    HUGGINGFACE_AUTO_TOKEN = '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'\n",
        "    model.config=CONFIG\n",
        "    model.cpu().push_to_hub(MODEL_SAVE_REPO,\n",
        "\t\t\t                # use_temp_dir=True,\n",
        "\t\t\t                use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
        "    tokenizer.push_to_hub(MODEL_SAVE_REPO,\n",
        "     \t\t\t\t\t  # use_temp_dir=True,\n",
        "    \t\t\t\t\t  use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
        "    print(f'### torch kobart-chat model has saved at {MODEL_SAVE_REPO} ###')\n",
        "\n",
        "class ArgsCLI(LightningCLI):\n",
        "    def add_arguments_to_parser(self, parser):\n",
        "        parser.add_argument('--chat',\n",
        "                            action='store_true',\n",
        "                            default=False,\n",
        "                            help='response generation on given user input')\n",
        "\n",
        "        parser.add_argument('--train_file',\n",
        "                            type=str,\n",
        "                            default='Chatbot_data/dev_t.csv',\n",
        "                            help='train file')\n",
        "\n",
        "        parser.add_argument('--test_file',\n",
        "                            type=str,\n",
        "                            default='Chatbot_data/dev_v.csv',\n",
        "                            help='test file')\n",
        "\n",
        "        parser.add_argument('--tokenizer_path',\n",
        "                            type=str,\n",
        "                            default='tokenizer',\n",
        "                            help='tokenizer')\n",
        "\n",
        "        parser.add_argument('--max_seq_len',\n",
        "                            type=int,\n",
        "                            default=256,\n",
        "                            help='max sequence len')\n",
        "\n",
        "        parser.add_argument('--accumulate_grad_batches',\n",
        "                            type=int,\n",
        "                            default=1,\n",
        "                            help='accumulate_grad_batches')\n",
        "        \n",
        "        parser.add_argument('--num_workers',\n",
        "                            type=int,\n",
        "                            default=8,\n",
        "                            help='num of worker for dataloader')\n",
        "        \n",
        "        parser.add_argument('--batch_size',\n",
        "                            type=int,\n",
        "                            default=32,\n",
        "                            help='batch size for training (default: 96)')\n",
        "\n",
        "        parser.add_argument('--lr',\n",
        "                            type=float,\n",
        "                            default=5e-7,\n",
        "                            help='The initial learning rate')\n",
        "\n",
        "        parser.add_argument('--warmup_ratio',\n",
        "                            type=float,\n",
        "                            default=0.1,\n",
        "                            help='warmup ratio')\n",
        "\n",
        "        parser.add_argument('--model_path',\n",
        "                            type=str,\n",
        "                            default=None,\n",
        "                            help='kobart model path')\n",
        "        \n",
        "        parser.add_argument('--checkpoint_path',\n",
        "                            type=str,\n",
        "                            help='checkpoint path')\n",
        "\n",
        "        parser.add_argument('--default_root_dir',\n",
        "                            type=str,\n",
        "                            help='default_root_dir')\n",
        "\n",
        "# few shot dialouge Generation\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 filepath:str,\n",
        "                 tokenizer_path:str,\n",
        "                 max_seq_len:int=256) -> None:\n",
        "        super().__init__()\n",
        "        self.filepath = filepath\n",
        "        self.data = pd.read_csv(self.filepath).dropna()\n",
        "        self.bos_token = '<s>'\n",
        "        self.eos_token = '</s>'\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "\n",
        "    def _resize_token_embeddings(self):\n",
        "        tokens = {\"additional_special_tokens\":[\"<P01>\",\"<P02>\",\"<P03>\",\"<P04>\",\"<P05>\",\"<P06>\",\"<P07>\",\"<P08>\",\"<P09>\"]}\n",
        "        self.tokenizer.add_special_tokens(tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def make_input_id_mask(self, \n",
        "                           tokens,\n",
        "                           index):\n",
        "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_id)\n",
        "        if len(input_id) < self.max_seq_len:\n",
        "            while len(input_id) < self.max_seq_len:\n",
        "                input_id += [self.tokenizer.pad_token_id]\n",
        "                attention_mask += [0]\n",
        "        else:\n",
        "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
        "            input_id = input_id[:self.max_seq_len - 1] + [self.tokenizer.eos_token_id]\n",
        "            attention_mask = attention_mask[:self.max_seq_len]\n",
        "        return input_id, attention_mask\n",
        "\n",
        "    def masking(self,\n",
        "                input,\n",
        "                mask):\n",
        "        input = torch.tensor([input])\n",
        "        rand = torch.rand(input.shape)\n",
        "        mask_arr = (rand < 0.15) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n",
        "        mask_ids = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
        "        input[0,mask_ids] = mask\n",
        "        del rand, mask_arr\n",
        "        return input.tolist()[0]\n",
        "\n",
        "    def _labeling(self,\n",
        "                  label):\n",
        "        tokens = [self.tokenizer.bos_token]+self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n",
        "        label_ids = self.tokenizer.convert_tokens_to_ids(tokens[1:])\n",
        "        if len(label_ids) < self.max_seq_len:\n",
        "            while len(label_ids)<self.max_seq_len:\n",
        "                label_ids+=[-100]\n",
        "        else:\n",
        "            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n",
        "        del tokens\n",
        "        return label_ids\n",
        "\n",
        "    def __getitem__(self, \n",
        "                    index):\n",
        "        record = self.data.iloc[index]\n",
        "        # samples = self.data.sample(n=3)\n",
        "        q, a = record['Q'], record['A']\n",
        "        q_tokens = [self.eos_token] + self.tokenizer.tokenize(q) + [self.eos_token]\n",
        "        a_tokens = [self.eos_token] + self.tokenizer.tokenize(a) + [self.eos_token]\n",
        "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(q_tokens, index)\n",
        "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(a_tokens[:-1], index)\n",
        "        # labels = self._labeling(a)\n",
        "        labels = self.tokenizer.convert_tokens_to_ids(a_tokens[1:(self.max_seq_len + 1)])\n",
        "        # encoder_input_id = self.masking(encoder_input_id, self.tokenizer.mask_token_id)\n",
        "        if len(labels) < self.max_seq_len:\n",
        "            while len(labels) < self.max_seq_len:\n",
        "                # for cross entropy loss masking\n",
        "                labels += [-100]\n",
        "        # encoder_input_id = self.masking(encoder_input_id, self.tokenizer.mask_token_id)\n",
        "        del record, q, a, q_tokens, a_tokens\n",
        "        return {\n",
        "                    'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
        "                    'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
        "                    'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
        "                    'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
        "                    'labels': np.array(labels, dtype=np.int_)\n",
        "               }\n",
        "\n",
        "\n",
        "class ChatDataModule(pl.core.LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 train_file,\n",
        "                 test_file,\n",
        "                 tokenizer_path,\n",
        "                 max_seq_len=512,\n",
        "                 batch_size=64,\n",
        "                 num_workers=8):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size # batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.train_file_path = train_file\n",
        "        self.test_file_path = test_file\n",
        "        self.tokenizer_path = tokenizer_path\n",
        "        self.num_workers = num_workers\n",
        "        self.prepare_data_pre_node = True\n",
        "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
        "    \n",
        "    def setup(self,\n",
        "              stage):\n",
        "        # split dataset\n",
        "        self.train = ChatDataset(self.train_file_path,\n",
        "                                 self.tokenizer_path,\n",
        "                                 self.max_seq_len)\n",
        "\n",
        "        self.test = ChatDataset(self.test_file_path,\n",
        "                                self.tokenizer_path,\n",
        "                                self.max_seq_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          # pin_memory=True,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.test,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers= self.num_workers,\n",
        "                          # pin_memory=True,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers= self.num_workers,\n",
        "                          # pin_memory=True,\n",
        "                          shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class Base(pl.core.LightningModule):\n",
        "    def __init__(self,\n",
        "                 lr,\n",
        "                 batch_size,\n",
        "                 warmup=0.1,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.lr=lr\n",
        "        self.warmup_ratio = warmup\n",
        "        self.batch_size=batch_size\n",
        "       # self.save_hyperparameters()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr = self.lr,\n",
        "                          correct_bias=False,\n",
        "                          no_deprecation_warning=True)\n",
        "        # optimizer = optim.AdamW(params=optimizer_grouped_parameters,\n",
        "        #                         lr=self.lr,\n",
        "        #                         weight_decay=0.1)\n",
        "\n",
        "        # optimizer = Adafactor(optimizer_grouped_parameters,\n",
        "        #                       lr=self.lr,\n",
        "        #                       beta1=0.9,\n",
        "        #                       relative_step=False,\n",
        "        #                       warmup_init=True,\n",
        "        #                       weight_decay=0.1,)\n",
        "\n",
        "        # warm up lr\n",
        "        num_nodes = self.trainer.strategy.num_nodes\n",
        "        devices = len(self.trainer.strategy.parallel_devices) if type(self.trainer.strategy.parallel_devices) is list else self.trainer.strategy.parallel_devices\n",
        "        num_workers = int(devices) * int(num_nodes)\n",
        "        data_len = len(self.trainer.fit_loop._data_source.instance.train)\n",
        "       # data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader().dataset)\n",
        "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
        "\n",
        "        num_train_steps = int(data_len / (self.batch_size * num_workers) * self.trainer.max_epochs)\n",
        "        logging.info(f'num_train_steps : {num_train_steps}')\n",
        "\n",
        "        num_warmup_steps = int(num_train_steps * self.warmup_ratio)\n",
        "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
        "\n",
        "        # get_cosine_schedule_with_warmup get_cosine_with_hard_restarts_schedule_with_warmup get_constant_schedule_with_warmup\n",
        "        scheduler = \\\n",
        "            get_cosine_schedule_with_warmup(\n",
        "                optimizer=optimizer,\n",
        "                num_warmup_steps=num_warmup_steps,\n",
        "                num_training_steps=num_train_steps,\n",
        "                num_cycles=0.5)\n",
        "\n",
        "        self.lr_scheduler = {'scheduler': scheduler,\n",
        "                        'monitor': 'loss',\n",
        "                        'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [self.lr_scheduler]\n",
        "        # return optimizer\n",
        "\n",
        "   # def optimizer_step(self, \n",
        "   #                    epoch,\n",
        "   #                    batch_idx,\n",
        "   #                    optimizer,\n",
        "   #                    optimizer_idx,\n",
        "   #                    optimizer_closure=None,\n",
        "   #                    second_order_closure=None,\n",
        "   #                    using_native_amp=False,\n",
        "   #                    on_tpu=None,\n",
        "   #                    using_lbfgs=None,):\n",
        "   #     if self.trainer._accelerator_connector._accelerator_flag == \"tpu\":\n",
        "   #         xm.optimizer_step(optimizer)\n",
        "   #     else:\n",
        "   #         optimizer.step(closure=optimizer_closure)\n",
        "   #     optimizer.step(closure=optimizer_closure)\n",
        "   #     optimizer.zero_grad()\n",
        "   #     self.lr_scheduler.step()\n",
        "\n",
        "   # def optimizer_zero_grad(self,\n",
        "   #                         epoch,\n",
        "   #                         batch_idx,\n",
        "   #                         optimizer,\n",
        "   #                         optimizer_idx):\n",
        "   #     optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "   # def backward(self,\n",
        "   #              loss,\n",
        "   #              optimizer,\n",
        "   #              optimizer_idx):\n",
        "   #     loss.backward()\n",
        "\n",
        "\n",
        "class KoBARTConditionalGeneration(Base):\n",
        "    def __init__(self,\n",
        "                 lr,\n",
        "                 batch_size,    \n",
        "                 model_path,\n",
        "                 tokenizer_path,\n",
        "                 warmup=0.1,\n",
        "                 **kwargs):\n",
        "        super().__init__(lr, warmup, batch_size, **kwargs)\n",
        "        try:\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(model_path,)\n",
        "        except:\n",
        "            print(f\"Error) Cannot read model from {model_path}\\n\"\n",
        "                  f\"exception in porgress\")\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(model_path,)\n",
        "                                                                       # revision=\"b6451e14d59c84af077bd5e1e6447437f4a21d0b\")\n",
        "       # self.model.config = CONFIG\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")\n",
        "        self.bos_token = '<s>'\n",
        "        self.eos_token = '</s>'\n",
        "        # self.model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\"))\n",
        "        self.model.train().share_memory()\n",
        "\n",
        "    def mle_loss(self,\n",
        "                 logits,\n",
        "                 labels):\n",
        "        lprobs = F.log_softmax(logits, dim=-1)\n",
        "        return -F.nll_loss(lprobs.view(-1, self.model.config.vocab_size),\n",
        "                           labels.view(-1),\n",
        "                           reduction='sum')\n",
        "\n",
        "    def _resize_token_embeddings(self):\n",
        "        tokens = {\"additional_special_tokens\":[\"<P01>\",\"<P02>\",\"<P03>\",\"<P04>\",\"<P05>\",\"<P06>\",\"<P07>\",\"<P08>\",\"<P09>\"]}\n",
        "        self.tokenizer.add_special_tokens(tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def forward(self,\n",
        "                inputs):\n",
        "        return self.model(input_ids=inputs['input_ids'],\n",
        "                          attention_mask=inputs['attention_mask'],\n",
        "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
        "                          decoder_attention_mask=inputs['decoder_attention_mask'],\n",
        "                          labels=inputs['labels'],\n",
        "                          return_dict=True)\n",
        "        # output.loss += self.mle_loss(output.logits, inputs['labels'])\n",
        "        # return output\n",
        "\n",
        "    def training_step(self,\n",
        "                      batch,\n",
        "                      batch_idx):\n",
        "        outs = self(batch)\n",
        "        self.log('train_loss', outs.loss.detach().cpu(), prog_bar=True, on_step=True, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        # self.log('perplexity', torch.exp(outs.loss), prog_bar=True, on_step=True, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        return outs.loss\n",
        "\n",
        "    # def training_step_end(self,\n",
        "    #                       training_step_outputs):\n",
        "    #     return training_step_outputs\n",
        "\n",
        "    # def training_epoch_end(self,\n",
        "    #                        training_step_outputs):\n",
        "    #     return training_step_outputs\n",
        "\n",
        "    def validation_step(self,\n",
        "                        batch,\n",
        "                        batch_idx):\n",
        "        outs = self(batch)\n",
        "        self.log('val_loss', outs['loss'].detach().cpu(), prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        # self.log('val_perplexity', torch.exp(outs['loss']), prog_bar=True, on_step=False, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "\n",
        "    # def validation_epoch_end(self,\n",
        "    #                          outputs):\n",
        "    #     bleu = bleu_score(self.predictions, self.targets)\n",
        "\n",
        "    def chat(self, \n",
        "             text):\n",
        "        input_ids =  [self.tokenizer.eos_token_id] + self.tokenizer.encode(text) + [self.tokenizer.eos_token_id]\n",
        "        res_ids = self.model.generate(torch.tensor([input_ids]),\n",
        "                                      num_beams=10,\n",
        "                                      top_p=0.8,\n",
        "                                      top_k=200,\n",
        "                                      temperature=1,\n",
        "                                      max_length=60,\n",
        "                                      min_length=20,\n",
        "                                      length_penalty=0.65,\n",
        "                                      do_sample=True,\n",
        "                                      repetition_penalty=0.83,\n",
        "                                      no_repeat_ngram_size=3,\n",
        "                                      encoder_no_repeat_ngram_size=3,\n",
        "                                      num_return_sequences=5)\n",
        "        a = self.tokenizer.batch_decode(res_ids.tolist())\n",
        "        for x in a:\n",
        "            print(\"생성 문장\",x)\n",
        "        return a[0].replace('<s>', '').replace('</s>', '').replace('<usr>','').replace('<sys>','')\n",
        "\n",
        "\n",
        "def cli_main(): # args:ArgsType=None\n",
        "    callbacks = [ModelCheckpoint(monitor='val_loss',\n",
        "                                 dirpath=\"logs/\" ,# args.default_root_dir,\n",
        "                                 filename='best-checkpoint',\n",
        "                                 verbose=True,\n",
        "                                 save_last=True,\n",
        "                                 mode='min',\n",
        "                                 save_top_k=1),\n",
        "                # pl_loggers.TensorBoardLogger(save_dir=os.path.join(args.default_root_dir,'tb_logs'),\n",
        "                #                              log_graph=False,),\n",
        "                 LearningRateMonitor(),\n",
        "                 EarlyStopping(monitor=\"val_loss\",\n",
        "                               mode=\"min\",\n",
        "                               stopping_threshold=1e-4,\n",
        "                               min_delta=0.00,\n",
        "                               patience=1,\n",
        "                               divergence_threshold=9.0),\n",
        "                 TQDMProgressBar(refresh_rate=50),\n",
        "                # StochasticWeightAveraging(swa_lrs=1e-2), # 23/04/24 기준 lightning에서 오류 있는 callback\n",
        "                 DeviceStatsMonitor()]\n",
        "   # strategy = ColossalAIStrategy(use_chunk=True,\n",
        "   #                               enable_distributed_storage=True,\n",
        "   #                               placement_policy='cuda',\n",
        "   #                               initial_scale=32)\n",
        "    cli = ArgsCLI(model_class=KoBARTConditionalGeneration,\n",
        "                  datamodule_class=ChatDataModule,\n",
        "                  trainer_defaults={\"callbacks\":callbacks},\n",
        "                  # args=args\n",
        "                  )\n",
        "    cli.trainer.fit(model=cli.model,\n",
        "                    datamodule=cli.datamodule)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pl.cli_lightning_logo()\n",
        "   # args = parser.parse_args()\n",
        "   # logging.info(args)\n",
        "\n",
        "    cli_main()\n",
        "    # parser = pl.Trainer().add_argparse_args(parser)\n",
        "    # model = KoBARTConditionalGeneration(args)\n",
        "    # dm = ChatDataModule(train_file=args.train_file,\n",
        "    #                     test_file=args.test_file,\n",
        "    #                     tok_vocab=args.tokenizer_path,\n",
        "    #                     batch_size=args.batch_size,\n",
        "    #                     max_seq_len=args.max_seq_len,\n",
        "    #                     num_workers=args.num_workers)\n",
        "\n",
        "    # trainer = pl.Trainer.from_argparse_args(args,\n",
        "    #                                         logger=tb_logger,\n",
        "    #                                         callbacks=[checkpoint_callback,\n",
        "    #                                                    ds_callback,\n",
        "    #                                                    lr_logger,\n",
        "    #                                                    es_callback,\n",
        "    #                                                    refresh_callback])\n",
        "\n",
        "    # cli.trainer.fit(model=cli.model, datamodule=cli.datamodule) # ckpt_path=args.default_root_dir+\"/last.ckpt\")\n",
        "    print(\"\\nTrain Done\\n\")\n",
        "    save_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\"\n",
        "    save_hf_repo(model.model.cpu(), model.tokenizer)\n",
        "\n",
        "    if args.chat:\n",
        "        model.model.eval()\n",
        "        # que=[]\n",
        "        while 1:\n",
        "            q = input('user > ').strip()\n",
        "            # if len(que) > 5:\n",
        "            #   que.pop(0)\n",
        "            # que.append(\"<usr>\"+q)\n",
        "            if q in ['quit', 'q', 'Q', 'Quit', 'QUIT', 'exit', 'end']:\n",
        "                break\n",
        "            elif q == 'save':\n",
        "                save_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\"\n",
        "                torch.save(model.model.state_dict(),save_path)\n",
        "                print(f'kobart-chat model.pth has saved at {save_path}')\n",
        "            # else:\n",
        "            #     text=\"\"\n",
        "            #     for x in que:\n",
        "            #         text+=x\n",
        "            # print(text)\n",
        "            result=model.chat(q)\n",
        "            # que.append(result)\n",
        "            print(\"Simsimi > {}\".format(result))\n",
        "    else:\n",
        "        # torch.save(model.model.state_dict(),save_path)\n",
        "        print(f'### torch kobart-chat model.pth has saved at {save_path} ###')\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VWqKWADpq6V",
        "outputId": "193f40b9-f17c-4d2f-e55b-bad0a6701cdf",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer.sh\n"
          ]
        }
      ],
      "source": [
        "#@title trainer.sh : training hyperparameters\n",
        "#@markdown gogamza/kobart-base-v1 / hyunwoongko/kobart / Gunulhona/tbsentmodel_v1 \\\n",
        "#@markdown rm -rdf logs\n",
        "#     python -m torch_xla.distributed.xla_dist \\\n",
        "#       --tpu_cores 8\\\n",
        "#       --resume_from_checkpoint logs/last.ckpt\\\n",
        "%%writefile trainer.sh\n",
        "TRAIN_FILE=Chatbot_data/train0.csv #@param [\"Chatbot_data/train0.csv\", \"Chatbot_data/train1.csv\", \"Chatbot_data/train2.csv\", \"Chatbot_data/train3.csv\", \"Chatbot_data/train4.csv\", \"Chatbot_data/train5.csv\", \"Chatbot_data/train6.csv\", \"Chatbot_data/train7.csv\", \"Chatbot_data/train8.csv\", \"Chatbot_data/train9.csv\", \"Chatbot_data/train.csv\", \"Chatbot_data/dev.csv\", \"Chatbot_data/dev_small.csv\"] {type:\"raw\"}\n",
        "TEST_FILE=Chatbot_data/dev_small.csv #@param[\"Chatbot_data/dev_small.csv\", \"Chatbot_data/dev.csv\", \"Chatbot_data/test.csv\"] {type:\"raw\"}\n",
        "MAX_SEQ_LEN=181 #@param [\"128\", \"181\", \"256\", \"512\", \"768\", \"1024\", \"2048\", \"4096\"] {type:\"raw\", allow-input: true}\n",
        "LEARNING_RATE=5e-5 #@param {type:\"raw\"}\n",
        "GRADIENT_CLIP=1.0 #@param {type:\"raw\"}\n",
        "ACCUMULATE_GRAD_BATCHES=4 #@param {type:\"raw\"}\n",
        "BATCH_SIZE=32 #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"512\", \"4096\", \"32768\", \"262144\", \"2097152\", \"16777216\", \"134217728\", \"1073741824\", \"8589934592\", \"68719476736\"] {type:\"raw\", allow-input: true}\n",
        "MAXEPOCHS=2 #@param {type:\"raw\"}\n",
        "STRATEGY=xla #@param [\"auto\", \"bagua\", \"colossalai\", \"collaborative\", \"ddp\", \"ddp2\", \"ddp_find_unused_parameters_false\", \"ddp_fully_sharded\", \"ddp_sharded\", \"ddp_sharded_find_unused_parameters_false\", \"ddp_sharded_spawn\", \"ddp_sharded_spawn_find_unused_parameters_false\", \"ddp_spawn\", \"ddp_spawn_find_unused_parameters_false\", \"ddp_fork\" ,\"deepspeed\", \"deepspeed_stage_1\", \"deepspeed_stage_2\", \"deepspeed_stage_2_offload\", \"deepspeed_stage_3\", \"deepspeed_stage_3_offload\", \"deepspeed_stage_3_offload_nvme\", \"dp\", \"fsdp\", \"fsdp_native\", \"horovod\", \"hpu_parallel\", \"hpu_single\", \"ipu_strategy\", \"single_device\", \"single_tpu\", \"tpu_spawn\", \"tpu_spawn_debug\", \"xla\", \"xla_debug\"]{type:\"raw\", allow-input: false}\n",
        "MODEL_NAME=Gunulhona/tbstmodel_v4 #@param [\"gogamza/kobart-base-v1\", \"Gunulhona/tbstmodel_v2\"] {type:\"raw\", allow-input: true}\n",
        "TOKENIZER_PATH=Gunulhona/tbbarttokenizer #@param [\"gogamza/kobart-base-v1\", \"Gunulhona/tbbarttokenizer\"] {type:\"raw\", allow-input: false}\n",
        "NUMWORKERS=2 #@param {type:\"raw\"}\n",
        "WARMUP=0.1 #@param {type:\"raw\"}\n",
        "PROFILE=xla #@param [\"simple\", \"advanced\", \"pytorch\", \"xla\"]  {type:\"raw\", allow-input: false}\n",
        "FAST_DEV_RUN=false #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n",
        "SYNC_BATCHNORM=true #@param [\"true\", \"false\"] {type:\"raw\", allow-input: false}\n",
        "CORES=8 #@param {type:\"raw\"}\n",
        "\n",
        "for i in Chatbot_data/fold/*; do \\\n",
        "    echo $i $TOKENIZER_PATH\n",
        "    python \\\n",
        "        /content/kobart_chit_chat.py fit\\\n",
        "        --trainer.accumulate_grad_batches $ACCUMULATE_GRAD_BATCHES\\\n",
        "        --trainer.gradient_clip_val $GRADIENT_CLIP\\\n",
        "        --trainer.max_epochs $MAXEPOCHS\\\n",
        "        --trainer.precision bf16-mixed\\\n",
        "        --trainer.accelerator tpu\\\n",
        "        --trainer.devices $CORES\\\n",
        "        --trainer.use_distributed_sampler true\\\n",
        "        --trainer.sync_batchnorm $SYNC_BATCHNORM\\\n",
        "        --trainer.profiler $PROFILE\\\n",
        "        --trainer.strategy $STRATEGY\\\n",
        "        --trainer.num_sanity_val_steps 3\\\n",
        "        --trainer.log_every_n_steps 30\\\n",
        "        --trainer.fast_dev_run $FAST_DEV_RUN\\\n",
        "        --model.lr $LEARNING_RATE\\\n",
        "        --model.model_path  $MODEL_NAME\\\n",
        "        --model.warmup $WARMUP\\\n",
        "        --model.tokenizer_path $TOKENIZER_PATH\\\n",
        "        --model.batch_size $BATCH_SIZE\\\n",
        "        --data.max_seq_len $MAX_SEQ_LEN\\\n",
        "        --data.train_file $i\\\n",
        "        --data.test_file $TEST_FILE\\\n",
        "        --data.tokenizer_path $TOKENIZER_PATH\\\n",
        "\n",
        "done\n",
        "exit 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz-jB6euSRqD"
      },
      "outputs": [],
      "source": [
        "!rm -rdf logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqySdNofEb1q"
      },
      "source": [
        "<p>텐서보드 열기</p>\n",
        "스크래치 셀에서 열어서 같이 보기 CTRL + ALT + \"N\"\n",
        "\n",
        "> xla profile default address => grpc://x.x.x.x:8466 \\\n",
        " xla client profile default address => localhost:9012\n",
        "```shell\n",
        "!echo [ $TPU_NAME ]\n",
        "!export TPU_LOAD_LIBRARY=0\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logs --load_fast=false --port 9001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ACv1xGtqvL5"
      },
      "source": [
        "### TPU 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA2sKQcYk9se",
        "outputId": "796fc06a-7c20-4f5d-b251-d9d8c342b4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting xmp_test.py\n"
          ]
        }
      ],
      "source": [
        "#@title XMP TEST\n",
        "%%writefile xmp_test.py\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "from lightning_utilities.core.imports import module_available\n",
        "print(f'Lightning TPU? {module_available(\"torch_xla\")}')\n",
        "\n",
        "# Common Cloud TPU computation but different CPU computation is OK\n",
        "test_bart = BartForConditionalGeneration.from_pretrained(\"Gunulhona/tbstmodel_v4\").eval()\n",
        "bart_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")\n",
        "\n",
        "def simple_map_fn(index, flags):\n",
        "  torch.manual_seed(1234)\n",
        "  device = xm.xla_device()\n",
        "  test_bart.to(device)\n",
        "  input_ids = bart_tokenizer(\n",
        "                \"<s>안녕하세요!</s>\",\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=128,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                add_special_tokens=True,\n",
        "                return_token_type_ids=False,\n",
        "                verbose=True,\n",
        "                ).input_ids.to(device)\n",
        "  print(\"Process\", index ,\"is using\", xm.xla_real_devices([str(device)])[0])\n",
        "  time.sleep(3)\n",
        "  print(xm.xla_real_devices([str(device)])[0],\"sleep end\")\n",
        "  t = torch.randn((2, 2), device=device)  # Common Cloud TPU computation\n",
        "  out = str(t)  # Each process uses the XLA tensors the same way\n",
        "\n",
        "  if xm.is_master_ordinal():  # Divergent CPU-only computation (no XLA tensors beyond this point!)\n",
        "    print(test_bart.device, t.cpu())\n",
        "\n",
        "  print(xm.xla_real_devices([str(test_bart.device)])[0],\n",
        "        bart_tokenizer.batch_decode(test_bart.generate(input_ids,temperature=2.3, max_length=5)))\n",
        "\n",
        "  # Barrier to prevent master from exiting before workers connect.\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "def XMP_TEST():\n",
        "  flags={}\n",
        "  xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  gc.collect()\n",
        "  XMP_TEST()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJcc6yqrY-hU",
        "outputId": "6a374cab-4ff8-4f45-e042-b92f43258a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "Process 0 is using TPU:0\n",
            "TPU:0 sleep end\n",
            "xla:1 tensor([[-0.6989, -0.0987],\n",
            "        [ 0.7337, -0.9071]])\n",
            "Process 3 is using TPU:3\n",
            "Process 4 is using TPU:4\n",
            "TPU:3 sleep end\n",
            "Process 2 is using TPU:2\n",
            "TPU:4 sleep end\n",
            "2023-04-24 03:05:02.918851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Process 7 is using TPU:7\n",
            "Process 6 is using TPU:6\n",
            "TPU:2 sleep end\n",
            "Process 1 is using TPU:1\n",
            "TPU:0 ['<usr> 오오오<usr>']\n",
            "2023-04-24 03:05:05.423122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Process 5 is using TPU:5\n",
            "TPU:7 sleep end\n",
            "2023-04-24 03:05:06.747195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:3 ['<usr> 오오오<usr>']\n",
            "TPU:6 sleep end\n",
            "2023-04-24 03:05:07.433567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:1 sleep end\n",
            "TPU:4 ['<usr> 오오오<usr>']\n",
            "2023-04-24 03:05:08.512301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:5 sleep end\n",
            "TPU:2 ['<usr> 오오오<usr>']\n",
            "2023-04-24 03:05:09.298898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:7 ['<usr> 오오오<usr>']\n",
            "2023-04-24 03:05:10.136300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:6 ['<usr> 오오오<usr>']\n",
            "2023-04-24 03:05:11.080922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "TPU:1 ['<usr> 오오오<usr>']\n",
            "TPU:5 ['<usr> 오오오<usr>']\n"
          ]
        }
      ],
      "source": [
        "!python xmp_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0RHSDGYjc2"
      },
      "source": [
        "### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJErWl0lYI-d",
        "outputId": "dc3818b1-b9b8-4df0-e125-390ce5c3d64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grpc://10.30.45.122:8470   Timestamp: 08:56:10\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "\n",
            "Chatbot_data/fold/train_27019_1.csv Gunulhona/tbbarttokenizer\n",
            "Can Use Torch_XLA? True\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "\n",
            "\u001b[0;35m\n",
            "                    ####\n",
            "                ###########\n",
            "             ####################\n",
            "         ############################\n",
            "    #####################################\n",
            "##############################################\n",
            "#########################  ###################\n",
            "#######################    ###################\n",
            "####################      ####################\n",
            "##################       #####################\n",
            "################        ######################\n",
            "#####################        #################\n",
            "######################     ###################\n",
            "#####################    #####################\n",
            "####################   #######################\n",
            "###################  #########################\n",
            "##############################################\n",
            "    #####################################\n",
            "         ############################\n",
            "             ####################\n",
            "                  ##########\n",
            "                     ####\n",
            "\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3215167523\n",
            "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
            "INFO: Global seed set to 3215167523\n",
            "INFO:lightning.fabric.utilities.seed:Global seed set to 3215167523\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: True, using: 8 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: True, using: 8 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "#@title 실행\n",
        "import os\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "import gc\n",
        "gc.collect()\n",
        "os.environ[\"USE_TORCH\"] = \"ON\" #@param {type:\"raw\"}\n",
        "os.environ[\"TPU_LOG_DIR\"] = \"disabled\" #@param {type:\"raw\"}\n",
        "os.environ[\"_TPU_AVAILABLE\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"TPU_NUM_DEVICES\"] = \"8\" #@param {type:\"raw\"}\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"TRIM_GRAPH_SIZE\"] = \"1000000\" #@param {type:\"raw\"}\n",
        "# os.environ[\"MALLOC_MMAP_THRESHOLD_\"] = \"134961168\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"1000000000\" #@param {type:\"raw\"}\n",
        "os.environ[\"PR_SET_PDEATHSIG\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"PL_RECONCILE_PROCESS\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_USE_BF16\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_DOWNCAST_BF16\"]=\"1\" #@param {type:\"raw\"\n",
        "os.environ[\"XLA_USE_32BIT_LONG\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_IR_DEBUG\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_HLO_DEBUG\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"PT_XLA_DEBUG\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_SYNC_WAIT\"] = \"1\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_GET_TENSORS_OPBYOP\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"XLA_SYNC_TENSORS_OPBYOP\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = \"nlcc\" #@param ['\\\"nlcc\\\"', '\\\"gloo\\\"'] {type:\"raw\", allow-input: false}\n",
        "os.environ[\"XLA_CUDA\"] = \"0\" #@param {type:\"raw\"}\n",
        "os.environ[\"BUNDLE_LIBTPU\"] = \"1\" #@param {type:\"raw\"}\n",
        "# os.environ[\"XRT_TPU_CONFIG\"]=\"localservice;0;localhost:51011\" #@param {type:\"raw\"}\n",
        "# os.environ[\"XRT_WORKERS\"] = \"localservice:0;grpc://localhost:9002\" #@param {type:\"raw\"}\n",
        "os.environ[\"NCCL_DEBUG\"] = \"INFO\" #@param {type:\"raw\"}\n",
        "tpu_profile_service_address = os.environ[\"COLAB_TPU_ADDR\"].replace('8470', '8466')\n",
        "print(os.environ[\"TPU_NAME\"], profiler_client.monitor(tpu_profile_service_address, 100, 2))\n",
        "assert os.environ[\"COLAB_TPU_ADDR\"], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "# !rm -r logs\n",
        "# !mkdir logs/tb_logs/lightning_logs\n",
        "!bash trainer.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_A0QexsQbaJe"
      },
      "outputs": [],
      "source": [
        "#@title chat.sh\n",
        "#gogamza/kobart-base-v1 hyunwoongko/kobart\n",
        "# batch-size 64/512/4096\n",
        "%%writefile chat.sh\n",
        "python \"/content/kobart_chit_chat.py\"\\\n",
        "  --gradient_clip_val 1.0\\\n",
        "  --max_epochs 1\\\n",
        "  --precision bf16\\\n",
        "  --default_root_dir logs\\\n",
        "  --train_file \"Chatbot_data/dev.csv\"\\\n",
        "  --test_file \"Chatbot_data/dev.csv\"\\\n",
        "  --model_path  \"Gunulhona/tbstmodel\"\\\n",
        "  --tokenizer_path \"Gunulhona/tbbarttokenizer\"\\\n",
        "  --num_sanity_val_steps 0\\\n",
        "  --chat\\\n",
        "  --resume_from_checkpoint \"/content/logs/last.ckpt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "61ZDYjiSbogr"
      },
      "outputs": [],
      "source": [
        "#@title 테스트 채팅\n",
        "!bash chat.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vsOq9keGAC4F"
      },
      "outputs": [],
      "source": [
        "#@title 강제 저장\n",
        "from kobart_chit_chat import KoBARTConditionalGeneration\n",
        "pl_model = KoBARTConditionalGeneration({\"gradient_clip_val\":1.0,\n",
        "                            \"max_epochs\":1,\n",
        "                            \"lr\":5e-8,\n",
        "                            \"batch-size\": 8,\n",
        "                            \"precision\":16,\n",
        "                            \"default_root_dir\":\"logs\",\n",
        "                            \"train_file\":\"Chatbot_data/train2.csv\",\n",
        "                            \"test_file\":\"Chatbot_data/dev.csv\",\n",
        "                            \"model_path\":\"gogamza/kobart-base-v1\",\n",
        "                            \"resume_from_checkpoint\":\"/content/logs/last.ckpt\"})\n",
        "import torch\n",
        "pl_model = pl_model.load_from_checkpoint(checkpoint_path=\"/content/logs/last.ckpt\")\n",
        "torch.save(pl_model.model.state_dict(),\n",
        "           \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zY2vudD07MP5"
      },
      "outputs": [],
      "source": [
        "#@title Statedict 로딩 테스트\n",
        "from transformers import BartForConditionalGeneration\n",
        "import torch\n",
        "# BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v1\").load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\"))\n",
        "BartForConditionalGeneration.from_pretrained(\"Gunulhona/tbstmodel_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QZrYgHRVOJ0L"
      },
      "outputs": [],
      "source": [
        "#@title Push to Huggingface Repository\n",
        "import torch\n",
        "from transformers import BartConfig\n",
        "from kobart_chit_chat import KoBARTConditionalGeneration\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")\n",
        "tokenizer.model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "HF_REPO_NAME = \"Gunulhona/tbstmodel_v2\" #@param {type:\"string\"}\n",
        "pl_model = KoBARTConditionalGeneration({\"gradient_clip_val\":1.0,\n",
        "                                         \"max_epochs\": 20 ,\n",
        "                                         \"default_root_dir\": \"logs\",\n",
        "                                         \"train_file\": \"Chatbot_data/fold/train.csv\",\n",
        "                                         \"test_file\": \"Chatbot_data/test.csv\",\n",
        "                                         \"batch_size\": 32,\n",
        "                                         \"avg_type\": \"norm_avg\",\n",
        "                                         \"subtask\": \"STS\",\n",
        "                                         \"model_path\":HF_REPO_NAME,\n",
        "                                         \"tokenizer_path\":\"Gunulhona/tbbarttokenizer\",\n",
        "                                         \"accumulate_grad_batches\": 1,\n",
        "                                         \"resume_from_checkpoint\":\"/content/logs/last.ckpt\"})\n",
        "\n",
        "pl_model = pl_model.load_from_checkpoint(checkpoint_path=\"/content/logs/last.ckpt\")\n",
        "\n",
        "def save_hf_repo(model, config, repo_name):\n",
        "    MODEL_SAVE_REPO = repo_name\n",
        "    HUGGINGFACE_AUTO_TOKEN = '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'\n",
        "    # model.config=config\n",
        "    model.cpu().push_to_hub(MODEL_SAVE_REPO,\n",
        "\t\t\t                use_temp_dir=True,\n",
        "\t\t\t                use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
        "#     tokenizer.push_to_hub(MODEL_SAVE_REPO,\n",
        "#                           use_tmep_dir=True,\n",
        "#                           use_auth_token=HUGGINGFACE_AUTO_TOKEN)\n",
        "\n",
        "save_hf_repo(model=pl_model.model.cpu(),\n",
        "             config=BartConfig.from_json_file('model_config.json'),\n",
        "             repo_name=HF_REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR6hNP6BWta0"
      },
      "source": [
        "# 숨김\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz8KK3i4rqgK"
      },
      "source": [
        "## Prompt 기반 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EF4Uv2ECJpXB"
      },
      "outputs": [],
      "source": [
        "#@title Data function\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def make_prompt(dinfo, pinfo):\n",
        "    return [f'[{dinfo[\"numberOfParticipants\"]}] 명이 나눈 [{dinfo[\"topic\"]}] 관련된 [{dinfo[\"type\"]}] 에 대해 다음 [발화] 를 [생성] //\\n',\n",
        "            f'',\n",
        "            f'[주제] {dinfo[\"topic\"]} {dinfo[\"type\"]}//\\n',\n",
        "            \",\".join([f'{p[\"age\"]} {p[\"gender\"]}'for p in pinfo])+f',{dinfo[\"type\"]},{dinfo[\"topic\"]} 의 정보로 작성된 [대화]//\\n',\n",
        "            \",\".join([f'{p[\"residentialProvince\"]}거주 {p[\"age\"]}'for p in pinfo])+f',{dinfo[\"type\"]},{dinfo[\"topic\"]}//\\n']\n",
        "\n",
        "def sns_chat(file_path, summary=False):\n",
        "    temp = [[],[]]\n",
        "    mask=\"<mask>\"\n",
        "    data = pd.read_json(file_path,\n",
        "             encoding_errors=\"ignore\")\n",
        "    for index, row in data.iterrows():\n",
        "        d_info = row[\"data\"][\"header\"][\"dialogueInfo\"]\n",
        "        p_info = row[\"data\"][\"header\"][\"participantsInfo\"]\n",
        "        prompt =f'{mask} {d_info[\"numberOfParticipants\"]}명이 나눈 {d_info[\"topic\"]} 관련된 {d_info[\"type\"]}//\\n'\n",
        "        end_word = [\".\",\"!\",\"?\",\"~\",\"ㅋ\",\"ㅠ\",\"ㅜ\",\"ㅎ\",\"^\"]\n",
        "        text, text2 = \"\",\"\"\n",
        "        t = None\n",
        "        sns_body = row[\"data\"][\"body\"][\"dialogue\"] if summary else row[\"data\"][\"body\"]\n",
        "        for body in sns_body:\n",
        "            if t and t != body[\"turnID\"]:\n",
        "                if text2 != \"\":\n",
        "                    temp[0].append(prompt + text2)\n",
        "                    temp[1].append(text[len(text2):])\n",
        "                t = body[\"turnID\"]\n",
        "                text2 = text\n",
        "                text += f'{body[\"utterance\"]}'\n",
        "            else:\n",
        "                t = body[\"turnID\"]\n",
        "                text += f'{body[\"utterance\"]}'\n",
        "                if body[\"utterance\"]!=\"\" and body[\"utterance\"][-1] not in end_word:\n",
        "                    text+=\".\"\n",
        "                text+=f\"{mask}\\n\"\n",
        "        temp[0].append(prompt + text2)\n",
        "        temp[1].append(text[len(text2):]+f\"{mask}\\n\")\n",
        "    return pd.DataFrame({\"Q\":temp[0],\"A\":temp[1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eS3CUfhk0o9c"
      },
      "outputs": [],
      "source": [
        "#@title 한국어 SNS + 한국어 대화 요약\n",
        "data = pd.DataFrame()\n",
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/한국어SNS/\"\n",
        "file_list = os.listdir(root_path)\n",
        "for i in [file for file in file_list if file.endswith('.json')]:\n",
        "    data = pd.concat([data, sns_chat(root_path+i)],\n",
        "                     ignore_index=True)\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/한국어 대화 요약/\"\n",
        "dir_list = os.listdir(root_path)\n",
        "for directory in dir_list:\n",
        "    file_list = os.listdir(root_path+directory)\n",
        "    for i in [file for file in file_list if file.endswith('.json')]:\n",
        "        data = pd.concat([data, sns_chat(root_path+directory+\"/\"+i, summary=True)],\n",
        "                         ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKtqa9OomzU"
      },
      "source": [
        "## TPU Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wiWasIyv73rj"
      },
      "outputs": [],
      "source": [
        "#@markdown writefile config.json\n",
        "%%writefile config.json\n",
        "{\n",
        "  \"activation_dropout\": 0.0,\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 2,\n",
        "  \"classifier_dropout\": 0.0,\n",
        "  \"d_model\": 768,\n",
        "  \"decoder_attention_heads\": 12,\n",
        "  \"decoder_ffn_dim\": 3072,\n",
        "  \"decoder_layerdrop\": 0.0,\n",
        "  \"decoder_layers\": 12,\n",
        "  \"decoder_start_token_id\": 2,\n",
        "  \"dropout\": 0.1,\n",
        "  \"encoder_attention_heads\": 12,\n",
        "  \"encoder_ffn_dim\": 3072,\n",
        "  \"encoder_layerdrop\": 0.0,\n",
        "  \"encoder_layers\": 2,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"forced_eos_token_id\": 2,\n",
        "  \"id2label\": {\n",
        "    \"0\": \"LABEL_0\",\n",
        "    \"1\": \"LABEL_1\",\n",
        "    \"2\": \"LABEL_2\"\n",
        "  },\n",
        "  \"init_std\": 0.02,\n",
        "  \"is_encoder_decoder\": True,\n",
        "  \"label2id\": {\n",
        "    \"LABEL_0\": 0,\n",
        "    \"LABEL_1\": 1,\n",
        "    \"LABEL_2\": 2\n",
        "  },\n",
        "  \"max_position_embeddings\": 1026,\n",
        "  \"model_type\": \"bart\",\n",
        "  \"pad_token_id\": 3,\n",
        "  \"num_hidden_layers\": 2,\n",
        "  \"scale_embedding\": False,\n",
        "  \"transformers_version\": \"4.14.0\",\n",
        "  \"use_cache\": True,\n",
        "  \"vocab_size\": 30000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5AyUCVYEPCxH"
      },
      "outputs": [],
      "source": [
        "#@title kobart_pet 파일 저장\n",
        "%%writefile kobart_pet.py\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (BartForConditionalGeneration,\n",
        "                          BartConfig,\n",
        "                          PreTrainedTokenizerFast)\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "# pl.seed_everything(72, workers=True)\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "\n",
        "parser = argparse.ArgumentParser(description='KoBART Chit-Chat')\n",
        "\n",
        "parser.add_argument('--checkpoint_path',\n",
        "                    type=str,\n",
        "                    help='checkpoint path')\n",
        "\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "class ArgsBase():\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--train_file',\n",
        "                            type=str,\n",
        "                            default='/dev_t.csv',\n",
        "                            help='train file')\n",
        "\n",
        "        parser.add_argument('--test_file',\n",
        "                            type=str,\n",
        "                            default='/dev_v.csv',\n",
        "                            help='test file')\n",
        "\n",
        "        parser.add_argument('--tokenizer_path',\n",
        "                            type=str,\n",
        "                            default='tokenizer',\n",
        "                            help='tokenizer')\n",
        "        parser.add_argument('--batch_size',\n",
        "                            type=int,\n",
        "                            default=4,\n",
        "                            help='')\n",
        "        parser.add_argument('--max_seq_len',\n",
        "                            type=int,\n",
        "                            default=1024,\n",
        "                            help='max seq len')\n",
        "        return parser\n",
        "\n",
        "\n",
        "class Pet_Dataset(Dataset):\n",
        "    # DROP_INDEX = [521220, 282913, 563216, 479493, 700506, 512223, 175561, 417687, 160844, 99439,\n",
        "    # 495977, 643219, 652972, 181412, 157691, 197617, 218616, 470251, 462476, 739754, 16696,\n",
        "    # 681468, 568982, 470818, 107852, 619731, 653948, 288011, 128605, 198422, 160296, 337961,\n",
        "    # 699346, 268925, 106534, 269500, 200660, 521189, 207688, 389179, 106470, 541426, 659934,\n",
        "    # 692149, 252129, 531928, 37374,  317932, 424110, 359891, 624185, 10418,\n",
        "    # 472406, 387344, 567790, 333374, 384333, 247310, 677924, 427995, 515477, 104626, 238678, 343527,]\n",
        "    def __init__(self, file_root_path=None, max_seq_len=512,train=False):\n",
        "        self.filepath = file_root_path\n",
        "        self.data = pd.read_csv(self.filepath)\n",
        "        # if train:\n",
        "        #     self.data = self.data.drop(index=self.DROP_INDEX,)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")\n",
        "        self.masking_start=self.tokenizer.encode(\"[]\")[0]\n",
        "        self.masking_end=self.tokenizer.encode(\"[]\")[-1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.__len__()\n",
        "\n",
        "    def _tokenize(self,text):\n",
        "        return self.tokenizer.convert_tokens_to_ids(\n",
        "            [self.tokenizer.bos_token]+self.tokenizer.tokenize(text)+[self.tokenizer.eos_token]\n",
        "            )\n",
        "\n",
        "    def _encode(self, text):\n",
        "        input_ids = self._tokenize(text)\n",
        "        return_len = len(input_ids)\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        if len(input_ids) < self.max_seq_len:\n",
        "            while len(input_ids)<self.max_seq_len:\n",
        "                input_ids+=[self.tokenizer.pad_token_id]\n",
        "                attention_mask+=[0]\n",
        "        else:\n",
        "            input_ids = input_ids[:self.max_seq_len-1]+[self.tokenizer.eos_token_id]\n",
        "            attention_mask = attention_mask[:self.max_seq_len]\n",
        "        return input_ids, attention_mask, return_len\n",
        "\n",
        "    def _labeling(self,label):\n",
        "        label_ids = self._tokenize(label)\n",
        "        if len(label_ids) < self.max_seq_len:\n",
        "            while len(label_ids)<self.max_seq_len:\n",
        "                label_ids+=[-100]\n",
        "        else:\n",
        "            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n",
        "        return label_ids\n",
        "\n",
        "    def _masking(self, tokens):\n",
        "        masked = tokens\n",
        "        while self.masking_start in masked and self.masking_end in masked:\n",
        "            masked[masked.index(self.masking_start):masked.index(self.masking_end)+1] = \\\n",
        "              [self.tokenizer.mask_token_id]*len(masked[masked.index(self.masking_start):masked.index(self.masking_end)+1])\n",
        "        return masked\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        record = self.data.iloc[index]\n",
        "        pattern, label = record[\"pattern\"], record[\"label\"]\n",
        "        label = pattern+label\n",
        "        encoder_input_ids, encoder_attention_mask, encoder_input_ids_len = self._encode(pattern)\n",
        "        decoder_input_ids, decoder_attention_mask, decoder_input_ids_len = self._encode(label)\n",
        "        encoder_input_ids = self._masking(encoder_input_ids)\n",
        "        encoder_input_ids[encoder_input_ids_len:] = decoder_input_ids[encoder_input_ids_len:]\n",
        "        encoder_attention_mask[encoder_input_ids_len:] = decoder_attention_mask[encoder_input_ids_len:]\n",
        "        label = self._labeling(label)\n",
        "\n",
        "        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n",
        "                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n",
        "                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n",
        "                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n",
        "                \"labels\":np.array(label,dtype=np.int_)}\n",
        "\n",
        "\n",
        "class ChatDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_file,\n",
        "                 test_file, tok_vocab,\n",
        "                 max_seq_len=1024,\n",
        "                 batch_size=4,\n",
        "                 num_workers=None):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.train_file_path = train_file\n",
        "        self.test_file_path = test_file\n",
        "        self.tok_vocab = tok_vocab\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = argparse.ArgumentParser(\n",
        "            parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--num_workers',\n",
        "                            type=int,\n",
        "                            default=2,\n",
        "                            help='num of worker for dataloader')\n",
        "        return parser\n",
        "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
        "    def setup(self, stage):\n",
        "        # split dataset\n",
        "        # self.train = Pet_Dataset(self.train_file_path,\n",
        "        #                          self.max_seq_len,\n",
        "        #                          True)\n",
        "\n",
        "        self.test = Pet_Dataset(self.test_file_path,\n",
        "                                self.max_seq_len)\n",
        "\n",
        "        self.multi_dataset, self.file_map = [],[]\n",
        "        file_list = os.listdir(self.train_file_path)\n",
        "        for i in [file for file in file_list if file.endswith('.csv')]:\n",
        "                self.multi_dataset.append(Pet_Dataset(self.train_file_path + i,\n",
        "                                        self.max_seq_len,\n",
        "                                        True))\n",
        "                self.file_map.append(i)\n",
        "\n",
        "    def __train_dataloader(self):\n",
        "        return DataLoader(self.train,\n",
        "                           batch_size=self.batch_size,\n",
        "                           num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.test,\n",
        "                         batch_size=self.batch_size,\n",
        "                         num_workers=self.num_workers, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers, shuffle=False)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return dict(zip(self.file_map,\n",
        "                        [DataLoader(data_set,\n",
        "                            batch_size=self.batch_size,\n",
        "                            num_workers=self.num_workers,\n",
        "                            shuffle=True) for data_set in self.multi_dataset]))\n",
        "\n",
        "\n",
        "class Base(pl.LightningModule):\n",
        "    def __init__(self, hparams, **kwargs) -> None:\n",
        "        super(Base, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        # add model specific args\n",
        "        parser = argparse.ArgumentParser(\n",
        "            parents=[parent_parser], add_help=False)\n",
        "\n",
        "        parser.add_argument('--batch-size',\n",
        "                            type=int,\n",
        "                            default=4,\n",
        "                            help='batch size for training (default: 4)')\n",
        "\n",
        "        parser.add_argument('--lr',\n",
        "                            type=float,\n",
        "                            default=5e-8,\n",
        "                            help='The initial learning rate')\n",
        "\n",
        "        parser.add_argument('--warmup_ratio',\n",
        "                            type=float,\n",
        "                            default=0.1,\n",
        "                            help='warmup ratio')\n",
        "\n",
        "        parser.add_argument('--model_path',\n",
        "                            type=str,\n",
        "                            default=None,\n",
        "                            help='kobart model path')\n",
        "        return parser\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.lr, correct_bias=False)\n",
        "        # warm up lr\n",
        "        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n",
        "        data_len = len(self.trainer._data_connector._train_dataloader_source.dataloader()['kor_nli.csv'].dataset)\n",
        "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
        "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
        "        logging.info(f'num_train_steps : {num_train_steps}')\n",
        "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
        "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "        lr_scheduler = {'scheduler': scheduler,\n",
        "                        'monitor': 'loss', 'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "class KoBARTConditionalGeneration(Base):\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super(KoBARTConditionalGeneration, self).__init__(hparams, **kwargs)\n",
        "        self.bos_token = \"<s>\"\n",
        "        self.eos_token = \"</s>\"\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.hparams.model_path)\n",
        "        self.model = BartForConditionalGeneration(BartConfig.from_json_file(\"config.json\"))\n",
        "        # self.model = BartForConditionalGeneration.from_pretrained(self.hparams.model_path)\n",
        "        self.model.share_memory().train()\n",
        "\n",
        "    def _resize_token_embeddings(self):\n",
        "        tokens = {\"additional_special_tokens\":[\"<P01>\",\"<P02>\",\"<P03>\",\"<P04>\",\"<P05>\",\"<P06>\",\"<P07>\",\"<P08>\",\"<P09>\"]}\n",
        "        self.tokenizer.add_special_tokens(tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def forward(self, inputs,):\n",
        "        return self.model(input_ids=inputs['input_ids'],\n",
        "                         attention_mask=inputs['attention_mask'],\n",
        "                         decoder_input_ids=inputs['decoder_input_ids'],\n",
        "                         decoder_attention_mask=inputs['decoder_attention_mask'],\n",
        "                         labels=inputs['labels'],\n",
        "                         return_dict=True)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # outs = self(batch,)\n",
        "        loss = torch.sum([self(batch[dataset]).loss for dataset in batch])\n",
        "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "        self.model.model.decoder.eval()\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # outs = self(batch)\n",
        "        loss = torch.sum([self(batch[dataset]).loss for dataset in batch])\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def chat(self, text):\n",
        "        input_ids =  self.tokenizer.encode(text) # [self.tokenizer.bos_token_id] + self.tokenizer.encode(text) + [self.tokenizer.eos_token_id]\n",
        "        res_ids = self.model.generate(torch.tensor([input_ids]),\n",
        "                                      max_length=self.hparams.max_seq_len,\n",
        "                                      num_beams=1,\n",
        "                                      # max_new_tokens=3,\n",
        "                                      # eos_token_id=self.tokenizer.eos_token_id,\n",
        "                                      bad_words_ids=[[self.tokenizer.unk_token_id,self.tokenizer.pad_token_id]],\n",
        "                                      top_p=0.80,\n",
        "                                      top_k=200,\n",
        "                                      temperature=0.88,\n",
        "                                      do_sample=True,\n",
        "                                      length_penalty = 1.0,\n",
        "                                      repetition_penalty=0.83,\n",
        "                                      no_repeat_ngram_size=1,)\n",
        "        a = self.tokenizer.batch_decode(res_ids.tolist())\n",
        "        for x in a:\n",
        "            print(\"생성 문장\",x)\n",
        "        return a[0].replace('<s>', '').replace('</s>', '').replace('<usr>','').replace('<sys>','')\n",
        "\n",
        "#main만 고쳐서 실행\n",
        "if __name__ == '__main__':\n",
        "    parser = Base.add_model_specific_args(parser)\n",
        "    parser = ArgsBase.add_model_specific_args(parser)\n",
        "    parser = ChatDataModule.add_model_specific_args(parser)\n",
        "    parser = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\",\n",
        "                                                 stopping_threshold=1e-4,\n",
        "                                                 min_delta=0.00, patience=3,\n",
        "                                                 divergence_threshold=9.0)],\n",
        "                                                 ).add_argparse_args(parser)\n",
        "    args = parser.parse_args()\n",
        "    logging.info(args)\n",
        "\n",
        "    model = KoBARTConditionalGeneration(args)\n",
        "\n",
        "    dm = ChatDataModule(args.train_file,\n",
        "                        args.test_file,\n",
        "                        args.model_path,\n",
        "                        max_seq_len=args.max_seq_len,\n",
        "                        num_workers=args.num_workers)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='loss',\n",
        "                                                       dirpath=args.default_root_dir,\n",
        "                                                       filename=f\"best-checkpoint\",\n",
        "                                                    #    verbose=True,\n",
        "                                                       save_last=True,\n",
        "                                                       mode='min',\n",
        "                                                       save_top_k=-1,)\n",
        "\n",
        "    tb_logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir, 'tb_logs'))\n",
        "    lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "    trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger,\n",
        "                                            callbacks=[checkpoint_callback, lr_logger],)\n",
        "    trainer.fit(model, dm,)# ckpt_path=\"/content/logs/kobart_chitchat-last.ckpt\",)\n",
        "    if args.chat:\n",
        "        model.model.eval()\n",
        "        # que=[]\n",
        "        while 1:\n",
        "            q = input('prompt > ').strip()\n",
        "            # if len(que) > 5:\n",
        "            #   que.pop(0)\n",
        "            # que.append(\"<usr>\"+q)\n",
        "            if q == 'quit':\n",
        "                break\n",
        "            elif q == 'save':\n",
        "                save_path = \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/EMO_Model/KoBART_Chat_Model_V1.0.pth\"\n",
        "                torch.save(model.model.state_dict(),save_path)\n",
        "                print(f'kobart-chat model.pth has saved at {save_path}')\n",
        "            # else:\n",
        "            #     text=\"\"\n",
        "            #     for x in que:\n",
        "            #         text+=x\n",
        "            # print(text)\n",
        "            result=model.chat(q)\n",
        "            # que.append(result)\n",
        "            print(\"Result > {}\".format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjC1OKhURE7_"
      },
      "outputs": [],
      "source": [
        "# TPU Training\n",
        "import os\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] =\"0\"\n",
        "os.environ[\"TRIM_GRAPH_SIZE\"] = \"1000000\"\n",
        "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '1000000000'\n",
        "os.environ[\"PR_SET_PDEATHSIG\"]=\"1\"\n",
        "os.environ[\"PL_RECONCILE_PROCESS\"]=\"1\"\n",
        "os.environ['XLA_USE_32BIT_LONG'] = \"0\"\n",
        "os.environ['XLA_USE_BF16'] = \"1\"\n",
        "os.environ['SLURM_JOB_NAME'] = 'bash'\n",
        "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "%%shell\n",
        "NCCL_DEBUG=WARN ;\\\n",
        " python \"/content/kobart_pet.py\"\\\n",
        "  --gradient_clip_val 1.0\\\n",
        "  --max_epochs 1\\\n",
        "  --lr 5e-8\\\n",
        "  --precision 16\\\n",
        "  --batch_size 512\\\n",
        "  --max_seq_len 512\\\n",
        "  --default_root_dir logs\\\n",
        "  --train_file \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/PET data/\"\\\n",
        "  --test_file \"/content/drive/MyDrive/Colab Notebooks/data/chatbot/PET data/pretrain/Test.csv\"\\\n",
        "  --model_path  \"gogamza/kobart-base-v1\"\\\n",
        "  --tokenizer_path \"Gunulhona/tubbarttokenizer\"\\\n",
        "  --progress_bar_refresh_rate 1\\\n",
        "  --num_workers 8\\\n",
        "  --tpu_cores 8\\\n",
        "  --num_sanity_val_steps=1\\\n",
        "  --accelerator \"tpu\"\\\n",
        "  --strategy \"ddp\"\\\n",
        "  --weights_save_path \"/content/weight.pth\"\\\n",
        "  --profiler \"pytorch\"\\\n",
        "  --sync_batchnorm True\\\n",
        "  --chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXwBT36tj7PJ"
      },
      "source": [
        "멈춘건 데이터 때문이긴 함 --> 아마도 걍 너무 많아서? 몰?루"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BBBKntPN9prB"
      },
      "outputs": [],
      "source": [
        "#@markdown DROP INDEX\n",
        "DROP_INDEX = [521220,    282913,    563216,    479493,    700506,    512223,    175561,    417687,    160844,    99439,\n",
        "    495977,    643219,    652972,    181412,    157691,    197617,    218616,    470251,    462476,    739754,    16696,\n",
        "    681468,    568982,    470818,    107852,    619731,    653948,    288011,    128605,    198422,    160296,    337961,\n",
        "    699346,    268925,    106534,    269500,    200660,    521189,    207688,    389179,    106470,    541426,    659934,\n",
        "    692149,    252129,    531928,    37374,     317932,    424110,    359891,    624185,    10418,\n",
        "    472406,    387344,    567790,    333374,    384333,    247310,    677924,    427995,    515477,    104626,    238678,    343527,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8LgeolU1fB_"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs --load_fast=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MNbEob60AQEO"
      },
      "outputs": [],
      "source": [
        "#@title Chatting\n",
        "!python \"/content/kobart_chit_chat.py\"\\\n",
        "  --gradient_clip_val 1.0\\\n",
        "  --max_epochs 1\\\n",
        "  --lr 1e-7\\\n",
        "  --batch-size 16\\\n",
        "  --default_root_dir logs \\\n",
        "  --train_file \"Chatbot_data/train.csv\"\\\n",
        "  --test_file \"Chatbot_data/test.csv\"\\\n",
        "  --model_path  \"gogamza/kobart-base-v1\"\\\n",
        "  --tokenizer_path \"Gunulhona/tubbarttokenizer\"\\\n",
        "  --tpu_cores 8\\\n",
        "  --accelerator \"ddp\"\\\n",
        "  --resume_from_checkpoint \"/content/logs/kobart_chitchat-last.ckpt\"\\\n",
        "  --chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY8Muli0mPjQ"
      },
      "outputs": [],
      "source": [
        "!rm -rdf logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq_UmUIOD3f-"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ0HEcXlEfaa"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"[]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnT7LqlXoltY"
      },
      "source": [
        "## few shot test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx-O_haTRGx0"
      },
      "outputs": [],
      "source": [
        "#@markdown few-shot generation code\n",
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3LP7ZrGiUwk"
      },
      "outputs": [],
      "source": [
        "from flair.models import TARSTagger\n",
        "from flair.data import Sentence\n",
        "# 1. 제로샷 NER tagger 로드\n",
        "tars = TARSTagger.load('tars-ner')\n",
        "# 2. 테스트 문장 준비\n",
        "sentences = [\n",
        "    Sentence(\"The Humboldt University of Berlin is situated near the Spree in Berlin, Germany\"),\n",
        "    Sentence(\"Bayern Munich played against Real Madrid\"),\n",
        "    Sentence(\"I flew with an Airbus A380 to Peru to pick up my Porsche Cayenne\"),\n",
        "    Sentence(\"Game of Thrones is my favorite series\"),\n",
        "]\n",
        "# 3. \"축구팀\", \"TV 프로그램\" 및 \"강\"과 같은 명명된 엔터티의 일부 클래스 정의\n",
        "labels = [\"Soccer Team\", \"University\", \"Vehicle\", \"River\", \"City\", \"Country\", \"Person\", \"Movie\", \"TV Show\"]\n",
        "tars.add_and_switch_to_new_task('task 1', labels, label_type='ner')\n",
        "# 4. 이 클래스에 대한 예측 및 결과 출력\n",
        "for sentence in sentences:\n",
        "    tars.predict(sentence)\n",
        "    print(sentence.to_tagged_string(\"ner\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aH_V8nrCjwsR"
      },
      "outputs": [],
      "source": [
        "#@title flair few shot\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Set, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from tqdm import tqdm\n",
        "\n",
        "import flair\n",
        "from flair.data import Dictionary, Sentence, Span, SpanLabel\n",
        "from flair.datasets import DataLoader, FlairDatapointDataset\n",
        "from flair.embeddings import (\n",
        "    TokenEmbeddings,\n",
        "    TransformerDocumentEmbeddings,\n",
        "    TransformerWordEmbeddings,\n",
        ")\n",
        "from flair.file_utils import cached_path\n",
        "from flair.models.sequence_tagger_model import SequenceTagger\n",
        "from flair.models.text_classification_model import TextClassifier\n",
        "from flair.training_utils import store_embeddings\n",
        "\n",
        "class FewshotClassifier(flair.nn.Classifier[Sentence]):\n",
        "    def __init__(self):\n",
        "        self._current_task = None\n",
        "        self._task_specific_attributes = {}\n",
        "        self.label_nearest_map = None\n",
        "        self.tars_model: flair.nn.Classifier[Sentence]\n",
        "\n",
        "        super(FewshotClassifier, self).__init__()\n",
        "\n",
        "    def forward_loss(\n",
        "        self, data_points: Union[List[Sentence], Sentence]\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, int]]:\n",
        "\n",
        "        if not isinstance(data_points, list):\n",
        "            data_points = [data_points]\n",
        "\n",
        "        # Transform input data into TARS format\n",
        "        sentences = self._get_tars_formatted_sentences(data_points)\n",
        "\n",
        "        loss = self.tars_model.forward_loss(sentences)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def tars_embeddings(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _get_tars_formatted_sentence(self, label, sentence):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _get_tars_formatted_sentences(self, sentences: List[Sentence]):\n",
        "        label_text_pairs = []\n",
        "        all_labels = [label.decode(\"utf-8\") for label in self.get_current_label_dictionary().idx2item]\n",
        "        for sentence in sentences:\n",
        "            label_text_pairs_for_sentence = []\n",
        "            if self.training and self.num_negative_labels_to_sample is not None:\n",
        "\n",
        "                positive_labels = list(\n",
        "                    OrderedDict.fromkeys([label.value for label in sentence.get_labels(self.label_type)])\n",
        "                )\n",
        "\n",
        "                sampled_negative_labels = self._get_nearest_labels_for(positive_labels)\n",
        "\n",
        "                for label in positive_labels:\n",
        "                    label_text_pairs_for_sentence.append(self._get_tars_formatted_sentence(label, sentence))\n",
        "                for label in sampled_negative_labels:\n",
        "                    label_text_pairs_for_sentence.append(self._get_tars_formatted_sentence(label, sentence))\n",
        "\n",
        "            else:\n",
        "                for label in all_labels:\n",
        "                    label_text_pairs_for_sentence.append(self._get_tars_formatted_sentence(label, sentence))\n",
        "            label_text_pairs.extend(label_text_pairs_for_sentence)\n",
        "\n",
        "        return label_text_pairs\n",
        "\n",
        "    def _get_nearest_labels_for(self, labels):\n",
        "\n",
        "        # if there are no labels, return a random sample as negatives\n",
        "        if len(labels) == 0:\n",
        "            tags = self.get_current_label_dictionary().get_items()\n",
        "            import random\n",
        "\n",
        "            sample = random.sample(tags, k=self.num_negative_labels_to_sample)\n",
        "            return sample\n",
        "\n",
        "        already_sampled_negative_labels = set()\n",
        "\n",
        "        # otherwise, go through all labels\n",
        "        for label in labels:\n",
        "\n",
        "            plausible_labels = []\n",
        "            plausible_label_probabilities = []\n",
        "            for plausible_label in self.label_nearest_map[label]:\n",
        "                if plausible_label in already_sampled_negative_labels or plausible_label in labels:\n",
        "                    continue\n",
        "                else:\n",
        "                    plausible_labels.append(plausible_label)\n",
        "                    plausible_label_probabilities.append(self.label_nearest_map[label][plausible_label])\n",
        "\n",
        "            # make sure the probabilities always sum up to 1\n",
        "            plausible_label_probabilities = np.array(plausible_label_probabilities, dtype=\"float64\")\n",
        "            plausible_label_probabilities += 1e-08\n",
        "            plausible_label_probabilities /= np.sum(plausible_label_probabilities)\n",
        "\n",
        "            if len(plausible_labels) > 0:\n",
        "                num_samples = min(self.num_negative_labels_to_sample, len(plausible_labels))\n",
        "                sampled_negative_labels = np.random.choice(\n",
        "                    plausible_labels,\n",
        "                    num_samples,\n",
        "                    replace=False,\n",
        "                    p=plausible_label_probabilities,\n",
        "                )\n",
        "                already_sampled_negative_labels.update(sampled_negative_labels)\n",
        "\n",
        "        return already_sampled_negative_labels\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"Populate label similarity map based on cosine similarity before running epoch\n",
        "        If the `num_negative_labels_to_sample` is set to an integer value then before starting\n",
        "        each epoch the model would create a similarity measure between the label names based\n",
        "        on cosine distances between their BERT encoded embeddings.\n",
        "        \"\"\"\n",
        "        if mode and self.num_negative_labels_to_sample is not None:\n",
        "            self._compute_label_similarity_for_current_epoch()\n",
        "            super().train(mode)\n",
        "\n",
        "        super().train(mode)\n",
        "\n",
        "    def _compute_label_similarity_for_current_epoch(self):\n",
        "        \"\"\"\n",
        "        Compute the similarity between all labels for better sampling of negatives\n",
        "        \"\"\"\n",
        "\n",
        "        # get and embed all labels by making a Sentence object that contains only the label text\n",
        "        all_labels = [label.decode(\"utf-8\") for label in self.get_current_label_dictionary().idx2item]\n",
        "        label_sentences = [Sentence(label) for label in all_labels]\n",
        "\n",
        "        self.tars_embeddings.eval()  # TODO: check if this is necessary\n",
        "        self.tars_embeddings.embed(label_sentences)\n",
        "        self.tars_embeddings.train()\n",
        "\n",
        "        # get each label embedding and scale between 0 and 1\n",
        "        if isinstance(self.tars_embeddings, TokenEmbeddings):\n",
        "            encodings_np = [sentence[0].get_embedding().cpu().detach().numpy() for sentence in label_sentences]\n",
        "        else:\n",
        "            encodings_np = [sentence.get_embedding().cpu().detach().numpy() for sentence in label_sentences]\n",
        "\n",
        "        normalized_encoding = minmax_scale(encodings_np)\n",
        "\n",
        "        # compute similarity matrix\n",
        "        similarity_matrix = cosine_similarity(normalized_encoding)\n",
        "\n",
        "        # the higher the similarity, the greater the chance that a label is\n",
        "        # sampled as negative example\n",
        "        negative_label_probabilities = {}\n",
        "        for row_index, label in enumerate(all_labels):\n",
        "            negative_label_probabilities[label] = {}\n",
        "            for column_index, other_label in enumerate(all_labels):\n",
        "                if label != other_label:\n",
        "                    negative_label_probabilities[label][other_label] = similarity_matrix[row_index][column_index]\n",
        "        self.label_nearest_map = negative_label_probabilities\n",
        "\n",
        "    def get_current_label_dictionary(self):\n",
        "        label_dictionary = self._task_specific_attributes[self._current_task][\"label_dictionary\"]\n",
        "        return label_dictionary\n",
        "\n",
        "    def get_current_label_type(self):\n",
        "        return self._task_specific_attributes[self._current_task][\"label_type\"]\n",
        "\n",
        "    def is_current_task_multi_label(self):\n",
        "        return self._task_specific_attributes[self._current_task][\"multi_label\"]\n",
        "\n",
        "    def add_and_switch_to_new_task(\n",
        "        self,\n",
        "        task_name,\n",
        "        label_dictionary: Union[List, Set, Dictionary, str],\n",
        "        label_type: str,\n",
        "        multi_label: bool = True,\n",
        "        force_switch: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Adds a new task to an existing TARS model. Sets necessary attributes and finally 'switches'\n",
        "        to the new task. Parameters are similar to the constructor except for model choice, batch\n",
        "        size and negative sampling. This method does not store the resultant model onto disk.\n",
        "        :param task_name: a string depicting the name of the task\n",
        "        :param label_dictionary: dictionary of the labels you want to predict\n",
        "        :param label_type: string to identify the label type ('ner', 'sentiment', etc.)\n",
        "        :param multi_label: whether this task is a multi-label prediction problem\n",
        "        :param force_switch: if True, will overwrite existing task with same name\n",
        "        \"\"\"\n",
        "        if task_name in self._task_specific_attributes and not force_switch:\n",
        "            log.warning(\"Task `%s` already exists in TARS model. Switching to it.\", task_name)\n",
        "        else:\n",
        "            # make label dictionary if no Dictionary object is passed\n",
        "            if isinstance(label_dictionary, Dictionary):\n",
        "                label_dictionary = label_dictionary.get_items()\n",
        "            if type(label_dictionary) == str:\n",
        "                label_dictionary = [label_dictionary]\n",
        "\n",
        "            # prepare dictionary of tags (without B- I- prefixes and without UNK)\n",
        "            tag_dictionary = Dictionary(add_unk=False)\n",
        "            for tag in label_dictionary:\n",
        "                if tag == \"<unk>\" or tag == \"O\":\n",
        "                    continue\n",
        "                if tag[1] == \"-\":\n",
        "                    tag = tag[2:]\n",
        "                    tag_dictionary.add_item(tag)\n",
        "                else:\n",
        "                    tag_dictionary.add_item(tag)\n",
        "\n",
        "            self._task_specific_attributes[task_name] = {\n",
        "                \"label_dictionary\": tag_dictionary,\n",
        "                \"label_type\": label_type,\n",
        "                \"multi_label\": multi_label,\n",
        "            }\n",
        "\n",
        "        self.switch_to_task(task_name)\n",
        "\n",
        "    def list_existing_tasks(self) -> Set[str]:\n",
        "        \"\"\"\n",
        "        Lists existing tasks in the loaded TARS model on the console.\n",
        "        \"\"\"\n",
        "        return set(self._task_specific_attributes.keys())\n",
        "\n",
        "    def switch_to_task(self, task_name):\n",
        "        \"\"\"\n",
        "        Switches to a task which was previously added.\n",
        "        \"\"\"\n",
        "        if task_name not in self._task_specific_attributes:\n",
        "            log.error(\n",
        "                \"Provided `%s` does not exist in the model. Consider calling \" \"`add_and_switch_to_new_task` first.\",\n",
        "                task_name,\n",
        "            )\n",
        "        else:\n",
        "            self._current_task = task_name\n",
        "\n",
        "    def _drop_task(self, task_name):\n",
        "        if task_name in self._task_specific_attributes:\n",
        "            if self._current_task == task_name:\n",
        "                log.error(\n",
        "                    \"`%s` is the current task.\" \" Switch to some other task before dropping this.\",\n",
        "                    task_name,\n",
        "                )\n",
        "            else:\n",
        "                self._task_specific_attributes.pop(task_name)\n",
        "        else:\n",
        "            log.warning(\"No task exists with the name `%s`.\", task_name)\n",
        "\n",
        "    @staticmethod\n",
        "    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n",
        "        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n",
        "        if len(sentences) != len(filtered_sentences):\n",
        "            log.warning(f\"Ignore {len(sentences) - len(filtered_sentences)} sentence(s) with no tokens.\")\n",
        "        return filtered_sentences\n",
        "\n",
        "    @property\n",
        "    def label_type(self):\n",
        "        return self.get_current_label_type()\n",
        "\n",
        "    def predict_zero_shot(\n",
        "        self,\n",
        "        sentences: Union[List[Sentence], Sentence],\n",
        "        candidate_label_set: Union[List[str], Set[str], str],\n",
        "        multi_label: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Method to make zero shot predictions from the TARS model\n",
        "        :param sentences: input sentence objects to classify\n",
        "        :param candidate_label_set: set of candidate labels\n",
        "        :param multi_label: indicates whether multi-label or single class prediction. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        # check if candidate_label_set is empty\n",
        "        if candidate_label_set is None or len(candidate_label_set) == 0:\n",
        "            log.warning(\"Provided candidate_label_set is empty\")\n",
        "            return\n",
        "\n",
        "        # make list if only one candidate label is passed\n",
        "        if isinstance(candidate_label_set, str):\n",
        "            candidate_label_set = {candidate_label_set}\n",
        "\n",
        "        # create label dictionary\n",
        "        label_dictionary = Dictionary(add_unk=False)\n",
        "        for label in candidate_label_set:\n",
        "            label_dictionary.add_item(label)\n",
        "\n",
        "        # note current task\n",
        "        existing_current_task = self._current_task\n",
        "\n",
        "        # create a temporary task\n",
        "        self.add_and_switch_to_new_task(\n",
        "            task_name=\"ZeroShot\",\n",
        "            label_dictionary=label_dictionary,\n",
        "            label_type=\"-\".join(label_dictionary.get_items()),\n",
        "            multi_label=multi_label,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # make zero shot predictions\n",
        "            self.predict(sentences)\n",
        "        finally:\n",
        "            # switch to the pre-existing task\n",
        "            self.switch_to_task(existing_current_task)\n",
        "            self._drop_task(\"ZeroShot\")\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4daVLUjkRFm"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWuWWY3WmYcc"
      },
      "source": [
        "pipeline trasformers 지원\n",
        "```\n",
        "['audio-classification',\n",
        "'automatic-speech-recognition',\n",
        "'conversational',\n",
        "'feature-extraction',\n",
        "'fill-mask',\n",
        "'image-classification',\n",
        "'image-segmentation',\n",
        "'ner',\n",
        "'object-detection',\n",
        "'question-answering',\n",
        "'sentiment-analysis',\n",
        "'summarization',\n",
        "'table-question-answering',\n",
        "'text-classification',\n",
        "'text-generation',\n",
        "'text2text-generation',\n",
        "'token-classification',\n",
        "'translation',\n",
        "'zero-shot-classification',\n",
        "'translation_XX_to_YY']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pHqJHYH5DJv2"
      },
      "outputs": [],
      "source": [
        "#@title huggingface pipelines zeroshot classifier\n",
        "from typing import List, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers.file_utils import add_end_docstrings\n",
        "from transformers.tokenization_utils import TruncationStrategy\n",
        "from transformers.utils import logging\n",
        "from transformers.pipelines.base import PIPELINE_INIT_ARGS, ArgumentHandler, ChunkPipeline\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class ZeroShotClassificationArgumentHandler(ArgumentHandler):\n",
        "    \"\"\"\n",
        "    Handles arguments for zero-shot for text classification by turning each possible label into an NLI\n",
        "    premise/hypothesis pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def _parse_labels(self, labels):\n",
        "        if isinstance(labels, str):\n",
        "            labels = [label.strip() for label in labels.split(\",\") if label.strip()]\n",
        "        return labels\n",
        "\n",
        "    def __call__(self, sequences, labels, hypothesis_template):\n",
        "        if len(labels) == 0 or len(sequences) == 0:\n",
        "            raise ValueError(\"You must include at least one label and at least one sequence.\")\n",
        "        if hypothesis_template.format(labels[0]) == hypothesis_template:\n",
        "            raise ValueError(\n",
        "                (\n",
        "                    'The provided hypothesis_template \"{}\" was not able to be formatted with the target labels. '\n",
        "                    \"Make sure the passed template includes formatting syntax such as {{}} where the label should go.\"\n",
        "                ).format(hypothesis_template)\n",
        "            )\n",
        "\n",
        "        if isinstance(sequences, str):\n",
        "            sequences = [sequences]\n",
        "\n",
        "        sequence_pairs = []\n",
        "        for sequence in sequences:\n",
        "            sequence_pairs.extend([[sequence, hypothesis_template.format(label)] for label in labels])\n",
        "\n",
        "        return sequence_pairs, sequences\n",
        "\n",
        "\n",
        "@add_end_docstrings(PIPELINE_INIT_ARGS)\n",
        "class ZeroShotClassificationPipeline(ChunkPipeline):\n",
        "    \"\"\"\n",
        "    NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification` trained on NLI (natural\n",
        "    language inference) tasks.\n",
        "\n",
        "    Any combination of sequences and labels can be passed and each combination will be posed as a premise/hypothesis\n",
        "    pair and passed to the pretrained model. Then, the logit for *entailment* is taken as the logit for the candidate\n",
        "    label being valid. Any NLI model can be used, but the id of the *entailment* label must be included in the model\n",
        "    config's :attr:*~transformers.PretrainedConfig.label2id*.\n",
        "\n",
        "    This NLI pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
        "    `\"zero-shot-classification\"`.\n",
        "\n",
        "    The models that this pipeline can use are models that have been fine-tuned on an NLI task. See the up-to-date list\n",
        "    of available models on [huggingface.co/models](https://huggingface.co/models?search=nli).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args_parser=ZeroShotClassificationArgumentHandler(), *args, **kwargs):\n",
        "        self._args_parser = args_parser\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if self.entailment_id == -1:\n",
        "            logger.warning(\n",
        "                \"Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to \"\n",
        "                \"-1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\"\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def entailment_id(self):\n",
        "        for label, ind in self.model.config.label2id.items():\n",
        "            if label.lower().startswith(\"entail\"):\n",
        "                return ind\n",
        "        return -1\n",
        "\n",
        "    def _parse_and_tokenize(\n",
        "        self, sequence_pairs, padding=True, add_special_tokens=True, truncation=TruncationStrategy.ONLY_FIRST, **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parse arguments and tokenize only_first so that hypothesis (label) is not truncated\n",
        "        \"\"\"\n",
        "        return_tensors = self.framework\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            # Override for tokenizers not supporting padding\n",
        "            logger.error(\n",
        "                \"Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\"\n",
        "            )\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                sequence_pairs,\n",
        "                add_special_tokens=add_special_tokens,\n",
        "                return_tensors=return_tensors,\n",
        "                return_token_type_ids =False,\n",
        "                padding=padding,\n",
        "                truncation=truncation,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            if \"too short\" in str(e):\n",
        "                # tokenizers might yell that we want to truncate\n",
        "                # to a value that is not even reached by the input.\n",
        "                # In that case we don't want to truncate.\n",
        "                # It seems there's not a really better way to catch that\n",
        "                # exception.\n",
        "\n",
        "                inputs = self.tokenizer(\n",
        "                    sequence_pairs,\n",
        "                    add_special_tokens=add_special_tokens,\n",
        "                    return_tensors=return_tensors,\n",
        "                    padding=padding,\n",
        "                    truncation=TruncationStrategy.DO_NOT_TRUNCATE,\n",
        "                )\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "        if kwargs.get(\"multi_class\", None) is not None:\n",
        "            kwargs[\"multi_label\"] = kwargs[\"multi_class\"]\n",
        "            logger.warning(\n",
        "                \"The `multi_class` argument has been deprecated and renamed to `multi_label`. \"\n",
        "                \"`multi_class` will be removed in a future version of Transformers.\"\n",
        "            )\n",
        "        preprocess_params = {}\n",
        "        if \"candidate_labels\" in kwargs:\n",
        "            preprocess_params[\"candidate_labels\"] = self._args_parser._parse_labels(kwargs[\"candidate_labels\"])\n",
        "        if \"hypothesis_template\" in kwargs:\n",
        "            preprocess_params[\"hypothesis_template\"] = kwargs[\"hypothesis_template\"]\n",
        "\n",
        "        postprocess_params = {}\n",
        "        if \"multi_label\" in kwargs:\n",
        "            postprocess_params[\"multi_label\"] = kwargs[\"multi_label\"]\n",
        "        return preprocess_params, {}, postprocess_params\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        sequences: Union[str, List[str]],\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Classify the sequence(s) given as inputs. See the [`ZeroShotClassificationPipeline`] documentation for more\n",
        "        information.\n",
        "\n",
        "        Args:\n",
        "            sequences (`str` or `List[str]`):\n",
        "                The sequence(s) to classify, will be truncated if the model input is too large.\n",
        "            candidate_labels (`str` or `List[str]`):\n",
        "                The set of possible class labels to classify each sequence into. Can be a single label, a string of\n",
        "                comma-separated labels, or a list of labels.\n",
        "            hypothesis_template (`str`, *optional*, defaults to `\"This example is {}.\"`):\n",
        "                The template used to turn each label into an NLI-style hypothesis. This template must include a {} or\n",
        "                similar syntax for the candidate label to be inserted into the template. For example, the default\n",
        "                template is `\"This example is {}.\"` With the candidate label `\"sports\"`, this would be fed into the\n",
        "                model like `\"<cls> sequence to classify <sep> This example is sports . <sep>\"`. The default template\n",
        "                works well in many cases, but it may be worthwhile to experiment with different templates depending on\n",
        "                the task setting.\n",
        "            multi_label (`bool`, *optional*, defaults to `False`):\n",
        "                Whether or not multiple candidate labels can be true. If `False`, the scores are normalized such that\n",
        "                the sum of the label likelihoods for each sequence is 1. If `True`, the labels are considered\n",
        "                independent and probabilities are normalized for each candidate by doing a softmax of the entailment\n",
        "                score vs. the contradiction score.\n",
        "\n",
        "        Return:\n",
        "            A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
        "\n",
        "            - **sequence** (`str`) -- The sequence for which this is the output.\n",
        "            - **labels** (`List[str]`) -- The labels sorted by order of likelihood.\n",
        "            - **scores** (`List[float]`) -- The probabilities for each of the labels.\n",
        "        \"\"\"\n",
        "        if len(args) == 0:\n",
        "            pass\n",
        "        elif len(args) == 1 and \"candidate_labels\" not in kwargs:\n",
        "            kwargs[\"candidate_labels\"] = args[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unable to understand extra arguments {args}\")\n",
        "\n",
        "        return super().__call__(sequences, **kwargs)\n",
        "\n",
        "    def preprocess(self, inputs, candidate_labels=None, hypothesis_template=\"This example is {}.\"):\n",
        "        sequence_pairs, sequences = self._args_parser(inputs, candidate_labels, hypothesis_template)\n",
        "\n",
        "        for i, (candidate_label, sequence_pair) in enumerate(zip(candidate_labels, sequence_pairs)):\n",
        "            model_input = self._parse_and_tokenize([sequence_pair])\n",
        "\n",
        "            yield {\n",
        "                \"candidate_label\": candidate_label,\n",
        "                \"sequence\": sequences[0],\n",
        "                \"is_last\": i == len(candidate_labels) - 1,\n",
        "                **model_input,\n",
        "            }\n",
        "\n",
        "    def _forward(self, inputs):\n",
        "        candidate_label = inputs[\"candidate_label\"]\n",
        "        sequence = inputs[\"sequence\"]\n",
        "        model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}\n",
        "        outputs = self.model(**model_inputs)\n",
        "\n",
        "        model_outputs = {\n",
        "            \"candidate_label\": candidate_label,\n",
        "            \"sequence\": sequence,\n",
        "            \"is_last\": inputs[\"is_last\"],\n",
        "            **outputs,\n",
        "        }\n",
        "        return model_outputs\n",
        "\n",
        "    def postprocess(self, model_outputs, multi_label=False):\n",
        "        candidate_labels = [outputs[\"candidate_label\"] for outputs in model_outputs]\n",
        "        sequences = [outputs[\"sequence\"] for outputs in model_outputs]\n",
        "        logits = np.concatenate([output[\"logits\"].numpy() for output in model_outputs])\n",
        "        N = logits.shape[0]\n",
        "        n = len(candidate_labels)\n",
        "        num_sequences = N // n\n",
        "        reshaped_outputs = logits.reshape((num_sequences, n, -1))\n",
        "\n",
        "        if multi_label or len(candidate_labels) == 1:\n",
        "            # softmax over the entailment vs. contradiction dim for each label independently\n",
        "            entailment_id = self.entailment_id\n",
        "            contradiction_id = -1 if entailment_id == 0 else 0\n",
        "            entail_contr_logits = reshaped_outputs[..., [contradiction_id, entailment_id]]\n",
        "            scores = np.exp(entail_contr_logits) / np.exp(entail_contr_logits).sum(-1, keepdims=True)\n",
        "            scores = scores[..., 1]\n",
        "        else:\n",
        "            # softmax the \"entailment\" logits over all candidate labels\n",
        "            entail_logits = reshaped_outputs[..., self.entailment_id]\n",
        "            scores = np.exp(entail_logits) / np.exp(entail_logits).sum(-1, keepdims=True)\n",
        "\n",
        "        top_inds = list(reversed(scores[0].argsort()))\n",
        "        return {\n",
        "            \"sequence\": sequences[0],\n",
        "            \"labels\": [candidate_labels[i] for i in top_inds],\n",
        "            \"scores\": scores[0, top_inds].tolist(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii783v0bF-8V"
      },
      "outputs": [],
      "source": [
        "import transformers.pipelines  as pipe #zero_shot_classificaiton #import ZeroShotClassificationPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoIilI5dLTwy"
      },
      "outputs": [],
      "source": [
        "pipe.zero_shot_classification.ZeroShotClassificationPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9kO5MsYlwp3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classification = pipeline(\"zero-shot-classification\",model=\"Gunulhona/tbbcmodel\",\n",
        "                          PretrainedConfig=\"Gunulhona/tbbcmodel\",\n",
        "                          PretrainedTokenizer=\"Gunulhona/tbbarttokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYuulOW7o4KU"
      },
      "outputs": [],
      "source": [
        "from transformers.pipeline.zero_shot_classification import ZeroShotClassificationArgumentHandler, ZeroShotClassificationPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qeicVskqcC6"
      },
      "outputs": [],
      "source": [
        "#@title 연결끊김방지\n",
        "#@markdown ```javascript\n",
        "#@markdown function ClickConnect(){\n",
        "#@markdown     console.log(\"Working\");\n",
        "#@markdown     document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "#@markdown }setInterval(ClickConnect,30000)\n",
        "#@markdown ```\n",
        "\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,30000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fz40d4qpYOi"
      },
      "source": [
        "ZeroShotClassificationPipeline에서 수정해야됨\n",
        "```python\n",
        "[line:94] try:\n",
        "            inputs = self.tokenizer(\n",
        "                sequence_pairs,\n",
        "                add_special_tokens=add_special_tokens,\n",
        "                return_tensors=return_tensors,\n",
        "                return_token_type_ids =False,\n",
        "                padding=padding,\n",
        "                truncation=truncation,\n",
        "            )\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFTx9Q47srJX"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j-DFvnVstgh"
      },
      "outputs": [],
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "alt.Chart(cars).mark_bar().encode(\n",
        "  x=alt.X('Miles_per_Gallon', bin=True),\n",
        "  y='count()',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfaaTJE3srJY"
      },
      "outputs": [],
      "source": [
        "# Fetch a single <1MB file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAcp7a1jl1LX"
      },
      "outputs": [],
      "source": [
        "classification(\"나는 슬프다\",[\"슬픔\",\"눈물\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQoS46k60gmC"
      },
      "outputs": [],
      "source": [
        " os.environ['COLAB_TPU_ADDR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r2bI-SppBsy0"
      },
      "outputs": [],
      "source": [
        "#@title Bart Custom Config\n",
        "import json\n",
        "encoder_layer_num=2#@param{type:\"integer\"}\n",
        "decoder_layer_num=16#@param{type:\"integer\"}\n",
        "\n",
        "BartCustomConfig={\n",
        "  \"activation_dropout\": 0.0,\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 2,\n",
        "  \"classifier_dropout\": 0.0,\n",
        "  \"d_model\": 768,\n",
        "  \"decoder_attention_heads\": decoder_layer_num,\n",
        "  \"decoder_ffn_dim\": 3072,\n",
        "  \"decoder_layerdrop\": 0.0,\n",
        "  \"decoder_layers\": decoder_layer_num,\n",
        "  \"decoder_start_token_id\": 2,\n",
        "  \"dropout\": 0.1,\n",
        "  \"encoder_attention_heads\": decoder_layer_num,\n",
        "  \"encoder_ffn_dim\": 3072,\n",
        "  \"encoder_layerdrop\": 0.0,\n",
        "  \"encoder_layers\": encoder_layer_num,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"forced_eos_token_id\": 2,\n",
        "  \"id2label\": {\n",
        "    \"0\": \"LABEL_0\",\n",
        "    \"1\": \"LABEL_1\",\n",
        "    \"2\": \"LABEL_2\"\n",
        "  },\n",
        "  \"init_std\": 0.02,\n",
        "  \"is_encoder_decoder\": True,\n",
        "  \"label2id\": {\n",
        "    \"LABEL_0\": 0,\n",
        "    \"LABEL_1\": 1,\n",
        "    \"LABEL_2\": 2\n",
        "  },\n",
        "  \"max_position_embeddings\": 1026,\n",
        "  \"model_type\": \"bart\",\n",
        "  \"pad_token_id\": 3,\n",
        "  \"num_hidden_layers\": encoder_layer_num,\n",
        "  \"scale_embedding\": False,\n",
        "  \"transformers_version\": \"4.14.0\",\n",
        "  \"use_cache\": True,\n",
        "  \"vocab_size\": 30000\n",
        "}\n",
        "config = $BartCustomConfig #@param {type:\"string\"}\n",
        "with open(\"config.json\",'w') as f:\n",
        "    json.dump(BartCustomConfig,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WHDRuuwY4Jle"
      },
      "outputs": [],
      "source": [
        "#@title Custom Bart 파일 저장\n",
        "%%writefile bart_custom.py\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_end_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    MaskedLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqQuestionAnsweringModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput,\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import logging\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"facebook/bart-large\"\n",
        "_CONFIG_FOR_DOC = \"BartConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n",
        "\n",
        "\n",
        "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/bart-large\",\n",
        "    # See all BART models at https://huggingface.co/models?filter=bart\n",
        "]\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    if pad_token_id is None:\n",
        "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), float(\"-inf\"))\n",
        "    mask_cond = torch.arange(mask.size(-1))\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)\n",
        "\n",
        "\n",
        "class BartLearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
        "        # and adjust num_embeddings appropriately. Other models don't have this hack\n",
        "        self.offset = 2\n",
        "        super().__init__(num_embeddings + self.offset, embedding_dim)\n",
        "\n",
        "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids_shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
        "        )\n",
        "        return super().forward(positions + self.offset)\n",
        "\n",
        "\n",
        "class BartAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit awkward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to be reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
        "        # partitioned aross GPUs when using tensor-parallelism.\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value\n",
        "\n",
        "# GPT Neo attention\n",
        "class GPTNeoSelfAttention(nn.Module):\n",
        "    def __init__(self, config, attention_type):\n",
        "        super().__init__()\n",
        "\n",
        "        max_positions = config.max_position_embeddings\n",
        "        bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n",
        "            1, 1, max_positions, max_positions\n",
        "        )\n",
        "\n",
        "        # local causal self attention is a sliding window where each token can only attend to the previous\n",
        "        # window_size tokens. This is implemented by updating the causal mask such that for each token\n",
        "        # all other tokens are masked except the previous window_size tokens.\n",
        "        if attention_type == \"local\":\n",
        "            bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n",
        "\n",
        "        self.register_buffer(\"bias\", bias)\n",
        "        self.register_buffer(\"masked_bias\", torch.tensor(-1e9))\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        if self.head_dim * self.num_heads != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads}).\"\n",
        "            )\n",
        "\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
        "        \"\"\"\n",
        "        Splits hidden_size dim into attn_head_size and num_heads\n",
        "        \"\"\"\n",
        "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
        "        tensor = tensor.view(new_shape)\n",
        "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
        "\n",
        "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
        "        \"\"\"\n",
        "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
        "        \"\"\"\n",
        "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
        "        return tensor.view(new_shape)\n",
        "\n",
        "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
        "        # Keep the attention weights computation in fp32 to avoid overflow issues\n",
        "        query = query.to(torch.float32)\n",
        "        key = key.to(torch.float32)\n",
        "\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "        query_length, key_length = query.size(-2), key.size(-2)\n",
        "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()\n",
        "        attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = attn_weights.to(value.dtype)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attn_weights = attn_weights * head_mask\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        layer_past=None,\n",
        "        head_mask=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "\n",
        "        query = self.q_proj(hidden_states)\n",
        "        key = self.k_proj(hidden_states)\n",
        "        value = self.v_proj(hidden_states)\n",
        "\n",
        "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
        "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
        "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
        "\n",
        "        if layer_past is not None:\n",
        "            past_key = layer_past[0]\n",
        "            past_value = layer_past[1]\n",
        "            key = torch.cat((past_key, key), dim=-2)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "\n",
        "        if use_cache is True:\n",
        "            present = (key, value)\n",
        "        else:\n",
        "            present = None\n",
        "\n",
        "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
        "\n",
        "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        attn_output = self.resid_dropout(attn_output)\n",
        "\n",
        "        outputs = (attn_output, present)\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs  # a, present, (attentions)\n",
        "\n",
        "\n",
        "class GPTNeoAttention(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_layers = config.attention_layers\n",
        "        self.attention_type = self.attention_layers[layer_id]\n",
        "\n",
        "        if self.attention_type in [\"global\", \"local\"]:\n",
        "            self.attention = GPTNeoSelfAttention(config, self.attention_type)\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Only attn layer types 'global' and 'local' exist, but got `config.attention_layers`: \"\n",
        "                f\"{config.attention_layers}. Select attn layer types from ['global', 'local'] only.\"\n",
        "            )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        layer_past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        return self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_past=layer_past,\n",
        "            head_mask=head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class BartEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = BartAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        layer_head_mask: torch.Tensor,\n",
        "        output_attentions: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                *(encoder_attention_heads,)*.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if hidden_states.dtype == torch.float16 and (\n",
        "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
        "        ):\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BartDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = BartAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = BartAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.lmhead = BartClassificationHead(self.embed_dim,config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape *(batch, seq_len, embed_dim)*\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (`torch.FloatTensor`):\n",
        "                cross attention input to the layer of shape *(batch, seq_len, embed_dim)*\n",
        "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                *(encoder_attention_heads,)*.\n",
        "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
        "                size *(decoder_attention_heads,)*.\n",
        "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=cross_attn_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.lmhead(self.fc1(hidden_states))#self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BartClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        inner_dim: int,\n",
        "        num_classes: int,\n",
        "        pooler_dropout: float,\n",
        "        config\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.act = ACT2FN[config.activaiton_function]\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor):\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.act(hidden_states) # torch.tanh(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BartPretrainedModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"encoder\\.version\", r\"decoder\\.version\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, (BartDecoder, BartEncoder)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "\n",
        "class PretrainedBartModel(BartPretrainedModel):\n",
        "    def __init_subclass__(self):\n",
        "        warnings.warn(\n",
        "            \"The class `PretrainedBartModel` has been depreciated, please use `BartPretrainedModel` instead.\",\n",
        "            FutureWarning,\n",
        "        )\n",
        "\n",
        "\n",
        "BART_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "    Parameters:\n",
        "        config ([`BartConfig`]):\n",
        "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "            load the weights associated with the model, only the configuration. Check out the\n",
        "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "BART_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Summarization example:\n",
        "    ```python\n",
        "    >>> from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
        "    >>> # Generate Summary\n",
        "    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5)\n",
        "    >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
        "    ```\n",
        "    Mask filling example:\n",
        "    ```python\n",
        "    >>> from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "    >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
        "    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    >>> logits = model(input_ids).logits\n",
        "    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "    >>> probs = logits[0, masked_index].softmax(dim=0)\n",
        "    >>> values, predictions = probs.topk(5)\n",
        "    >>> tokenizer.decode(predictions).split()\n",
        "    ```\n",
        "\"\"\"\n",
        "\n",
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Indices of decoder input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
        "            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
        "            For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
        "            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
        "            for denoising pre-training following the paper.\n",
        "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "            be used by default.\n",
        "            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_inputs`] and\n",
        "            modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more information\n",
        "            on the default strategy.\n",
        "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
        "            1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            ``decoder_input_ids``` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n",
        "            shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids`\n",
        "            you can choose to directly pass an embedded representation. This is useful if you want more control over\n",
        "            how to convert `input_ids` indices into associated vectors than the model's internal embedding lookup\n",
        "            matrix.\n",
        "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "            of `inputs_embeds`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BartEncoder(BartPretrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    [`BartEncoderLayer`].\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "                than the model's internal embedding lookup matrix.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            if head_mask.size()[0] != (len(self.layers)):\n",
        "                raise ValueError(\n",
        "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "                )\n",
        "\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )\n",
        "\n",
        "\n",
        "class BartDecoder(BartPretrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.d_model,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
        "            ).to(self.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                of the decoder.\n",
        "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
        "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
        "                selected in `[0, 1]`:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
        "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "                all ``decoder_input_ids``` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor`\n",
        "                of shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n",
        "                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n",
        "                control over how to convert `input_ids` indices into associated vectors than the model's internal\n",
        "                embedding lookup matrix.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
        "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
        "            if attn_mask is not None:\n",
        "                if attn_mask.size()[0] != (len(self.layers)):\n",
        "                    raise ValueError(\n",
        "                        \"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "                    )\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    cross_attn_layer_head_mask=(\n",
        "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
        "                    ),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartModel(BartPretrainedModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = BartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=Seq2SeqModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        # different to other models, Bart automatically creates decoder_input_ids from\n",
        "        # input_ids if no decoder_input_ids are provided\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            if input_ids is None:\n",
        "                raise ValueError(\n",
        "                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n",
        "                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n",
        "                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n",
        "                )\n",
        "\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n",
        ")\n",
        "class BartForMaskedLM(BartPretrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.model = BartModel(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z--ouCGJ9hie"
      },
      "outputs": [],
      "source": [
        "from transformers import BartConfig, PreTrainedTokenizerFast, BartModel, BartForConditionalGeneration, BartForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHsz8ra5NYeT"
      },
      "outputs": [],
      "source": [
        "conf = BartConfig.from_json_file(\"config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LSN31nr9oSN"
      },
      "outputs": [],
      "source": [
        "conf = BartConfig(**BartCustomConfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv8uSuz_8FfE"
      },
      "outputs": [],
      "source": [
        "if \"custom_bart\" in locals() or \"custom_bart\" in globals():\n",
        "    del custom_bart\n",
        "custom_bart = BartForConditionalGeneration(conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5xGoVMC-z_X"
      },
      "outputs": [],
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Gunulhona/tbbarttokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_g2iz2BK7T"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(\"테스트1\", return_tensors=\"pt\", max_length=128, padding=\"max_length\", return_token_type_ids=False, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOOZoZXhTUxV"
      },
      "outputs": [],
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "device = xm.xla_device()\n",
        "\n",
        "custom_bart.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as6OnLTobEsp"
      },
      "outputs": [],
      "source": [
        "custom_bart(input_ids=tokens[\"input_ids\"],\n",
        "            attention_mask=tokens[\"attention_mask\"],\n",
        "            labels=tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7VcxEJtfMzL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(custom_bart.state_dict(),\"test.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0kiOmOIbrN2X"
      },
      "outputs": [],
      "source": [
        "#@title lightning tpu test mnist\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchmetrics.functional import accuracy\n",
        "from torchvision import transforms\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "# Note - you must have torchvision installed for this example\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "### Defining The `MNISTDataModule`\n",
        "\n",
        "class MNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, data_dir: str = \"./\"):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "        # self.dims is returned when you call dm.size()\n",
        "        # Setting default dims here because we know them.\n",
        "        # Could optionally be assigned dynamically in dm.setup()\n",
        "        self.dims = (1, 28, 28)\n",
        "        self.num_classes = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "class MNISTDataModule(LightningDataModule):\n",
        "    def __init__(self, data_dir: str = \"./\"):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        # self.dims is returned when you call dm.size()\n",
        "        # Setting default dims here because we know them.\n",
        "        # Could optionally be assigned dynamically in dm.setup()\n",
        "        self.dims = (1, 28, 28)\n",
        "        self.num_classes = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "class LitModel(LightningModule):\n",
        "    def __init__(self, channels, width, height, num_classes, hidden_size=64, learning_rate=2e-4):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(channels * width * height, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai62EUrh9s-A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "\n",
        "os.environ[\"TRIM_GRAPH_SIZE\"] = \"1000000\"\n",
        "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '1000000000'\n",
        "os.environ[\"PR_SET_PDEATHSIG\"]=\"1\"\n",
        "os.environ[\"PL_RECONCILE_PROCESS\"]=\"1\"\n",
        "# os.environ['XLA_USE_32BIT_LONG'] = '1'\n",
        "os.environ['XLA_USE_BF16'] = '0'\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] =\"false\"\n",
        "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "# Init DataModule\n",
        "dm = MNISTDataModule()\n",
        "# Init model from datamodule's attributes\n",
        "model = LitModel(*dm.size(), dm.num_classes)\n",
        "# Init trainer\n",
        "trainer = Trainer(max_epochs=1, progress_bar_refresh_rate=20, tpu_cores=8)\n",
        "# Train\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edbCBFpT36kM"
      },
      "outputs": [],
      "source": [
        "device = xm.xla_device()\n",
        "test_bart.to(device)\n",
        "input_ids = bart_tokenizer(\n",
        "              \"테스트 출럭값을 계산\",\n",
        "              return_tensors=\"pt\",\n",
        "              max_length=128,\n",
        "              padding=\"max_length\",\n",
        "              truncation=True,\n",
        "              add_special_tokens=True,\n",
        "              return_token_type_ids=False,\n",
        "              verbose=True,\n",
        "              ).input_ids.to(device)\n",
        "print(input_ids)\n",
        "print(\"Process\" ,\"is using\", xm.xla_real_devices([str(device)])[0])\n",
        "test_bart(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEIS_csI92mW"
      },
      "source": [
        "## end of notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_oy6a4e9-dG"
      },
      "outputs": [],
      "source": [
        "!rm -rdf logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aattQAB6lq_k"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"torch_xla==nightly\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZwiNi-3lsk9"
      },
      "outputs": [],
      "source": [
        "! pip install git+git://github.com/williamFalcon/pytorch-lightning.git@master --upgrade"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "XfahULH_8nv3",
        "1Av3zZOEJaIK",
        "md5AYWOVJUU6",
        "5Rp6SvAOq0Z1",
        "y_Zhj7rd8Xgh",
        "2ACv1xGtqvL5",
        "KR6hNP6BWta0",
        "oz8KK3i4rqgK",
        "MBKtqa9OomzU",
        "EnT7LqlXoltY",
        "lEIS_csI92mW"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "685542cc4ffc4ec6929c6b510f80dde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc1fb4ebd3d747eda75f0942bdd1e914",
              "IPY_MODEL_4f59dbf8696f4e34889e6650c27bdf1e",
              "IPY_MODEL_bcd35461dced433380a7c9a66052804c"
            ],
            "layout": "IPY_MODEL_54143cc64d064c07b17cb86dc5a9eff7"
          }
        },
        "dc1fb4ebd3d747eda75f0942bdd1e914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f49862b59745f0ac9751138913a4d5",
            "placeholder": "​",
            "style": "IPY_MODEL_badf81cec039443b87d46e0cba279c5b",
            "value": "folding data...: "
          }
        },
        "4f59dbf8696f4e34889e6650c27bdf1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718e2ef821cd4c259fd0ba29361a2ac8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a88bd9b4199e44858a0de775c3762b39",
            "value": 1
          }
        },
        "bcd35461dced433380a7c9a66052804c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3f24430b08a4009a105d8e3e0f70ad6",
            "placeholder": "​",
            "style": "IPY_MODEL_ffd3ada2dc5a42aaba3a10152db01299",
            "value": " 2/? [00:24&lt;00:00, 12.07s/it]"
          }
        },
        "54143cc64d064c07b17cb86dc5a9eff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f49862b59745f0ac9751138913a4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "badf81cec039443b87d46e0cba279c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "718e2ef821cd4c259fd0ba29361a2ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a88bd9b4199e44858a0de775c3762b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3f24430b08a4009a105d8e3e0f70ad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd3ada2dc5a42aaba3a10152db01299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}