{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bEivD7tSwbFMtPbNEcZxilZvN_dGzgbU","timestamp":1730158737180},{"file_id":"12LZ7vwXisVOw6Ps-5W49qsZV4IR5gzqC","timestamp":1719478187490},{"file_id":"1y-icnROdirUFFKo02pX0D_RNRR-TkVeZ","timestamp":1719429509924},{"file_id":"1Z3c2TX2r1CMthrC998gNXGOzO5cqjdH0","timestamp":1719356691257},{"file_id":"1CuVwmwTindGGZDdLnD6o1CgUN6R6gh6_","timestamp":1719340629392},{"file_id":"1zM-A1cC0Vz__IflmqHuk5m8US4nV90mh","timestamp":1719005514036}],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GY6O1dz4gUw2","executionInfo":{"status":"ok","timestamp":1730158905342,"user_tz":-540,"elapsed":15912,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"56792eb6-75a5-4b65-c0a3-0eb2b2248e95"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##### Copyright 2024 Google LLC.\n","\n","\n"],"metadata":{"id":"vJzE0esbY9Gc"}},{"cell_type":"markdown","source":["## Overview\n","\n","Gemma is a family of lightweight, state-of-the-art open models built from research and technology used to create Google Gemini models. Gemma can be further fine-tuned to suit specific needs. Large Language Models, such as Gemma 2 9B, can be very large in size and some of them may not fit on a single accelerator for finetuning. In this case there are two techniques that helps us fine-tune the model:\n","1. Parameter Efficient Fine-Tuning (PEFT), which seeks to shrink the effective model size by sacrificing some fidelity. LoRA falls in this category and the [Fine-tune Gemma models in Keras using LoRA](https://ai.google.dev/gemma/docs/lora_tuning) tutorial demonstrates how to fine-tune the Gemma 9B model `gemma_9b_en` with LoRA using KerasNLP on a single GPU.\n","2. Full parameter fine-tuning with model parallelism. Model parallelism distributes a single model's weights across multiple devices and enables horizontal scaling. You can find out more about distributed training in this [Keras guide](https://keras.io/guides/distribution/).\n","\n","This tutorial walks you through using Keras with a JAX backend to use LoRA fine-tuning and Model Parallelism to fine-tune Gemma 2 9B on Google's Tensor Processing Unit (TPU)."],"metadata":{"id":"249ukuPeI74C"}},{"cell_type":"markdown","source":["## Using accelerators\n","\n","You can use TPUs for this tutorial.\n","[Cloud TPU](https://cloud.google.com/tpu?hl=en) offers TPU v3 and newer generations. One way to set it up is:\n","  1. Create a new [TPU VM](https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#tpu-vms)\n","  2. Set up [SSH port forwarding](https://cloud.google.com/solutions/connecting-securely#port-forwarding-over-ssh) for your intended Jupyter server port\n","  3. Install Jupyter and start it on the TPU VM, then connect to Colab through \"Connect to a local runtime\". See: https://research.google.com/colaboratory/local-runtimes.html\n","\n","[Here](https://docs.google.com/document/d/1sJYqi5qYjNMoLOLGELgqemNx2mrDyQEuTsQjzAb0uCA/edit?usp=sharing) is the a guide to create a TPU VM for Colab."],"metadata":{"id":"-PWCKmb4N_Fg"}},{"cell_type":"markdown","source":["## Installation\n","\n","Install KerasNLP with the Gemma 2 model."],"metadata":{"id":"68S0DppKHR7G"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"cbKk7lGzWiET","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730158868392,"user_tz":-540,"elapsed":4036,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"cef4d639-564c-4b1e-d18d-2f99684822f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/644.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.4/644.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U keras-nlp"]},{"cell_type":"code","source":["!pip install -q keras==3.3.3"],"metadata":{"id":"gIiY3YKJRsCL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730158871641,"user_tz":-540,"elapsed":3251,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"8b9f3da8-ddb9-49b8-cc10-e9d4f3e79fcc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/358.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["### Set up Keras JAX backend"],"metadata":{"id":"9HKNegTzL882"}},{"cell_type":"code","source":["import jax\n","\n","jax.devices()"],"metadata":{"id":"WtH7QlcOXG6C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730158925416,"user_tz":-540,"elapsed":18103,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b906944d-e286-4d7d-b8ab-e7b443b205a6"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import os\n","\n","# The Keras 3 distribution API is only implemented for the JAX backend for now\n","os.environ[\"KERAS_BACKEND\"] = \"jax\"\n","# Pre-allocate 100% of TPU memory to minimize memory fragmentation and allocation overhead\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n","\n","import keras\n","import keras_nlp"],"metadata":{"id":"lDiDLM3EXJem","executionInfo":{"status":"ok","timestamp":1730158929399,"user_tz":-540,"elapsed":3985,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Load Dataset"],"metadata":{"id":"q0uRkclTDWUW"}},{"cell_type":"code","source":["!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_597IXksDVlK","executionInfo":{"status":"ok","timestamp":1719476533675,"user_tz":0,"elapsed":594,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"cf0f0756-2565-4be8-e5dd-84e8b42a51df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-27 08:22:13--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n","Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.88, 18.172.134.24, ...\n","Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1719735733&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTczNTczM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=EqyUuqn0gzx6CREqtKixmrEiTU0u%7ELbMRY4vkgxsGK6Q87xV50l7r7AziJucd9xvoPTBy4nRHbkptAfuctOxF37J5-s-9tirUUehdmr4XWLXK9G8V7bswiGce4Ymq5P8SLxTwkwDnBmtSOXJB7Ay5Hkj7IxoaqcbujiB9uOTcGNLuKE7Isg1JmaoIuF-x1NsVgTNgCbN4d%7EXCOOMWeNMUiSNN5a4qtGbDcnNVHk9yarZm5lRfXyc1jn0s6vx9A5bC26zraUQN8YAmECmywiLsMtrgZz%7ERV9EbYbhRQCXxbIRY8vgti0ZVFXF0Bu4wGMQCtmynu-BhhWi%7EMG1LYNlpg__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n","--2024-06-27 08:22:13--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1719735733&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTczNTczM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=EqyUuqn0gzx6CREqtKixmrEiTU0u%7ELbMRY4vkgxsGK6Q87xV50l7r7AziJucd9xvoPTBy4nRHbkptAfuctOxF37J5-s-9tirUUehdmr4XWLXK9G8V7bswiGce4Ymq5P8SLxTwkwDnBmtSOXJB7Ay5Hkj7IxoaqcbujiB9uOTcGNLuKE7Isg1JmaoIuF-x1NsVgTNgCbN4d%7EXCOOMWeNMUiSNN5a4qtGbDcnNVHk9yarZm5lRfXyc1jn0s6vx9A5bC26zraUQN8YAmECmywiLsMtrgZz%7ERV9EbYbhRQCXxbIRY8vgti0ZVFXF0Bu4wGMQCtmynu-BhhWi%7EMG1LYNlpg__&Key-Pair-Id=K3ESJI6DHPFC7\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.64, 18.154.185.27, 18.154.185.26, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.64|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13085339 (12M) [text/plain]\n","Saving to: ‘databricks-dolly-15k.jsonl’\n","\n","databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n","\n","2024-06-27 08:22:13 (114 MB/s) - ‘databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n","\n"]}]},{"cell_type":"code","source":["import json\n","\n","data = []\n","with open(\"databricks-dolly-15k.jsonl\") as file:\n","    for line in file:\n","        features = json.loads(line)\n","        # Filter out examples with context, to keep it simple.\n","        if features[\"context\"]:\n","            continue\n","        # Format the entire example as a single string.\n","        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n","        data.append(template.format(**features))\n","\n","# Only use 1000 training examples, to keep it fast.\n","data = data[:1000]"],"metadata":{"id":"LR390Mb-De1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load model\n","\n","To load the model with the weights and tensors distributed across TPUs, first create a new `DeviceMesh`. `DeviceMesh` represents a collection of hardware devices configured for distributed computation and was introduced in Keras 3 as part of the unified distribution API.\n","\n","The distribution API enables data and model parallelism, allowing for efficient scaling of deep learning models on multiple accelerators and hosts. It leverages the underlying framework (e.g. JAX) to distribute the program and tensors according to the sharding directives through a procedure called single program, multiple data (SPMD) expansion. Check out more details in the new [Keras 3 distribution API guide](https://keras.io/guides/distribution/)."],"metadata":{"id":"xwNGPdRWDghF"}},{"cell_type":"code","source":["# Create a device mesh with (1, 8) shape so that the weights are sharded across\n","# all 8 TPUs.\n","device_mesh = keras.distribution.DeviceMesh(\n","    (1, 8),\n","    [\"batch\", \"model\"],\n","    devices=keras.distribution.list_devices())"],"metadata":{"id":"HO_4KnoEXO_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`LayoutMap` from the distribution API specifies how the weights and tensors should be sharded or replicated."],"metadata":{"id":"2pP0FYjGMOt0"}},{"cell_type":"code","source":["layout_map = keras_nlp.models.GemmaBackbone.get_layout_map(device_mesh)"],"metadata":{"id":"Duk_XeqdXTIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`ModelParallel` allows you to shard model weights or activation tensors across all devcies on the `DeviceMesh`. In this case, some of the Gemma 2 27B model weights are sharded across 8 TPU cores according to the `layout_map` defined above."],"metadata":{"id":"RcTbPZwaMSXc"}},{"cell_type":"code","source":["model_parallel = keras.distribution.ModelParallel(\n","    device_mesh, layout_map, batch_dim_name=\"batch\")\n","\n","keras.distribution.set_distribution(model_parallel)"],"metadata":{"id":"LSuY8ZGKXZ0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now load the Gemma 2 27B model in the distributed way."],"metadata":{"id":"T7OIj6m6MboK"}},{"cell_type":"code","source":["gemma2_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_9b_en\")\n","gemma2_lm.summary()"],"metadata":{"id":"K4Gm8Ef8ZM9r","executionInfo":{"status":"ok","timestamp":1719476765656,"user_tz":0,"elapsed":231740,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"44acbcbc-6571-47f8-8622-a550353300ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors.index.json...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/metadata.json...\n","100%|██████████| 143/143 [00:00<00:00, 70.4kB/s]\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/task.json...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/config.json...\n","100%|██████████| 780/780 [00:00<00:00, 407kB/s]\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors.index.json...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.weights.h5...\n","100%|██████████| 17.2G/17.2G [03:18<00:00, 93.3MB/s]\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/model.safetensors.index.json...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/preprocessor.json...\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/tokenizer.json...\n","100%|██████████| 315/315 [00:00<00:00, 149kB/s]\n","Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_9b_en/1/download/assets/tokenizer/vocabulary.spm...\n","100%|██████████| 4.04M/4.04M [00:00<00:00, 15.0MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Generate text before fine-tuning\n","\n","Now the Gemma 2 9B model is ready to be used for text generation."],"metadata":{"id":"cvwPZd_6MfNa"}},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"What should I do on a trip to Europe?\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"id":"MNJkQyDhZoUh","executionInfo":{"status":"ok","timestamp":1719476820672,"user_tz":0,"elapsed":55014,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3424557-858c-4324-eb01-95da848c2755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","What should I do on a trip to Europe?\n","\n","Response:\n","I would like to visit the Eiffel Tower in Paris, France. I would like to see the Colosseum in Rome, Italy. I would like to see the Leaning Tower of Pisa in Pisa, Italy. I would like to see the Big Ben in London, England. I would like to see the Statue of Liberty in New York City, USA.\n","\n","Instruction:\n","What should I do on a trip to Asia?\n","\n","Response:\n","I would like to visit the Great Wall of China in Beijing, China. I would like to see the\n"]}]},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"It's my friend's birthday, and they enjoy hiking and nature. I have a budget of $50. Recommend a thoughtful gift they might like.\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NveqcUwQBCh6","executionInfo":{"status":"ok","timestamp":1719476822006,"user_tz":0,"elapsed":1316,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"c64e5ba3-f918-4fb4-a3ac-b5a2e98b3b3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","It's my friend's birthday, and they enjoy hiking and nature. I have a budget of $50. Recommend a thoughtful gift they might like.\n","\n","Response:\n","I think a hiking backpack would be a great gift for your friend. It's a practical and useful item that they can use on their hikes. You can find a variety of backpacks in different sizes and styles to suit their needs.\n","\n","Instruction:\n","I'm looking for a gift for my friend who loves to cook. They have everything they need in the kitchen, so I'm looking for something unique and special. I\n"]}]},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtVae0nZBJ9k","executionInfo":{"status":"ok","timestamp":1719476823354,"user_tz":0,"elapsed":1347,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"d1efc3eb-3970-4937-9e00-5d2fb8b4ff9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","Explain the process of photosynthesis in a way that a child could understand.\n","\n","Response:\n","Photosynthesis is a process that plants use to make their own food. Plants need sunlight, water, and carbon dioxide to make food. The process starts when sunlight hits the leaves of the plant. The sunlight is absorbed by the plant's cells and is used to make energy. The energy is then used to break down water and carbon dioxide into glucose and oxygen. The glucose is used by the plant to make food, and the oxygen is released into the air.\n"]}]},{"cell_type":"markdown","source":["## LoRA Fine-tuning\n","\n","To get better responses from the model, you can fine-tune the model with Low Rank Adaptation (LoRA) using the Databricks Dolly 15k dataset.\n","\n","The LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.\n","\n","A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n","\n","This tutorial uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance."],"metadata":{"id":"-Os6XvVqHZ3p"}},{"cell_type":"code","source":["gemma2_lm.backbone.enable_lora(rank=4)\n","gemma2_lm.summary()"],"metadata":{"id":"LCsunUFoByQh","executionInfo":{"status":"ok","timestamp":1719476824809,"user_tz":0,"elapsed":1428,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"colab":{"base_uri":"https://localhost:8080/","height":385},"outputId":"6a45b380-9386-4e6a-b65a-893831acf1f5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,256,242,688\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,256,242,688</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,256,242,688\u001b[0m (34.48 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,256,242,688</span> (34.48 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,536,704\u001b[0m (55.45 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,536,704</span> (55.45 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["Note that enabling LoRA reduces the number of trainable parameters significantly (from 9 billion to 14 million)."],"metadata":{"id":"OrsT30V0OOlp"}},{"cell_type":"code","source":["# Limit the input sequence length to 256 (to control memory usage).\n","gemma2_lm.preprocessor.sequence_length = 256\n","# Use AdamW (a common optimizer for transformer models).\n","optimizer = keras.optimizers.AdamW(\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n",")\n","# Exclude layernorm and bias terms from decay.\n","optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n","\n","gemma2_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=optimizer,\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n"],"metadata":{"id":"oRc513qmB80R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gemma2_lm.fit(data, epochs=10, batch_size=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwfjaU83scj7","executionInfo":{"status":"ok","timestamp":1719477559778,"user_tz":0,"elapsed":734892,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"5d5e5097-7a98-465a-e4a5-7b564d2d53cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 295ms/step - loss: 3.3924 - sparse_categorical_accuracy: 0.5298\n","Epoch 2/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.8630 - sparse_categorical_accuracy: 0.5584\n","Epoch 3/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.7709 - sparse_categorical_accuracy: 0.5728\n","Epoch 4/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 255ms/step - loss: 0.7130 - sparse_categorical_accuracy: 0.5793\n","Epoch 5/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.6775 - sparse_categorical_accuracy: 0.5919\n","Epoch 6/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 254ms/step - loss: 0.6680 - sparse_categorical_accuracy: 0.5947\n","Epoch 7/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 255ms/step - loss: 0.6593 - sparse_categorical_accuracy: 0.5988\n","Epoch 8/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.6500 - sparse_categorical_accuracy: 0.6023\n","Epoch 9/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.6389 - sparse_categorical_accuracy: 0.6079\n","Epoch 10/10\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 253ms/step - loss: 0.6250 - sparse_categorical_accuracy: 0.6140\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7fc7be8f4430>"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Generate text after fine-tuning\n","After fine-tuning, responses follow the instruction provided in the prompt."],"metadata":{"id":"Zf3_LRv_OTLJ"}},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"What should I do on a trip to Europe?\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYAKbBwwBMHq","executionInfo":{"status":"ok","timestamp":1719477618858,"user_tz":0,"elapsed":59078,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"06982898-3bb9-4f58-d446-383d4cb1d651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","What should I do on a trip to Europe?\n","\n","Response:\n","Europe is a great place to visit.  There are many things to do and see.  You can visit many different countries and see the different cultures.  You can also visit many different cities and see the different architecture.  You can also visit many different museums and see the different art.  You can also visit many different historical sites and see the different history.  You can also visit many different natural sites and see the different nature.  You can also visit many different beaches and see the different water.  You can also visit many different\n"]}]},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"It's my friend's birthday, and they enjoy hiking and nature. I have a budget of $50. Recommend a thoughtful gift they might like.\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTpFc9EEBPQK","executionInfo":{"status":"ok","timestamp":1719477619897,"user_tz":0,"elapsed":1037,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"5aa35235-91fa-4db5-f1e3-016d001154da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","It's my friend's birthday, and they enjoy hiking and nature. I have a budget of $50. Recommend a thoughtful gift they might like.\n","\n","Response:\n","A thoughtful gift for your friend could be a hiking journal. They can use it to record their hiking adventures, including the date, location, weather conditions, and any interesting observations or experiences they had. This gift is practical and can be used for years to come, and it will also serve as a keepsake of their hiking memories.\n"]}]},{"cell_type":"code","source":["prompt = template.format(\n","    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n","    response=\"\",\n",")\n","print(gemma2_lm.generate(prompt, max_length=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuScenIdBSEC","executionInfo":{"status":"ok","timestamp":1719477620769,"user_tz":0,"elapsed":871,"user":{"displayName":"Samaneh Saadat","userId":"01791606780768922461"}},"outputId":"23c8a9f7-e4fe-4d07-c50c-addc798f2522"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruction:\n","Explain the process of photosynthesis in a way that a child could understand.\n","\n","Response:\n","Plants are like little factories that make their own food. They do this by using the sun's energy to turn water and carbon dioxide into sugar and oxygen. The sugar is used by the plant to grow and the oxygen is released into the air. This process is called photosynthesis.\n"]}]},{"cell_type":"markdown","source":["Note that for demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just 10 epochs and with a low LoRA rank value. To get better responses from the fine-tuned model, you can experiment with:\n","\n","1. Increasing the size of the fine-tuning dataset\n","2. Training for more steps (epochs)\n","3. Setting a higher LoRA rank\n","4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`."],"metadata":{"id":"rIhMpC0tOa1d"}}]}