{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1B76NAjB51NbQfhtHvTU7WbNTWISMvAhl","authorship_tag":"ABX9TyNOUuy+FmxQoX/ijpJVdtC9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3093e44ce71f4b1184c6ad41f9366828":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f88f6a06723c4c26a51520d4c1f38e09","IPY_MODEL_048fdf8bd4144256b590cf891482ca66","IPY_MODEL_4c93244f86144a629de856530d8cf57c"],"layout":"IPY_MODEL_105d7c0d88d245dfb3b783132b17cabd"}},"f88f6a06723c4c26a51520d4c1f38e09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ff0da55f425452b99dcaf0d65aa4343","placeholder":"​","style":"IPY_MODEL_1acf18ef4e2b44f0a3f776e502d866ae","value":"config.json: 100%"}},"048fdf8bd4144256b590cf891482ca66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fd06061b42b4883907eb9b1b933a0ed","max":503,"min":0,"orientation":"horizontal","style":"IPY_MODEL_271905d3f72849faafb23bb54cade703","value":503}},"4c93244f86144a629de856530d8cf57c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e57158efc35540f3931b1eb03ec222a0","placeholder":"​","style":"IPY_MODEL_316b0929b0ca4aa3a9d3f042bd82bdad","value":" 503/503 [00:00&lt;00:00, 41.4kB/s]"}},"105d7c0d88d245dfb3b783132b17cabd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ff0da55f425452b99dcaf0d65aa4343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1acf18ef4e2b44f0a3f776e502d866ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2fd06061b42b4883907eb9b1b933a0ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"271905d3f72849faafb23bb54cade703":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e57158efc35540f3931b1eb03ec222a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316b0929b0ca4aa3a9d3f042bd82bdad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef2bf0bf1adf4ff3952d03ee9725274f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3dfa51e077ff44dd9b165c241e2d89db","IPY_MODEL_279ed0a4d9b04c1ebfe7da72e293c150","IPY_MODEL_3e1d32656e34480d9ba035a927e2559b"],"layout":"IPY_MODEL_fa423d05dc834f96b3374498045ab662"}},"3dfa51e077ff44dd9b165c241e2d89db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8c1864808f4b4aaac32025025b9254","placeholder":"​","style":"IPY_MODEL_790f68676eb24742bbc954a6584e8aa9","value":"pytorch_model.bin: 100%"}},"279ed0a4d9b04c1ebfe7da72e293c150":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3372d6a39242198855a3c155219ebe","max":6088802697,"min":0,"orientation":"horizontal","style":"IPY_MODEL_137da20dc49e4f48bbbc863ddc51da93","value":6088802697}},"3e1d32656e34480d9ba035a927e2559b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b316cf5d7bf14a7da993cbdc4e1517c8","placeholder":"​","style":"IPY_MODEL_4981e5daedc74e8a9f76def793e3b776","value":" 6.09G/6.09G [02:47&lt;00:00, 32.3MB/s]"}},"fa423d05dc834f96b3374498045ab662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a8c1864808f4b4aaac32025025b9254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790f68676eb24742bbc954a6584e8aa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d3372d6a39242198855a3c155219ebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"137da20dc49e4f48bbbc863ddc51da93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b316cf5d7bf14a7da993cbdc4e1517c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4981e5daedc74e8a9f76def793e3b776":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0fb074f7ae34a00961f14fe2247c968":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8311db8fcda749a388e1d9df7cec44f7","IPY_MODEL_b31f036b859f4613a3f3203b5f1f6a6a","IPY_MODEL_39475f620182432facc0874b70a069e2"],"layout":"IPY_MODEL_2d2d0873e38e49d689aa9aba5f2b1e34"}},"8311db8fcda749a388e1d9df7cec44f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37344bea3ea9444181ce13fd6d693db0","placeholder":"​","style":"IPY_MODEL_8ea7a0dadb064d8b9e71dffa0eb99c37","value":"generation_config.json: 100%"}},"b31f036b859f4613a3f3203b5f1f6a6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80244154e07e4c4d820ce4f733363000","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55cec9f47a1b4631a923707f5e69f7e7","value":116}},"39475f620182432facc0874b70a069e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9da06749e8a46d9b81d54946ae003a6","placeholder":"​","style":"IPY_MODEL_2ecc8d45fb0f43f4bc05fd2c95b1ee7f","value":" 116/116 [00:00&lt;00:00, 7.94kB/s]"}},"2d2d0873e38e49d689aa9aba5f2b1e34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37344bea3ea9444181ce13fd6d693db0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea7a0dadb064d8b9e71dffa0eb99c37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80244154e07e4c4d820ce4f733363000":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55cec9f47a1b4631a923707f5e69f7e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9da06749e8a46d9b81d54946ae003a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ecc8d45fb0f43f4bc05fd2c95b1ee7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d31ca43a7494a8890104e4c1bc57090":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51a356cd86f84f2ab685ba274e855e23","IPY_MODEL_a08d331060304951b93a6ce4750d2e3e","IPY_MODEL_05c53b75268743f299b19991d241f72e"],"layout":"IPY_MODEL_b0b983e2e0b0422b9359dc072b15cb71"}},"51a356cd86f84f2ab685ba274e855e23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4022aa5e0d1149f9bf91cdecdec2b794","placeholder":"​","style":"IPY_MODEL_fc121834f82b4444ad34b180a93e0831","value":"tokenizer_config.json: 100%"}},"a08d331060304951b93a6ce4750d2e3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1682931e96f41ee821f8abf35430413","max":236,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ed9954fd9714a40bbaec897c9466537","value":236}},"05c53b75268743f299b19991d241f72e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a22fa254fdd749489e61720c23dfee47","placeholder":"​","style":"IPY_MODEL_3f0207f55d6645efbdb5185859d59eb8","value":" 236/236 [00:00&lt;00:00, 18.9kB/s]"}},"b0b983e2e0b0422b9359dc072b15cb71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4022aa5e0d1149f9bf91cdecdec2b794":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc121834f82b4444ad34b180a93e0831":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1682931e96f41ee821f8abf35430413":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ed9954fd9714a40bbaec897c9466537":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a22fa254fdd749489e61720c23dfee47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0207f55d6645efbdb5185859d59eb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9fad8a53d284e3d9eaa8adb157b379b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64fc8f0ff4094710b3493b1eff90251e","IPY_MODEL_3dec33cd6b1e4b598113299c3aca9032","IPY_MODEL_be64018d3a2c4a19a26d38b710116f09"],"layout":"IPY_MODEL_f94b0344782f4829ba14cc99775d52ca"}},"64fc8f0ff4094710b3493b1eff90251e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d84c158175294ee992d69cef0e73d95e","placeholder":"​","style":"IPY_MODEL_3aa3e8b7bda64f0884e5fb2772c102f7","value":"tokenizer.json: 100%"}},"3dec33cd6b1e4b598113299c3aca9032":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50e72b6991b24edab0dcfc95582ef1dc","max":2689059,"min":0,"orientation":"horizontal","style":"IPY_MODEL_587fa5004af64cb8ae38cf21098d4a9d","value":2689059}},"be64018d3a2c4a19a26d38b710116f09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a8de695f35a4202b8124b775a3c11d7","placeholder":"​","style":"IPY_MODEL_7ad7fef57fec4fd2bc69f8dbf7d19a85","value":" 2.69M/2.69M [00:00&lt;00:00, 5.54MB/s]"}},"f94b0344782f4829ba14cc99775d52ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84c158175294ee992d69cef0e73d95e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aa3e8b7bda64f0884e5fb2772c102f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50e72b6991b24edab0dcfc95582ef1dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"587fa5004af64cb8ae38cf21098d4a9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a8de695f35a4202b8124b775a3c11d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad7fef57fec4fd2bc69f8dbf7d19a85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1510b0bebf04555ba0b21969962e9a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_818cef3bf98d492fbe7a204f316b81de","IPY_MODEL_67ac877348894aa3b200afd273475110","IPY_MODEL_ae7a0531b72f46909fa9286a981632d3"],"layout":"IPY_MODEL_fd20878d760146608cad0efa6c6a79e0"}},"818cef3bf98d492fbe7a204f316b81de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f1d96d4dc64c508a0a28c0309f128d","placeholder":"​","style":"IPY_MODEL_96df4692ff47452e8f88a1ee77dd4e81","value":"special_tokens_map.json: 100%"}},"67ac877348894aa3b200afd273475110":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3b294ebf3294db8bb97729007611353","max":93,"min":0,"orientation":"horizontal","style":"IPY_MODEL_307cd30565234a96b384b9eebac2d226","value":93}},"ae7a0531b72f46909fa9286a981632d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7f3cb1a847645d69ad71db604a77922","placeholder":"​","style":"IPY_MODEL_0079e32b0cb64ba088dc7667b27163ad","value":" 93.0/93.0 [00:00&lt;00:00, 7.26kB/s]"}},"fd20878d760146608cad0efa6c6a79e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f1d96d4dc64c508a0a28c0309f128d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96df4692ff47452e8f88a1ee77dd4e81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3b294ebf3294db8bb97729007611353":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"307cd30565234a96b384b9eebac2d226":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7f3cb1a847645d69ad71db604a77922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0079e32b0cb64ba088dc7667b27163ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"weO225X2EjZ2"}},{"cell_type":"code","source":["%%shell\n","git clone https://github.com/oneapi-src/oneCCL.git\n","cd oneCCL\n","mkdir build\n","cd build\n","cmake ..\n","make -j install\n","mkdir -p /opt/intel/oneccl\n","mv ./_install/env /opt/intel/oneccl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPlW06ckPRG_","executionInfo":{"status":"ok","timestamp":1737086731588,"user_tz":-540,"elapsed":121402,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"a6cc029a-ef97-4cb7-aaed-f961afda06ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'oneCCL'...\n","remote: Enumerating objects: 12214, done.\u001b[K\n","remote: Counting objects: 100% (1032/1032), done.\u001b[K\n","remote: Compressing objects: 100% (268/268), done.\u001b[K\n","remote: Total 12214 (delta 879), reused 769 (delta 764), pack-reused 11182 (from 2)\u001b[K\n","Receiving objects: 100% (12214/12214), 206.95 MiB | 29.11 MiB/s, done.\n","Resolving deltas: 100% (8629/8629), done.\n","\u001b[0mCMake Deprecation Warning at CMakeLists.txt:18 (cmake_minimum_required):\n","  Compatibility with CMake < 3.10 will be removed from a future version of\n","  CMake.\n","\n","  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n","  to tell CMake that the project requires at least <min> but has been updated\n","  to work with policies introduced by <max> or earlier.\n","\n","\u001b[0m\n","-- The C compiler identification is GNU 11.4.0\n","-- The CXX compiler identification is GNU 11.4.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Installation directory: /content/oneCCL/build/_install\n","-- Build type: release\n","-- C compiler : /usr/bin/cc\n","-- CXX compiler : /usr/bin/c++\n","-- Build examples: ON\n","-- Build functional tests: ON\n","-- Build cmake configs: ON\n","-- Enable MPI support: ON\n","-- Enable MPI tests support: ON\n","-- Enable SYCL interop event support: ON\n","-- Enable OFI HMEM support: ON\n","-- Enable OFI out-of-tree providers support: OFF\n","-- Enable ITT profiling support: ON\n","-- Enable PMIX support: ON\n","-- Enable DRM support: TRUE\n","-- Enable stub backend: ON\n","-- Enable linker rpath flags: OFF\n","-- MPI_INCLUDE_DIR: /content/oneCCL/deps/mpi/include/\n","-- MPI_LIB_DIR: /content/oneCCL/deps/mpi/lib/\n","-- LIBFABRIC_LIB_DIR: /content/oneCCL/deps/ofi/lib/\n","-- LIBFABRIC_INCLUDE_DIR: /content/oneCCL/deps/ofi/include\n","-- HWLOC_INCLUDE_DIR: /content/oneCCL/deps/hwloc/include/\n","-- HWLOC_LIB_DIR: /content/oneCCL/deps/hwloc/lib/\n","-- ITT_INCLUDE_DIR: /content/oneCCL/deps/itt/include\n","-- ITT_LIB_DIR: /content/oneCCL/deps/itt/lib64\n","-- LEVEL_ZERO_INCLUDE_DIR: /content/oneCCL/deps/level_zero/include/\n","-- DRM_INCLUDE_DIR: /usr/include/drm\n","-- PMIX_INCLUDE_DIR: /content/oneCCL/deps/pmix/include/\n","-- COMPUTE_BACKEND is not defined\n","-- Enable ITT profiling support\n","-- Enable PMIX support\n","-- Enable DRM support\n","-- Enable stub backend\n","-- BF16 AVX512F compiler: ON\n","-- binutils version: 2.38\n","-- BF16 AVX512BF compiler: ON\n","-- BF16 target attributes: ON\n","-- BF16 GPU truncate: OFF\n","-- FP16 compiler: ON\n","-- FP16 AVX512FP16 compiler: OFF\n","-- FP16 target attributes: ON\n","-- FP16 GPU truncate: OFF\n","-- AVX compiler: ON\n","-- AVX target attributes: ON\n","\u001b[0m-- Git branch: master, commit: d20fd44\u001b[0m\n","-- SRC C_FLAGS:   -Wall -Wextra -Wno-unused-parameter -Werror  -D_GNU_SOURCE -fvisibility=internal -Wno-implicit-fallthrough  -Wformat -Wformat-security -D_FORTIFY_SOURCE=2 -fstack-protector -fstack-protector-strong -DCCL_ENABLE_MPI -pthread\n","-- SRC CXX_FLAGS:   -Wall -Wextra -Wno-unused-parameter -Werror  -D_GNU_SOURCE -fvisibility=internal -Wno-implicit-fallthrough -DCCL_ENABLE_ITT=1 -DCCL_ENABLE_PMIX=1 -DCCL_ENABLE_DRM=1 -DCCL_ENABLE_STUB_BACKEND=1 -faligned-new -DCCL_ENABLE_SYCL_INTEROP_EVENT=1  -Wformat -Wformat-security -D_FORTIFY_SOURCE=2 -fstack-protector -fstack-protector-strong -DCCL_ENABLE_MPI -pthread\n","-- SRC SHARED_LINKER_FLAGS:   -fPIE -fPIC -z noexecstack -z relro -z now -Wl,--version-script=/content/oneCCL/ccl.map\n","-- SRC INCLUDE_DIRS: /content/oneCCL/include;/content/oneCCL/src;/content/oneCCL/src/atl;/content/oneCCL/deps/level_zero/include/;/content/oneCCL/deps/ofi/include;/content/oneCCL/deps/hwloc/include/;/content/oneCCL/deps/itt/include;/content/oneCCL/deps/pmix/include/;/content/oneCCL/deps/mpi/include/\n","-- SRC LINK_DIRS: \n","-- SRC LINK_LIBS: dl;pthread;/content/oneCCL/deps/hwloc/lib//libhwloc.a;/content/oneCCL/deps/itt/lib64/libittnotify.a\n","-- Found NUMA: /usr/lib/x86_64-linux-gnu/libnuma.so\n","-- NUMA was found, include_dir: /usr/include, libraries: /usr/lib/x86_64-linux-gnu/libnuma.so\n","\u001b[0mCMake Deprecation Warning at tests/functional/CMakeLists.txt:16 (cmake_minimum_required):\n","  Compatibility with CMake < 3.10 will be removed from a future version of\n","  CMake.\n","\n","  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n","  to tell CMake that the project requires at least <min> but has been updated\n","  to work with policies introduced by <max> or earlier.\n","\n","\u001b[0m\n","\u001b[0mCMake Deprecation Warning at tests/functional/CMakeLists.txt:19 (cmake_policy):\n","  The OLD behavior for policy CMP0048 will be removed from a future version\n","  of CMake.\n","\n","  The cmake-policies(7) manual explains that the OLD behaviors of all\n","  policies are deprecated and that a policy should be set to OLD only under\n","  specific short-term circumstances.  Projects should be ported to the NEW\n","  behavior and not rely on setting a policy to OLD.\n","\n","\u001b[0m\n","-- FT CMAKE_PROJECT_NAME: oneCCL\n","-- FT PROJECT_NAME: oneCCL functional tests\n","-- PROC_MAPS: 2:1,2\n","\u001b[33mCMake Warning at tests/googletest-release-1.8.1/googletest/CMakeLists.txt:47 (project):\n","  VERSION keyword not followed by a value or was followed by a value that\n","  expanded to nothing.\n","\n","\u001b[0m\n","\u001b[0mCMake Deprecation Warning at tests/googletest-release-1.8.1/googletest/CMakeLists.txt:49 (cmake_minimum_required):\n","  Compatibility with CMake < 3.10 will be removed from a future version of\n","  CMake.\n","\n","  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n","  to tell CMake that the project requires at least <min> but has been updated\n","  to work with policies introduced by <max> or earlier.\n","\n","\u001b[0m\n","\u001b[33mCMake Warning (dev) at tests/googletest-release-1.8.1/googletest/cmake/internal_utils.cmake:239 (find_package):\n","  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules\n","  are removed.  Run \"cmake --help-policy CMP0148\" for policy details.  Use\n","  the cmake_policy command to set the policy and suppress this warning.\n","\n","Call Stack (most recent call first):\n","  tests/googletest-release-1.8.1/googletest/CMakeLists.txt:84 (include)\n","This warning is for project developers.  Use -Wno-dev to suppress it.\n","\u001b[0m\n","-- Found PythonInterp: /usr/local/bin/python (found version \"3.10.12\")\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","-- Found Threads: TRUE\n","-- FT build type: Release\n","-- FT CCL_ROOT: \n","-- FT INC_DIRS: /content/oneCCL/tests/functional/../googletest-release-1.8.1/googletest/include;/content/oneCCL/tests/functional/../googletest-release-1.8.1/googletest/src;/content/oneCCL/tests/functional/../../examples/include\n","-- FT COMPUTE_BACKEND: \n","-- Configuring done (1.2s)\n","-- Generating done (0.1s)\n","-- Build files have been written to: /content/oneCCL/build\n","[  1%] \u001b[32mBuilding CXX object tests/functional/gtest_build/CMakeFiles/gtest.dir/src/gtest-all.cc.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comm/stub_comm.cpp.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/stub_kvs_impl.cpp.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/atl_base_comm.cpp.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/atl_base_transport.cpp.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/mpi/atl_mpi.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/mpi/atl_mpi_comm.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/mpi/atl_mpi_ctx.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/ofi/atl_ofi.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/ofi/atl_ofi_comm.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/atl_def.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/ofi/atl_ofi_helper.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable_simple.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable_simple_internal.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/helper.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/kvs_keeper.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/pmi_listener.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/kvs/users_kvs.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/kvs/internal_kvs.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/kvs/internal_kvs_server.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_resizable_rt/pmi_resizable/resizable_pmi.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_rt/pmi_simple.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding C object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_rt/pmi/simple_pmi.c.o\u001b[0m\n","[ 12%] \u001b[32mBuilding C object src/CMakeFiles/ccl-objects.dir/atl/util/pm/pmi_rt/pmi/simple_pmiutil.c.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_common_op_attrs.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_allgather_op_attr.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_allgatherv_op_attr.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_alltoall_op_attr.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_allreduce_op_attr.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_barrier_op_attr.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_alltoallv_op_attr.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_pt2pt_op_attr.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_bcast_op_attr.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_reduce_op_attr.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/attr/ccl_reduce_scatter_op_attr.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/coll_param.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/coll_util.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/allgather.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/allgatherv/allgatherv.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/allreduce/allreduce.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/allreduce/allreduce_rma.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/algorithm_utils.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/alltoall.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/alltoallv.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/bcast.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/broadcast.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/barrier/barrier.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/double_tree_ops.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/recv.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/reduce.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/reduce_scatter/reduce_scatter.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/coll.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/algorithms/send.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/coll_check.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/group/group.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selection.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_allgather.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_allgatherv.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_allreduce.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_alltoall.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_alltoallv.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_barrier.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_recv.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_bcast.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_reduce.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_reduce_scatter.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/coll/selection/selector_send.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comm/atl_tag.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comm/comm.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comm/comm_selector.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/context/context.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/datatype/datatype.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/device/device.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/env/env.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/env/env_parser.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/event/ccl_event.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/event/impls/host_event.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/event/impls/native_event.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/global/global.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/framework/framework.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/log/log.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/request/request.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/stream/stream.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/exchange_utils.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/fd_info.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/memcpy.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/profile.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/spinlock.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/utils.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/version.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/utils/yield.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/api_wrapper/api_wrapper.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/api_wrapper/mpi_api_wrapper.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/api_wrapper/ofi_api_wrapper.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/common/api_wrapper/pmix_api_wrapper.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comp/bf16/bf16.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comp/bf16/bf16_intrisics.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comp/comp.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comp/fp16/fp16.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/comp/fp16/fp16_intrisics.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/exec/exec.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/exec/thread/base_thread.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/exec/thread/listener.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/exec/thread/service_worker.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/exec/thread/worker.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/fusion/fusion.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/hwloc/hwloc_wrapper.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/native_device_api/sycl/export.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/parallelizer/parallelizer.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/buffer/buffer_cache.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/cache/cache.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/buffer/buffer_manager.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/cache/recycle_storage.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/coll/coll_entry.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/cache/key.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/copy/copy_entry.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/copy/copy_helper.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/deps_entry.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/entry.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/factory/chunked_entry_factory.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/recv_copy_entry.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/entry/reduce_local_entry.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/queue/flow_control.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/queue/queue.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/queue/strict_queue.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/sched_base.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/sched.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/sched_group.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/sched_restart_manager.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/sched/sched_timer.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/topology/topo_manager.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/unordered_coll/unordered_coll.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_api_functions.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_coll_attr.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_comm_attr.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_comm_split_attr.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_datatype_attr.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_event.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_kvs_attr.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_app_api_init_attr.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_communicator.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_device.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_context.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_environment.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_stream.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_kvs.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_cpp_utils.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_attr.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_coll_attr.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_init_attr.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_comm_attr.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_comm_split_attr.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_kvs_attr.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/ccl-objects.dir/ccl_empty_stream.cpp.o\u001b[0m\n","[ 70%] \u001b[32m\u001b[1mLinking CXX static library libgtest.a\u001b[0m\n","[ 70%] Built target gtest\n","[ 71%] \u001b[32mBuilding CXX object tests/functional/gtest_build/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library libgtest_main.a\u001b[0m\n","[ 71%] Built target gtest_main\n","[ 71%] Built target ccl-objects\n","[ 71%] \u001b[32m\u001b[1mLinking CXX shared library libccl.so\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library libccl.a\u001b[0m\n","[ 71%] Built target ccl-static\n","[ 71%] Built target ccl\n","[ 71%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/src/benchmark.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object examples/common/CMakeFiles/version.dir/version.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object examples/cpu/CMakeFiles/cpu_allgather_test.dir/cpu_allgather_test.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object examples/cpu/CMakeFiles/cpu_allgatherv_test.dir/cpu_allgatherv_test.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object examples/cpu/CMakeFiles/cpu_broadcast_test.dir/cpu_broadcast_test.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object examples/pt2pt/CMakeFiles/ccl_bw.dir/src/ccl_bw.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object examples/cpu/CMakeFiles/cpu_allreduce_bf16_test.dir/cpu_allreduce_bf16_test.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object examples/pt2pt/CMakeFiles/ccl_latency.dir/src/ccl_latency.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object examples/external_launcher/CMakeFiles/external_launcher.dir/external_launcher.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgather_test.dir/allgather_test.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object examples/cpu/CMakeFiles/cpu_allreduce_test.dir/cpu_allreduce_test.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgather_test.dir/lp.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgather_test.dir/conf.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgatherv_test.dir/allgatherv_test.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgather_test.dir/transport.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allreduce_test.dir/allreduce_test.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allreduce_test.dir/conf.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgatherv_test.dir/conf.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoallv_test.dir/conf.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoall_test.dir/conf.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgatherv_test.dir/lp.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoallv_test.dir/alltoallv_test.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoallv_test.dir/lp.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoall_test.dir/transport.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allreduce_test.dir/lp.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allreduce_test.dir/transport.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoall_test.dir/alltoall_test.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoall_test.dir/lp.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/alltoallv_test.dir/transport.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/bcast_test.dir/transport.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/bcast_test.dir/conf.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/bcast_test.dir/bcast_test.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/allgatherv_test.dir/transport.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/bcast_test.dir/lp.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/broadcast_test.dir/broadcast_test.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_scatter_test.dir/reduce_scatter_test.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/broadcast_test.dir/conf.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/broadcast_test.dir/lp.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_scatter_test.dir/conf.cpp.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_scatter_test.dir/lp.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_test.dir/reduce_test.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_scatter_test.dir/transport.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/broadcast_test.dir/transport.cpp.o\u001b[0m\n","[ 89%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_test.dir/conf.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_test.dir/lp.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object tests/functional/CMakeFiles/reduce_test.dir/transport.cpp.o\u001b[0m\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable version\u001b[0m\n","[ 91%] Built target version\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable cpu_broadcast_test\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable cpu_allreduce_test\u001b[0m\n","[ 93%] Built target cpu_broadcast_test\n","[ 93%] Built target cpu_allreduce_test\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable cpu_allgather_test\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable cpu_allgatherv_test\u001b[0m\n","[ 94%] Built target cpu_allgather_test\n","[ 94%] Built target cpu_allgatherv_test\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable cpu_allreduce_bf16_test\u001b[0m\n","[ 95%] Built target cpu_allreduce_bf16_test\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable external_launcher\u001b[0m\n","[ 95%] Built target external_launcher\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable ccl_latency\u001b[0m\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable ccl_bw\u001b[0m\n","[ 95%] Built target ccl_bw\n","[ 95%] Built target ccl_latency\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable reduce_scatter_test\u001b[0m\n","[ 95%] Built target reduce_scatter_test\n","[ 96%] \u001b[32m\u001b[1mLinking CXX executable reduce_test\u001b[0m\n","[ 96%] Built target reduce_test\n","[ 96%] \u001b[32m\u001b[1mLinking CXX executable allreduce_test\u001b[0m\n","[ 96%] Built target allreduce_test\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable alltoallv_test\u001b[0m\n","[ 97%] Built target alltoallv_test\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable alltoall_test\u001b[0m\n","[ 97%] Built target alltoall_test\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable broadcast_test\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable bcast_test\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable allgatherv_test\u001b[0m\n","[ 98%] Built target broadcast_test\n","[ 98%] Built target bcast_test\n","[ 98%] Built target allgatherv_test\n","[ 99%] \u001b[32m\u001b[1mLinking CXX executable allgather_test\u001b[0m\n","[ 99%] Built target allgather_test\n","[100%] \u001b[32m\u001b[1mLinking CXX executable benchmark\u001b[0m\n","[100%] Built target benchmark\n","\u001b[36mInstall the project...\u001b[0m\n","-- Install configuration: \"Release\"\n","-- Installing: /content/oneCCL/build/_install/env/vars.sh\n","-- Installing: /content/oneCCL/build/_install/env/setvars.sh\n","-- Installing: /content/oneCCL/build/_install/modulefiles/ccl\n","-- Installing: /content/oneCCL/build/_install/share/doc/ccl/licensing/third-party-programs.txt\n","-- Installing: /content/oneCCL/build/_install/share/doc/ccl/licensing/LICENSE\n","-- Installing: /content/oneCCL/build/_install/lib/cmake/oneCCL/oneCCLConfig.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/cmake/oneCCL/oneCCLConfigVersion.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/libccl.so.1.0\n","-- Installing: /content/oneCCL/build/_install/lib/libccl.so.1\n","-- Installing: /content/oneCCL/build/_install/lib/libccl.so\n","-- Installing: /content/oneCCL/build/_install/lib/FindIntelSYCL.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/FindIntelSYCL_level_zero.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/FindNUMA.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/FindSYCL.cmake\n","-- Installing: /content/oneCCL/build/_install/lib/libccl.a\n","-- Installing: /content/oneCCL/build/_install/include\n","-- Installing: /content/oneCCL/build/_install/include/oneapi\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/context.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/stream_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/device_type_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_split_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/environment.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/communicator.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/exception.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/coll_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/datatype_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/device_types.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/aliases.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/context_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/context_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/device_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/device_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/api_functions.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/kvs_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/init_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_split_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/config.h\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/export_api.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/sycl\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/sycl/export.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/context.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/platform.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/queue.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/primitives.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/event.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/device.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/native_device_api/empty/export.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/stream_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/event.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/init_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/kvs_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/comm_split_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/types_policy.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/device.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/init_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/kvs.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/datatype_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/stream.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/coll_attr_ids.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/type_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/lp_types.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/types.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/kvs_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/datatype_attr_ids_traits.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/coll_attr.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl/string.hpp\n","-- Installing: /content/oneCCL/build/_install/include/oneapi/ccl.hpp\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/libfabric.so.1\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/libfabric.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libpsm3-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libtcp-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libpsmx2-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libshm-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/librxm-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libverbs-1.12-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/libfabric/lib/prov/libverbs-1.1-fi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/hydra_bstrap_proxy\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/hydra_nameserver\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/hydra_pmi_proxy\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpicc\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpicxx\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiexec\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiexec.hydra\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpigcc\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpigxx\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiicc\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiicpc\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiicpx\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpiicx\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/bin/mpirun\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/include\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/include/mpi.h\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/include/mpicxx.h\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/include/mpio.h\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpicxx.so.12.0.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpicxx.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpifort.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpifort.so.12.0.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpicxx.so.12\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpi.so.12\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpi.so.12.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpifort.so.12.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpi.so.12.0.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpifort.so.12\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpi.so\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/lib/libmpicxx.so.12.0\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc/\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_generic_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm_efa.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm-ofi_tcp-ofi-rxm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_clx-ap_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_tcp.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm_tcp.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_generic_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_ofi_mlx.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_ofi_efa.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_efa_100_x2.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_gnr_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_generic_ofi_mlx_hcoll.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm_cxi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_psm3_200.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_srf_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm-ofi_cxi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm-ofi_psm3.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_efa.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_clx-ap_ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm_tcp.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_ofi_psm3.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm-ofi_efa.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_emr_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_generic_ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_psm3.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_emr_shm-ofi_psm3_100.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_emr_shm-ofi_psm3_200.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm-ofi_tcp.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_srf_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_ofi_tcp-ofi-rxm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_mlx_100.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_emr_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_generic_shm-ofi_mlx_hcoll.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_gnr_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_clx-ap_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm-ofi_tcp.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm_efa_100_x2.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_icx_shm-ofi_mlx.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_spr_shm.dat\n","-- Installing: /content/oneCCL/build/_install/opt/mpi/etc//tuning_skx_shm-ofi.dat\n","-- Installing: /content/oneCCL/build/_install/share/doc/ccl/licensing/mpi/\n","-- Installing: /content/oneCCL/build/_install/share/doc/ccl/licensing/mpi//license.txt\n","-- Installing: /content/oneCCL/build/_install/share/doc/ccl/licensing/mpi//third-party-programs.txt\n","-- Installing: /content/oneCCL/build/_install/examples/benchmark/benchmark\n","-- Installing: /content/oneCCL/build/_install/examples/common/version\n","-- Installing: /content/oneCCL/build/_install/examples/cpu/cpu_allgather_test\n","-- Installing: /content/oneCCL/build/_install/examples/cpu/cpu_allgatherv_test\n","-- Installing: /content/oneCCL/build/_install/examples/cpu/cpu_allreduce_bf16_test\n","-- Installing: /content/oneCCL/build/_install/examples/cpu/cpu_allreduce_test\n","-- Installing: /content/oneCCL/build/_install/examples/cpu/cpu_broadcast_test\n","-- Installing: /content/oneCCL/build/_install/examples/pt2pt/ccl_bw\n","-- Installing: /content/oneCCL/build/_install/examples/pt2pt/ccl_latency\n","-- Installing: /content/oneCCL/build/_install/examples/external_launcher/external_launcher\n","-- Installing: /content/oneCCL/build/_install/examples/external_launcher/run.sh\n","-- Installing: /content/oneCCL/build/_install/examples/external_launcher/run_binary.sh\n","-- Installing: /content/oneCCL/build/allgather_test\n","-- Installing: /content/oneCCL/build/allgatherv_test\n","-- Installing: /content/oneCCL/build/allreduce_test\n","-- Installing: /content/oneCCL/build/alltoall_test\n","-- Installing: /content/oneCCL/build/alltoallv_test\n","-- Installing: /content/oneCCL/build/bcast_test\n","-- Installing: /content/oneCCL/build/broadcast_test\n","-- Installing: /content/oneCCL/build/reduce_scatter_test\n","-- Installing: /content/oneCCL/build/reduce_test\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAphU6E1lcvy","executionInfo":{"status":"ok","timestamp":1737086731589,"user_tz":-540,"elapsed":5,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"cac68744-3eaf-46e6-d3c4-7fc06466ef01","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","onnx\n","onnxruntime\n","onnx2pytorch\n","tensorrt\n","vllm\n","lmdeploy\n","openvino\n","ipex_llm[all]\n","optimum-intel[extras]\n","ray\n","nncf\n","konlpy\n","mpi4py\n","deepspeed"]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install intel_extension_for_pytorch==2.1.0 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/cpu/us/\n","!pip install https://intel-extension-for-pytorch.s3.amazonaws.com/torch_ccl/cpu/oneccl_bind_pt-2.1.0%2Bcpu-cp39-cp39-linux_x86_64.whl\n","!VLLM_TARGET_DEVICE=cpu pip install -r -U requirements.txt"],"metadata":{"id":"cATRLnmdnFU3","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat /proc/cpuinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"810xGT1_CF-0","executionInfo":{"status":"ok","timestamp":1737089800449,"user_tz":-540,"elapsed":426,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"d1db0627-c3de-4999-81c8-f750715ea46a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processor\t: 0\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 0\n","cpu cores\t: 6\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 1\n","cpu cores\t: 6\n","apicid\t\t: 2\n","initial apicid\t: 2\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 2\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 2\n","cpu cores\t: 6\n","apicid\t\t: 4\n","initial apicid\t: 4\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 3\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 3\n","cpu cores\t: 6\n","apicid\t\t: 6\n","initial apicid\t: 6\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 4\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 4\n","cpu cores\t: 6\n","apicid\t\t: 8\n","initial apicid\t: 8\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 5\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 5\n","cpu cores\t: 6\n","apicid\t\t: 10\n","initial apicid\t: 10\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 6\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 0\n","cpu cores\t: 6\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 7\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 1\n","cpu cores\t: 6\n","apicid\t\t: 3\n","initial apicid\t: 3\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 8\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 2\n","cpu cores\t: 6\n","apicid\t\t: 5\n","initial apicid\t: 5\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 9\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 3\n","cpu cores\t: 6\n","apicid\t\t: 7\n","initial apicid\t: 7\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 10\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 4\n","cpu cores\t: 6\n","apicid\t\t: 9\n","initial apicid\t: 9\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 11\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 7\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2200.168\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 12\n","core id\t\t: 5\n","cpu cores\t: 6\n","apicid\t\t: 11\n","initial apicid\t: 11\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs taa mmio_stale_data retbleed eibrs_pbrsb bhi\n","bogomips\t: 4400.33\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","os.environ[\"HF_WRITE_TOKEN\"] = userdata.get(\"HF_WRITE_TOKEN\")\n","os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HF_WRITE_TOKEN\")\n","!huggingface-cli login --token $HF_WRITE_TOKEN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3-EU5iXPMhF","executionInfo":{"status":"ok","timestamp":1737089862573,"user_tz":-540,"elapsed":1705,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"28715462-08e6-4f98-cc7b-a93652419f92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","The token `WriteToken` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `WriteToken`\n"]}]},{"cell_type":"markdown","source":["# Select Model"],"metadata":{"id":"uuZ4dVPUFWP5"}},{"cell_type":"code","source":["#@title Select Language Model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import gc\n","\n","if \"origin_model\" in globals() or \"origin_model\" in locals():\n","    del origin_model\n","\n","gc.collect()\n","\n","base_model_id = \"beomi/KoAlpaca-KoRWKV-1.5B\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M-v2-Instruct\", \"google/gemma-2b-it\", \"beomi/KoAlpaca-KoRWKV-1.5B\"] {allow-input: true}\n","\n","origin_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True,\n","    # quantization_config=bnb_config\n","    )\n","\n","processor = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    add_special_tokens=True,\n","    trust_remote_code=True)\n","processor.model_input_names=['input_ids', 'attention_mask']\n","if processor.pad_token is None:\n","    processor.pad_token = processor.eos_token\n","\n","processor.padding_side = \"right\"\n","processor.truncation_side = \"right\"\n","\n","@torch.inference_mode()\n","def inference(input_, model):\n","    template = {\n","        \"role\": \"user\",\n","        \"content\": input_\n","    }\n","    inputs = processor.apply_to_template(\n","        template,\n","        return_tensors='pt'\n","    )\n","    # inputs = processor(input_,\n","    #                    return_tensors=\"pt\",\n","    #                    padding=\"max_length\",\n","    #                    truncation=True,\n","    #                    max_length=128)\n","    outputs = model.generate(inputs,\n","                             max_new_tokens=50)\n","    return processor.batch_decode(outputs, skip_special_tokens=True)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["3093e44ce71f4b1184c6ad41f9366828","f88f6a06723c4c26a51520d4c1f38e09","048fdf8bd4144256b590cf891482ca66","4c93244f86144a629de856530d8cf57c","105d7c0d88d245dfb3b783132b17cabd","6ff0da55f425452b99dcaf0d65aa4343","1acf18ef4e2b44f0a3f776e502d866ae","2fd06061b42b4883907eb9b1b933a0ed","271905d3f72849faafb23bb54cade703","e57158efc35540f3931b1eb03ec222a0","316b0929b0ca4aa3a9d3f042bd82bdad","ef2bf0bf1adf4ff3952d03ee9725274f","3dfa51e077ff44dd9b165c241e2d89db","279ed0a4d9b04c1ebfe7da72e293c150","3e1d32656e34480d9ba035a927e2559b","fa423d05dc834f96b3374498045ab662","0a8c1864808f4b4aaac32025025b9254","790f68676eb24742bbc954a6584e8aa9","4d3372d6a39242198855a3c155219ebe","137da20dc49e4f48bbbc863ddc51da93","b316cf5d7bf14a7da993cbdc4e1517c8","4981e5daedc74e8a9f76def793e3b776","f0fb074f7ae34a00961f14fe2247c968","8311db8fcda749a388e1d9df7cec44f7","b31f036b859f4613a3f3203b5f1f6a6a","39475f620182432facc0874b70a069e2","2d2d0873e38e49d689aa9aba5f2b1e34","37344bea3ea9444181ce13fd6d693db0","8ea7a0dadb064d8b9e71dffa0eb99c37","80244154e07e4c4d820ce4f733363000","55cec9f47a1b4631a923707f5e69f7e7","a9da06749e8a46d9b81d54946ae003a6","2ecc8d45fb0f43f4bc05fd2c95b1ee7f","3d31ca43a7494a8890104e4c1bc57090","51a356cd86f84f2ab685ba274e855e23","a08d331060304951b93a6ce4750d2e3e","05c53b75268743f299b19991d241f72e","b0b983e2e0b0422b9359dc072b15cb71","4022aa5e0d1149f9bf91cdecdec2b794","fc121834f82b4444ad34b180a93e0831","e1682931e96f41ee821f8abf35430413","5ed9954fd9714a40bbaec897c9466537","a22fa254fdd749489e61720c23dfee47","3f0207f55d6645efbdb5185859d59eb8","c9fad8a53d284e3d9eaa8adb157b379b","64fc8f0ff4094710b3493b1eff90251e","3dec33cd6b1e4b598113299c3aca9032","be64018d3a2c4a19a26d38b710116f09","f94b0344782f4829ba14cc99775d52ca","d84c158175294ee992d69cef0e73d95e","3aa3e8b7bda64f0884e5fb2772c102f7","50e72b6991b24edab0dcfc95582ef1dc","587fa5004af64cb8ae38cf21098d4a9d","6a8de695f35a4202b8124b775a3c11d7","7ad7fef57fec4fd2bc69f8dbf7d19a85","c1510b0bebf04555ba0b21969962e9a0","818cef3bf98d492fbe7a204f316b81de","67ac877348894aa3b200afd273475110","ae7a0531b72f46909fa9286a981632d3","fd20878d760146608cad0efa6c6a79e0","c5f1d96d4dc64c508a0a28c0309f128d","96df4692ff47452e8f88a1ee77dd4e81","c3b294ebf3294db8bb97729007611353","307cd30565234a96b384b9eebac2d226","b7f3cb1a847645d69ad71db604a77922","0079e32b0cb64ba088dc7667b27163ad"]},"cellView":"form","id":"9W9RxgzzFX41","outputId":"3cc741cc-8431-4d58-9944-e6de3dd4e8b0","executionInfo":{"status":"ok","timestamp":1721366230425,"user_tz":-540,"elapsed":174669,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3093e44ce71f4b1184c6ad41f9366828"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/6.09G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2bf0bf1adf4ff3952d03ee9725274f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0fb074f7ae34a00961f14fe2247c968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d31ca43a7494a8890104e4c1bc57090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.69M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9fad8a53d284e3d9eaa8adb157b379b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1510b0bebf04555ba0b21969962e9a0"}},"metadata":{}}]},{"cell_type":"code","source":["#@title Select Vision Model\n","\n","from transformers import AutoProcessor, Owlv2ForObjectDetection, Owlv2VisionConfig\n","import torch\n","\n","base_model_id = \"google/owlv2-base-patch16-ensemble\" # @param [\"google/owlv2-base-patch16-ensemble\", \"\"] {allow-input: true}\n","\n","processor = AutoProcessor.from_pretrained(base_model_id)\n","origin_model = Owlv2ForObjectDetection.from_pretrained(base_model_id)\n","\n","@torch.inference_mode()\n","def inference(input_, model):\n","    model.eval()\n","    inputs = processor(input_, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    return outputs"],"metadata":{"cellView":"form","id":"ifokOEXAGJev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantize Model\n"],"metadata":{"id":"3h222ZK1EV0M"}},{"cell_type":"code","source":["from ipex_llm.transformers import AutoModelForCausalLM\n","from transformers import AutoTokenizer\n","\n","ipex_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True)\n","\n","outputs = inference(input_=\"test 입력 처리 요구\",\n","                    model=ipex_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"id":"6k0sn_rfOD1w","executionInfo":{"status":"error","timestamp":1721366234954,"user_tz":-540,"elapsed":4530,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"f112cb23-43c1-4b6c-c512-ae54e1fec751"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'PreTrainedTokenizerFast' object has no attribute 'apply_to_template'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b95b35b9d72c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mipex_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m outputs = inference(input_=\"test 입력 처리 요구\",\n\u001b[0m\u001b[1;32m      7\u001b[0m                     model=ipex_model)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-cfb4bf8e8825>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     }\n\u001b[0;32m---> 36\u001b[0;31m     inputs = processor.apply_to_template(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'PreTrainedTokenizerFast' object has no attribute 'apply_to_template'"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 onnx qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import numpy as np\n","import torch\n","import onnx\n","import onnxruntime as ort\n","import random\n","import ray\n","from copy import deepcopy\n","import string\n","from pathlib import Path\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","from optimum.onnxruntime import ORTModelForCausalLM, ORTConfig\n","from transformers.onnx import FeaturesManager\n","import transformers\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","@ray.remote\n","def make_texts():\n","    return \"\".join(random.sample('가나다라마바사아자차카타파하그느드르므브스으즈츠크트프흐구누두루무부수우주추쿠투푸후', 30))\n","\n","randomtexts = ray.get([ make_texts.remote() for i in range(10) ])\n","\n","\n","# Load your original model\n","original_model = origin_model\n","test_in = torch.randint(1000, (1, 128))\n","with torch.no_grad():\n","    torch.onnx.export(\n","        original_model,\n","        args=tuple(processor(randomtexts[0], return_tensors=\"pt\").values()),\n","        input_names=[\"input_ids\", \"attention_mask\"],\n","        output_names=[\"logits\"],\n","        dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n","                      'attention_mask': {0: 'batch_size', 1: 'sequence'},\n","                      'logits': {0: 'batch_size', 1: 'sequence'}},\n","        f=\"original_model.pt\")\n","\n","\n","# Quantize the model using ONNX\n","quantized_model = quantize_dynamic(\n","    model_input=\"original_model.pt\",\n","    model_output=\"quantized_model.onnx\",\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","# onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","result = []\n","for t in randomtexts:\n","    outputs = inference(input_=t,\n","                        model=origin_model)\n","    result.append(outputs)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","print(result[0])\n","\n","# Measure execution time for the quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","q_session = ort.InferenceSession(\"quantized_model.onnx\", providers=[\"CPUExecutionProvider\"])\n","\n","result = []\n","for t in randomtexts:\n","    preprocessed = processor(t, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n","    onnx_input= {\n","         \"input_ids\": preprocessed[\"input_ids\"].astype(np.int64),\n","         \"attention_mask\": preprocessed[\"attention_mask\"].astype(np.int64)\n","     }\n","    result.append(q_session.run(None, input_feed=onnx_input)[0])\n","end_time = time.time()\n","quantized_execution_time = end_time - start_time\n","print(\"Quantized model execution time:\", quantized_execution_time)\n","print(result[0].shape)\n","config = deepcopy(original_model.config)\n","\n","\n","\n","feature = \"causal-lm\"\n","model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(\n","    original_model, feature=feature)\n","onnx_config = model_onnx_config(original_model.config)\n","\n","# export\n","onnx_inputs, onnx_outputs = transformers.onnx.export(\n","        preprocessor=processor,\n","        model=original_model,\n","        config=onnx_config,\n","        opset=16,\n","        output=Path(\"trfs-model.onnx\"))\n","\n","q_model = ORTModelForCausalLM(q_session, original_model.config,\n","                              use_io_binding=False, use_cache=False)\n","\n","# @ray.remote\n","def ray_onnx(text):\n","    return inference(input_=text,\n","                     model=q_model)\n","\n","start_time = time.time()\n","result = [ray_onnx(t) for t in randomtexts]\n","end_time = time.time()\n","ray_quantized_execution_time = end_time - start_time\n","print(\"Ray + Quantized model execution time:\", ray_quantized_execution_time)\n","\n","# Compare execution times\n","speedup = original_execution_time / quantized_execution_time\n","print(\"Speedup:\", speedup)\n","speedup = original_execution_time / ray_quantized_execution_time\n","print(\"Ray Speedup:\", speedup)\n"],"metadata":{"id":"zyU0RmgXpreF","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"52099de6-0a40-4453-f827-76fbbeef9df3","executionInfo":{"status":"error","timestamp":1721369703335,"user_tz":-540,"elapsed":3442637,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["`attention_mask` was passed, but it is unused in this model.\n","WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.0/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.0/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.1/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.1/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.2/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.2/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.3/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.3/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.4/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.4/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.5/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.5/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.6/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.6/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.7/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.7/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.8/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.8/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.9/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.9/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.10/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.10/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.11/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.11/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.12/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.12/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.13/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.13/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.14/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.14/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.15/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.15/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.16/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.16/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.17/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.17/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.18/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.18/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.19/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.19/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.20/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.20/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.21/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.21/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.22/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.22/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.23/attention/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n","WARNING:root:Inference failed or unsupported type to quantize for tensor '/rwkv/blocks.23/feed_forward/time_shift/Slice_output_0', type is tensor_type {\n","  elem_type: 7\n","  shape {\n","    dim {\n","      dim_value: 3\n","    }\n","    dim {\n","      dim_value: 2\n","    }\n","  }\n","}\n",".\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'PreTrainedTokenizerFast' object has no attribute 'apply_to_template'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e51621459a61>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandomtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     outputs = inference(input_=t,\n\u001b[0m\u001b[1;32m     59\u001b[0m                         model=origin_model)\n\u001b[1;32m     60\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-cfb4bf8e8825>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     }\n\u001b[0;32m---> 36\u001b[0;31m     inputs = processor.apply_to_template(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'PreTrainedTokenizerFast' object has no attribute 'apply_to_template'"]}]},{"cell_type":"code","source":["import deepspeed\n","from ipex_llm import optimize_model\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","original_model = origin_model\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","result = []\n","for t in randomtexts:\n","    outputs = inference(input_=t,\n","                        model=origin_model)\n","    result.append(outputs)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","print(result[0])\n","\n","\n","world_size = int(os.getenv(\"WORLD_SIZE\", \"2\"))\n","local_rank = int(os.getenv(\"RANK\", \"-1\")) # RANK is automatically set by CCL distributed backend\n","\n","ds_model = deepspeed.init_inference(\n","    original_model, # an AutoModel of Transformers\n","    mp_size = world_size, # instance (process) count\n","    dtype=torch.int8,\n","    replace_method=\"auto\")\n","\n","ds_model = optimize_model(ds_model.module.to(f'cpu'), low_bit='sym_int4')\n","ds_model = ds_model.to(f'cpu:{local_rank}') # move partial model to local rank\n","result = []\n","with torch.inference_mode():\n","    start_time = time.time()\n","    result += [inference(t, ds_model) for t in randomtexts]\n","    end_time = time.time()\n","\n","quantized_execution_time = end_time - start_time\n","print(\"Deepspeed model execution time:\", quantized_execution_time)\n","print(result[0])"],"metadata":{"id":"piv_sBHMOqYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 tensorrt qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import ray\n","import tensorrt as trt\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load your original model\n","original_model = origin_model\n","test_time = 5\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(test_time):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# TensorRT quantization and execution\n","logger = trt.Logger(trt.Logger.INFO)\n","builder = trt.Builder(logger)\n","network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n","parser = trt.OnnxParser(network, logger)\n","\n","with open(\"quantized_model.onnx\", \"rb\") as model:\n","    if not parser.parse(model.read()):\n","        for error in range(parser.num_errors):\n","            print(parser.get_error(error))\n","\n","# Build TensorRT engine\n","engine = builder.build_cuda_engine(network)\n","\n","# Measure execution time for the TensorRT quantized model\n","start_time = time.time()\n","# Run inference with the TensorRT engine\n","for _ in range(test_time):\n","    preprocessed = processor(\"test 입력 처리 요구\",\n","                             return_tensors=\"np\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=128)\n","    inputs[0].host = preprocessed[\"input_ids\"].astype(np.float32)  # Assuming input_ids is the input tensor name\n","    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n","end_time = time.time()\n","trt_quantized_execution_time = end_time - start_time\n","print(\"TensorRT quantized model execution time:\", trt_quantized_execution_time)\n","\n","@ray.remote\n","def ray_inference(text):\n","    preprocessed = processor(text,\n","                            return_tensors=\"np\",\n","                            padding=\"max_length\",\n","                            truncation=True,\n","                            max_length=128)\n","    inputs[0].host = preprocessed[\"input_ids\"].astype(np.float32)  # Assuming input_ids is the input tensor name\n","    return do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n","\n","\n","start_time = time.time()\n","for _ in range(test_time):\n","    text_id = ray.put(\"test 입력 처리 요구\")\n","    ray.get([ray_inference.remote(text_id)])\n","\n","end_time = time.time()\n","ray_trt_quantized_execution_time = end_time - start_time\n","print(\"Ray + TensorRT quantized model execution time:\", ray_trt_quantized_execution_time)\n","\n","\n","# Compare execution times\n","speedup_trt = original_execution_time / trt_quantized_execution_time\n","speedup_ray = original_execution_time / ray_trt_quantized_execution_time\n","print(\"Speedup with TensorRT quantization:\", speedup_trt)\n","print(\"Speedup with Ray + TensorRT :\", speedup_ray)\n"],"metadata":{"id":"TwhYub_Hp6sT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8eb2ba60-e58f-4ed9-b4e2-a120c02cb8f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model execution time: 179.06584286689758\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 vllm qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","import os\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","os.environ[\"VLLM_TARGET_DEVICE\"] = \"cpu\"\n","\n","# Load your original model\n","original_model = origin_model\n","test_time = 5\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(test_time):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# vllm quantization and execution\n","# Initialize vLLM with the quantized model\n","llm = LLM(model=base_model_id,\n","          tensor_parallel_size=1,\n","          device='cpu')\n","\n","# Generate text using vLLM\n","prompts = [\"This is a prompt.\"]\n","sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n","start_time = time.time()\n","for _ in range(test_time):\n","    result = llm.generate(prompts, sampling_params)\n","end_time = time.time()\n","vllm_quantized_execution_time = end_time - start_time\n","print(\"vLLM quantized model execution time:\", vllm_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_vllm = original_execution_time / vllm_quantized_execution_time\n","print(\"Speedup with vLLM quantization:\", speedup_vllm)\n"],"metadata":{"id":"QPrvNnf_qGTZ","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"error","timestamp":1721228741162,"user_tz":-540,"elapsed":224353,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"85f85fc1-9d60-49ae-d1dd-cfac2637ede8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model execution time: 222.8475422859192\n","INFO 07-17 15:05:38 llm_engine.py:174] Initializing an LLM engine (v0.5.2) with config: model='Qwen/Qwen2-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen/Qwen2-1.5B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n","WARNING 07-17 15:05:39 cpu_executor.py:134] CUDA graph is not supported on CPU, fallback to the eager mode.\n","WARNING 07-17 15:05:39 cpu_executor.py:161] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e51d3b0cb176>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# vllm quantization and execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Initialize vLLM with the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m llm = LLM(model=base_model_id,\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mtensor_parallel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           device='cpu')\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         )\n\u001b[0;32m--> 150\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    151\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    247\u001b[0m             self.model_config)\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         self.model_executor = executor_class(\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, prompt_adapter_config)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Instantiate the worker and load the model to CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\u001b[0m in \u001b[0;36m_init_worker\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m         distributed_init_method = get_distributed_init_method(\n\u001b[1;32m     38\u001b[0m             get_ip(), get_open_port())\n\u001b[0;32m---> 39\u001b[0;31m         self.driver_worker = CPUWorker(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mparallel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/cpu_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, local_rank, rank, distributed_init_method, lora_config, multimodal_config, kv_cache_dtype, prompt_adapter_config, is_driver_worker)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_cached_hf_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0minit_cached_hf_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         self.model_runner: CPUModelRunner = CPUModelRunner(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mparallel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/cpu_model_runner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, lora_config, multimodal_config, kv_cache_dtype, prompt_adapter_config, is_driver_worker, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         self.attn_backend = get_attn_backend(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_attention_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\u001b[0m in \u001b[0;36mget_attn_backend\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size, is_blocksparse)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBlocksparseFlashAttentionBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\n\u001b[0m\u001b[1;32m     46\u001b[0m                                 \u001b[0msliding_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                 block_size)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\u001b[0m in \u001b[0;36mwhich_attn_to_use\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# FlashAttn in NVIDIA GPUs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mselected_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_Backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLASH_ATTN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Volta and Turing NVIDIA GPUs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \"\"\"\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":["\n","# prompt: 기존 모델 실행 시간과 lmdeploy qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","# 단 lmdeploy는 cli가 아닌 lmdeploy 패키지 import 를 통해 실행하여야 함\n","\n","import time\n","import torch\n","import onnx\n","import lmdeploy\n","from onnx2pytorch import ConvertModel\n","from transformers import AutoConfig, AutoModelForCausalLM\n","from lmdeploy import turbomind as tm\n","from lmdeploy import pipeline, TurbomindEngineConfig, ChatTemplateConfig\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","# Load your original model\n","original_model = origin_model\n","test_time = 5\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(test_time):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# lmdeploy quantization\n","onnx_model = onnx.load(\"quantized_model.onnx\" ,load_external_data=True)\n","\n","torch.save(original_model.model, \"hf_model.pth\")\n","\n","# pytorch_model = ConvertModel(onnx_model)\n","# config = AutoConfig.from_pretrained(\"bart\")  # 적절한 모델 구성으로 변경하세요\n","# config.architectures = [\"BartModel\"]  # 모델 아키텍처에 맞게 조정하세요\n","\n","# hf_model = AutoModelForCausalLM.from_config(config)\n","# hf_model.load_state_dict(pytorch_model.state_dict())\n","# hf_model.save_pretrained(\"onnx_hf_model\")\n","\n","tm_model = tm.TurboMind(model_path=base_model_id)\n","\n","tm.quantize(\n","    base_model_id,\n","    \"lmdeploy_quantized.tm\",\n","    w_bits=4, w_group_size=128)\n","quantized_tm_model = tm.TurboMind(model_path=quantized_model_path)\n","chat = quantized_tm_model.create_chat_session()\n","\n","# lmdeploy_pipe = pipeline(\n","#     base_model_id,\n","#     backend_config=TurbomindEngineConfig(model_format='hf', tp=4),\n","#     chat_template_config=ChatTemplateConfig(model_name='llama2'))\n","\n","# Measure execution time for the lmdeploy quantized model\n","# Run inference with the quantized model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(test_time):\n","    response, _ = quantized_chat.chat(\"This is a prompt.\")\n","end_time = time.time()\n","lmdeploy_quantized_execution_time = end_time - start_time\n","print(\"lmdeploy quantized model execution time:\", lmdeploy_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_lmdeploy = original_execution_time / lmdeploy_quantized_execution_time\n","print(\"Speedup with lmdeploy quantization:\", speedup_lmdeploy)\n"],"metadata":{"id":"VtG4Vcx7qMND","colab":{"base_uri":"https://localhost:8080/","height":630},"executionInfo":{"status":"error","timestamp":1721278381525,"user_tz":-540,"elapsed":30999,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b319acf2-aa54-47eb-8506-94a3be7bce09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model execution time: 29.716737508773804\n"]},{"output_type":"error","ename":"OSError","evalue":"Incorrect path_or_model_id: 'Gunulhona/tb_pretrained_sts/triton_models/tokenizer'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Gunulhona/tb_pretrained_sts/triton_models/tokenizer'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-c07f1a6ffed2>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# hf_model.save_pretrained(\"onnx_hf_model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTurboMind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m tm.quantize(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmdeploy/turbomind/turbomind.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, engine_config, model_source, model_name, model_format, group_size, tp, chat_template_config, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             tokenizer_model_path = osp.join(model_path, 'triton_models',\n\u001b[1;32m    190\u001b[0m                                             'tokenizer')\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             self.model_comm = self._from_workspace(model_path=model_path,\n\u001b[1;32m    193\u001b[0m                                                    engine_config=engine_config)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmdeploy/tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_file)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentencePieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmdeploy/tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_dir)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;34m'Can not find tokenizer.json. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 'It may take long time to initialize the tokenizer.')\n\u001b[0;32m--> 150\u001b[0;31m         self.model = AutoTokenizer.from_pretrained(model_dir,\n\u001b[0m\u001b[1;32m    151\u001b[0m                                                    trust_remote_code=True)\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_space_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'Gunulhona/tb_pretrained_sts/triton_models/tokenizer'. Please provide either the path to a local folder or the repo_id of a model on the Hub."]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 openvino qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","#@markdown Colab 환경에서는 AMD CPU 사용 중\n","\n","\n","import numpy as np\n","import openvino as ov\n","from optimum.intel import OVModelForCausalLM\n","from optimum.intel.openvino import OVQuantizationConfig\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load your original model\n","original_model = origin_model\n","test_time = 5\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(test_time):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Load the ONNX model\n","core = ov.Core()\n","ov_model = core.read_model(\"quantized_model.onnx\")\n","ov_model.reshape({model_input.any_name: ov.PartialShape([1, '?']) for model_input in ov_model.inputs})\n","\n","# Quantize the model\n","tput = {'PERFORMANCE_HINT': 'THROUGHPUT'}\n","compiled_model = core.compile_model(ov_model, 'CPU', tput)\n","\n","# ireqs = ov.AsyncInferQueue(compiled_model)\n","\n","# ov_model = OVModelForCausalLM.from_pretrained(base_model_id, export=True)\n","ov_model = OVModelForCausalLM(model=ov_model,\n","                              config=original_model.config,\n","                              use_cache=False,\n","                              use_io_binding=False,)\n","\n","\n","# Save the quantized model\n","# ov.serialize(quantized_model, \"quantized_model.xml\")\n","\n","# Load the quantized model\n","\n","# Measure execution time for the OpenVINO quantized model\n","start_time = time.time()\n","# Run inference with the OpenVINO quantized model\n","for _ in range(test_time):\n","    preprocessed = processor(\"test 입력 처리 요구\",\n","                             return_tensors=\"np\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=128)\n","    onnx_input= {\n","        \"input_ids\": preprocessed[\"input_ids\"].astype(np.int64),\n","        \"attention_mask\": preprocessed[\"attention_mask\"].astype(np.int64),\n","     }\n","\n","    # ireqs.start_async(onnx_input)\n","    result = inference(input_=\"test 입력 처리 요구\",\n","                       model=ov_model)\n","end_time = time.time()\n","# ireqs.wait_all()\n","openvino_quantized_execution_time = end_time - start_time\n","print(\"OpenVINO quantized model execution time:\", openvino_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_openvino = original_execution_time / openvino_quantized_execution_time\n","print(\"Speedup with OpenVINO quantization:\", speedup_openvino)\n"],"metadata":{"id":"Y7SVCYOfqeDn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721285623327,"user_tz":-540,"elapsed":464450,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b297c337-f072-439c-bf5e-c666e8006bb2","cellView":"code"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Original model execution time: 409.310045003891\n"]},{"output_type":"stream","name":"stderr","text":["Compiling the model to CPU ...\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["OpenVINO quantized model execution time: 45.05198836326599\n","Speedup with OpenVINO quantization: 9.08528257850723\n"]}]},{"cell_type":"markdown","source":["# AWQ"],"metadata":{"id":"6v8KdM8BNomI"}},{"source":["!pip install autoawq\n","!pip install peft\n","!pip install transformers --upgrade\n","!pip install av decord\n","# !git clone https://github.com/kongds/MoRA.git && pip install -e MoRA/peft-mora"],"cell_type":"code","metadata":{"id":"Lw7fduiLORql","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3d336c9c-afd9-4bbd-f42a-f24186580707"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting autoawq\n","  Downloading autoawq-0.2.8.tar.gz (71 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (2.6.0+cu124)\n","Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.2.0)\n","Collecting transformers<=4.47.1,>=4.45.0 (from autoawq)\n","  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.21.1)\n","Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.13.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq) (1.6.0)\n","Collecting datasets>=2.20 (from autoawq)\n","  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.23.0)\n","Requirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.20->autoawq)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\n","Collecting xxhash (from datasets>=2.20->autoawq)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.20->autoawq)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.11.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.6.0->autoawq)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->autoawq) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.6.0->autoawq) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (2024.11.6)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (0.5.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.6.0->autoawq) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n","Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: autoawq\n","  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autoawq: filename=autoawq-0.2.8-py3-none-any.whl size=108745 sha256=61fd4a5b90c121dbe14b57b9ba904e9cf6c067f00a9da601cc27875477fc751f\n","  Stored in directory: /root/.cache/pip/wheels/fd/03/fe/99c1c678bfe8aca712186466969ed866f52feda95ae1dcd1b1\n","Successfully built autoawq\n"]}]},{"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from awq import AutoAWQForCausalLM # installed package is auto awq, but the installed package named awq...\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer\n","import torch\n","\n","# model_id = \"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\"  # 한국어 모델 ID\n","model_id = \"Qwen/Qwen3-32B\"\n","\n","# model = AutoPeftModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, low_cpu_mem_usage=True)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# model = model.merge_and_unload()"],"cell_type":"code","metadata":{"id":"RM_lFYiXOSDS"},"execution_count":null,"outputs":[]},{"source":["quant_config = {\n","    \"quant_method\": \"awq\",\n","    \"bits\": 4,  # 양자화 비트 수 (예: 4, 8)\n","    \"group_size\": 128,  # 그룹 크기 (AWQ 알고리즘의 매개변수)\n","    \"damp_percent\": 0.01, # damp percent (AWQ 알고리즘의 매개변수)\n","    \"desc_act\": False # desc_act (AWQ 알고리즘의 매개변수)\n","}"],"cell_type":"code","metadata":{"id":"G1I-hxZHOSsb"},"execution_count":null,"outputs":[]},{"source":["# 몇 가지 예제 텍스트를 사용하여 calibration 데이터셋을 생성합니다.\n","calibration_data = [\n","    \"안녕하세요. 오늘 날씨가 어떻습니까?\",\n","    \"한국어로 된 질문에 답변해 주세요.\",\n","    \"이 모델은 AutoAWQ를 사용하여 양자화되었습니다.\"\n","]\n","inputs = tokenizer(calibration_data, return_tensors=\"pt\", padding=True)"],"cell_type":"code","metadata":{"id":"K8diKsUlOTGt"},"execution_count":null,"outputs":[]},{"source":["# Calibration 데이터셋을 사용하여 모델을 양자화합니다.\n","# Calibration 데이터셋이 없으면 AutoAWQ가 자동으로 생성합니다.\n","\n","# Qwen2VLAwqQuantizer의 경우, Qwen2VLAwqQuantizer를 사용해야합니다.\n","# from autoawq.quantizers.qwen2_vl_awq_quantizer import Qwen2VLAwqQuantizer\n","\n","#quantized_model = AutoAWQForCausalLM.from_pretrained(\n","#    model_id,\n","#    quantization_config=quant_config,\n","#    trust_remote_code=True,\n","#    quantizer_cls=Qwen2VLAwqQuantizer\n","#)\n","# quantized_model = AutoAWQForCausalLM.from_pretrained(\n","#     model_id,\n","#     quantization_config=quant_config,\n","#     trust_remote_code=True,\n","    # calibration data를 지정할 경우, 아래와 같이 지정합니다.\n","    # calib_data=inputs\n","# )\n","\n","# 모델을 저장합니다.\n","# quantized_model.save_pretrained(\"quantized_model\")\n","\n","from awq import AutoAWQForCausalLM\n","from transformers import AutoTokenizer\n","\n","quant_path = 'quantized-awq'\n","quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n","\n","# Load model\n","model = AutoAWQForCausalLM.from_pretrained(model_id, **{\"model_type\": \"hyperclovax_vlm\",\"low_cpu_mem_usage\": True})\n","tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n","\n","# Quantize\n","model.quantize(tokenizer, quant_config=quant_config)\n","\n","# Save quantized model\n","model.save_quantized(quant_path)\n","tokenizer.save_pretrained(quant_path)\n"],"cell_type":"code","metadata":{"id":"7Ox6932pOTlQ"},"execution_count":null,"outputs":[]},{"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    inputs = inputs.to(device)\n","else:\n","    device = torch.device(\"cpu\")\n","\n","# 양자화된 모델을 로드합니다.\n","quantized_model = AutoAWQForCausalLM.from_pretrained(\n","    \"/content/quantized-awq\",\n","    trust_remote_code=True).to(device)\n","\n","# 추론을 실행합니다.\n","prompt = \"한국어로 된 질문에 답변해 주세요. 어떤 삶을 살아왔나요?\"\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","outputs = quantized_model.generate(\n","    input_ids=inputs[\"input_ids\"].to(device))\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"cell_type":"code","metadata":{"id":"XTo7tJ4ROUGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repo_name = \"Gunulhona/Gemma-System-9B-MoRA-SimPO-AWQ\"\n","repo_name = \"Gunulhona/Qwen3-32B-awq-4bit\"\n","quantized_model.push_to_hub(repo_name)\n","tokenizer.push_to_hub(repo_name)"],"metadata":{"id":"7KFC0MIoOc53"},"execution_count":null,"outputs":[]}]}