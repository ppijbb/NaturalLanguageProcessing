{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1B76NAjB51NbQfhtHvTU7WbNTWISMvAhl","authorship_tag":"ABX9TyNWYM16Qxq+J9VhsqHLQX8L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"weO225X2EjZ2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAphU6E1lcvy","executionInfo":{"status":"ok","timestamp":1721031493785,"user_tz":-540,"elapsed":314,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"883accac-63ff-4a3f-f33c-0a65c9587620","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","onnx\n","onnxruntime\n","tensorrt\n","vllm\n","lmdeploy\n","openvino"]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install -r requirements.txt"],"metadata":{"id":"cATRLnmdnFU3","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Select Model"],"metadata":{"id":"uuZ4dVPUFWP5"}},{"cell_type":"code","source":["#@title Select Language Model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","\n","base_model_id = \"Gunulhona/tb_pretrained_sts\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M\", \"google/gemma-2b-it\"] {allow-input: true}\n","\n","origin_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True,\n","    # quantization_config=bnb_config\n","    )\n","\n","processor = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    add_special_tokens=True,\n","    trust_remote_code=True)\n","processor.model_input_names=['input_ids', 'attention_mask']\n","if processor.pad_token is None:\n","    processor.pad_token = processor.eos_token\n","\n","processor.padding_side = \"right\"\n","processor.truncation_side = \"right\"\n","\n","@torch.no_grad()\n","def inference(input_, model):\n","    model.eval()\n","    inputs = processor(input_,\n","                       return_tensors=\"pt\",\n","                    #    padding=\"max_length\",\n","                    #    truncation=True,\n","                    #    max_length=128\n","                       )\n","    outputs = model.generate(**inputs)\n","    return processor.batch_decode(outputs, skip_special_tokens=True)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"9W9RxgzzFX41","executionInfo":{"status":"ok","timestamp":1721050245389,"user_tz":-540,"elapsed":5505,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"e8113e30-6d3f-40ba-a30b-d39b9910f929"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of BartForCausalLM were not initialized from the model checkpoint at Gunulhona/tb_pretrained_sts and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["#@title Select Vision Model\n","\n","from transformers import AutoProcessor, Owlv2ForObjectDetection, Owlv2VisionConfig\n","import torch\n","\n","base_model_id = \"google/owlv2-base-patch16-ensemble\" # @param [\"google/owlv2-base-patch16-ensemble\", \"\"] {allow-input: true}\n","\n","processor = AutoProcessor.from_pretrained(base_model_id)\n","origin_model = Owlv2ForObjectDetection.from_pretrained(base_model_id)\n","\n","@torch.no_grad()\n","def inference(input_, model):\n","    model.eval()\n","    inputs = processor(input_, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    return outputs"],"metadata":{"cellView":"form","id":"ifokOEXAGJev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantize Model\n"],"metadata":{"id":"3h222ZK1EV0M"}},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 onnx qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import numpy as np\n","import torch\n","import onnx\n","import onnxruntime as ort\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","\n","# Load your original model\n","original_model = origin_model\n","test_in = torch.randint(1000, (1, 128))\n","torch.onnx.export(original_model,\n","                  args={\n","                      \"input_ids\": test_in,\n","                      \"decoder_input_ids\": test_in\n","                      },\n","                  f=\"original_model.pt\")\n","\n","\n","# Quantize the model using ONNX\n","quantized_model = quantize_dynamic(\n","    model_input=\"original_model.pt\",\n","    model_output=\"quantized_model.onnx\",\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","# onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Measure execution time for the quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","for _ in range(10):\n","    preprocessed = processor(\"test 입력 처리 요구\", return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n","    onnx_input= {\n","        #  \"input.1\": np.array([1]).astype(np.int64),\n","         \"input.1\": preprocessed[\"input_ids\"].astype(np.int64),\n","        #  \"input.2\": preprocessed[\"attention_mask\"].astype(np.int64)\n","     }\n","    q_session = ort.InferenceSession(\"quantized_model.onnx\", providers=[\"CPUExecutionProvider\"])\n","    q_session.run(None, input_feed=onnx_input)\n","end_time = time.time()\n","quantized_execution_time = end_time - start_time\n","print(\"Quantized model execution time:\", quantized_execution_time)\n","\n","# Compare execution times\n","speedup = original_execution_time / quantized_execution_time\n","print(\"Speedup:\", speedup)\n"],"metadata":{"id":"zyU0RmgXpreF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721047054975,"user_tz":-540,"elapsed":44481,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"803ead11-120d-4dba-a02b-fb7d500819cc"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]},{"output_type":"stream","name":"stdout","text":["Original model execution time: 6.747868299484253\n","Quantized model execution time: 6.006300687789917\n","Speedup: 1.123464949598986\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 tensorrt qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","\n","# Load your original model\n","original_model = origin_model\n","\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# TensorRT quantization and execution\n","logger = trt.Logger(trt.Logger.INFO)\n","builder = trt.Builder(logger)\n","network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n","parser = trt.OnnxParser(network, logger)\n","\n","with open(\"quantized_model.onnx\", \"rb\") as model:\n","    if not parser.parse(model.read()):\n","        for error in range(parser.num_errors):\n","            print(parser.get_error(error))\n","\n","# Build TensorRT engine\n","engine = builder.build_cuda_engine(network)\n","\n","# Measure execution time for the TensorRT quantized model\n","start_time = time.time()\n","# Run inference with the TensorRT engine\n","for _ in range(10):\n","    preprocessed = processor(\"test 입력 처리 요구\",\n","                             return_tensors=\"np\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=128)\n","    inputs[0].host = preprocessed[\"input_ids\"].astype(np.float32)  # Assuming input_ids is the input tensor name\n","    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n","end_time = time.time()\n","trt_quantized_execution_time = end_time - start_time\n","print(\"TensorRT quantized model execution time:\", trt_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_trt = original_execution_time / trt_quantized_execution_time\n","print(\"Speedup with TensorRT quantization:\", speedup_trt)\n"],"metadata":{"id":"TwhYub_Hp6sT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"309f1392-d963-4509-fd27-08ea223087b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 vllm qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","\n","# Load your original model\n","original_model = ...\n","\n","# Quantize the model using ONNX\n","quantized_model = onnx.quantize.quantize_dynamic(\n","    original_model,\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","\n","# vllm quantization and execution\n","# Initialize vLLM with the quantized model\n","llm = LLM(model=\"quantized_model.onnx\")\n","\n","# Generate text using vLLM\n","prompts = [\"This is a prompt.\"]\n","sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n","start_time = time.time()\n","result = llm.generate(prompts, sampling_params)\n","end_time = time.time()\n","vllm_quantized_execution_time = end_time - start_time\n","print(\"vLLM quantized model execution time:\", vllm_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_vllm = original_execution_time / vllm_quantized_execution_time\n","print(\"Speedup with vLLM quantization:\", speedup_vllm)\n"],"metadata":{"id":"QPrvNnf_qGTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 lmdeploy qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","# 단 lmdeploy는 cli가 아닌 lmdeploy 패키지 import 를 통해 실행하여야 함\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","import lmdeploy\n","\n","# Load your original model\n","original_model = ...\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# lmdeploy quantization\n","from lmdeploy.turbomind import TurboMind\n","turbomind = TurboMind()\n","quantized_model = turbomind.quantize(original_model)\n","\n","# Measure execution time for the lmdeploy quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","# ...\n","end_time = time.time()\n","lmdeploy_quantized_execution_time = end_time - start_time\n","print(\"lmdeploy quantized model execution time:\", lmdeploy_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_lmdeploy = original_execution_time / lmdeploy_quantized_execution_time\n","print(\"Speedup with lmdeploy quantization:\", speedup_lmdeploy)\n"],"metadata":{"id":"VtG4Vcx7qMND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 openvino qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import openvino as ov\n","\n","# Load your original model\n","original_model = ...\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Load the ONNX model\n","core = ov.Core()\n","model = core.read_model(\"path/to/save/model.onnx\")\n","\n","# Quantize the model\n","quantized_model = ov.quantize(model, {}, \"path/to/save/quantized_model.xml\")\n","\n","# Save the quantized model\n","ov.serialize(quantized_model, \"path/to/save/quantized_model.xml\")\n","\n","# Load the quantized model\n","compiled_model_quantized = core.compile_model(\"path/to/save/quantized_model.xml\", \"CPU\")\n","\n","# Measure execution time for the OpenVINO quantized model\n","start_time = time.time()\n","# Run inference with the OpenVINO quantized model\n","# ...\n","end_time = time.time()\n","openvino_quantized_execution_time = end_time - start_time\n","print(\"OpenVINO quantized model execution time:\", openvino_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_openvino = original_execution_time / openvino_quantized_execution_time\n","print(\"Speedup with OpenVINO quantization:\", speedup_openvino)\n"],"metadata":{"id":"Y7SVCYOfqeDn"},"execution_count":null,"outputs":[]}]}