{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1B76NAjB51NbQfhtHvTU7WbNTWISMvAhl","authorship_tag":"ABX9TyMrf27nsZwVfFSidCUiyczT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAphU6E1lcvy","executionInfo":{"status":"ok","timestamp":1721017063752,"user_tz":-540,"elapsed":455,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b1ee3b97-7e52-49ae-d5e7-4abf8923d673","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","onnx\n","tensorrt\n","vllm\n","lmdeploy\n","openvino"]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install -r requirements.txt"],"metadata":{"id":"cATRLnmdnFU3","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quanti"],"metadata":{"id":"3h222ZK1EV0M"}},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 onnx qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","\n","# Load your original model\n","original_model = ...\n","\n","# Quantize the model using ONNX\n","quantized_model = onnx.quantize.quantize_dynamic(\n","    original_model,\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Measure execution time for the quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","# ...\n","end_time = time.time()\n","quantized_execution_time = end_time - start_time\n","print(\"Quantized model execution time:\", quantized_execution_time)\n","\n","# Compare execution times\n","speedup = original_execution_time / quantized_execution_time\n","print(\"Speedup:\", speedup)\n"],"metadata":{"id":"zyU0RmgXpreF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 tensorrt qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","\n","# Load your original model\n","original_model = ...\n","\n","# Quantize the model using ONNX\n","quantized_model = onnx.quantize.quantize_dynamic(\n","    original_model,\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# TensorRT quantization and execution\n","logger = trt.Logger(trt.Logger.INFO)\n","builder = trt.Builder(logger)\n","network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n","parser = trt.OnnxParser(network, logger)\n","\n","with open(\"quantized_model.onnx\", \"rb\") as model:\n","    if not parser.parse(model.read()):\n","        for error in range(parser.num_errors):\n","            print(parser.get_error(error))\n","\n","# Build TensorRT engine\n","engine = builder.build_cuda_engine(network)\n","\n","# Measure execution time for the TensorRT quantized model\n","start_time = time.time()\n","# Run inference with the TensorRT engine\n","# ...\n","end_time = time.time()\n","trt_quantized_execution_time = end_time - start_time\n","print(\"TensorRT quantized model execution time:\", trt_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_trt = original_execution_time / trt_quantized_execution_time\n","print(\"Speedup with TensorRT quantization:\", speedup_trt)\n"],"metadata":{"id":"TwhYub_Hp6sT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 vllm qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","\n","# Load your original model\n","original_model = ...\n","\n","# Quantize the model using ONNX\n","quantized_model = onnx.quantize.quantize_dynamic(\n","    original_model,\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","\n","# vllm quantization and execution\n","# Initialize vLLM with the quantized model\n","llm = LLM(model=\"quantized_model.onnx\")\n","\n","# Generate text using vLLM\n","prompts = [\"This is a prompt.\"]\n","sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n","start_time = time.time()\n","result = llm.generate(prompts, sampling_params)\n","end_time = time.time()\n","vllm_quantized_execution_time = end_time - start_time\n","print(\"vLLM quantized model execution time:\", vllm_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_vllm = original_execution_time / vllm_quantized_execution_time\n","print(\"Speedup with vLLM quantization:\", speedup_vllm)\n"],"metadata":{"id":"QPrvNnf_qGTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 lmdeploy qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","# 단 lmdeploy는 cli가 아닌 lmdeploy 패키지 import 를 통해 실행하여야 함\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","import lmdeploy\n","\n","# Load your original model\n","original_model = ...\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# lmdeploy quantization\n","from lmdeploy.turbomind import TurboMind\n","turbomind = TurboMind()\n","quantized_model = turbomind.quantize(original_model)\n","\n","# Measure execution time for the lmdeploy quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","# ...\n","end_time = time.time()\n","lmdeploy_quantized_execution_time = end_time - start_time\n","print(\"lmdeploy quantized model execution time:\", lmdeploy_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_lmdeploy = original_execution_time / lmdeploy_quantized_execution_time\n","print(\"Speedup with lmdeploy quantization:\", speedup_lmdeploy)\n"],"metadata":{"id":"VtG4Vcx7qMND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 openvino qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import openvino as ov\n","\n","# Load your original model\n","original_model = ...\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","# ...\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Load the ONNX model\n","core = ov.Core()\n","model = core.read_model(\"path/to/save/model.onnx\")\n","\n","# Quantize the model\n","quantized_model = ov.quantize(model, {}, \"path/to/save/quantized_model.xml\")\n","\n","# Save the quantized model\n","ov.serialize(quantized_model, \"path/to/save/quantized_model.xml\")\n","\n","# Load the quantized model\n","compiled_model_quantized = core.compile_model(\"path/to/save/quantized_model.xml\", \"CPU\")\n","\n","# Measure execution time for the OpenVINO quantized model\n","start_time = time.time()\n","# Run inference with the OpenVINO quantized model\n","# ...\n","end_time = time.time()\n","openvino_quantized_execution_time = end_time - start_time\n","print(\"OpenVINO quantized model execution time:\", openvino_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_openvino = original_execution_time / openvino_quantized_execution_time\n","print(\"Speedup with OpenVINO quantization:\", speedup_openvino)\n"],"metadata":{"id":"Y7SVCYOfqeDn"},"execution_count":null,"outputs":[]}]}