{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1B76NAjB51NbQfhtHvTU7WbNTWISMvAhl","authorship_tag":"ABX9TyMBU5MEjCRl1+ZpIA0kn6yc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"21cf7df09eb140749b63bb0ff10d3742":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf4a553195d34d1a84338eb5b77ec359","IPY_MODEL_f8917c6f9fef495693583ae3e5348f83","IPY_MODEL_edc354a3a99c4d8db5e6aa0fbdb22e06"],"layout":"IPY_MODEL_c2ec7836a99746cc817d34c2c458c0eb"}},"bf4a553195d34d1a84338eb5b77ec359":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3838dd11495409bbbb5b3a24a7a3eaa","placeholder":"​","style":"IPY_MODEL_4ce40fdea0484a718a2fec5149b3419d","value":"config.json: 100%"}},"f8917c6f9fef495693583ae3e5348f83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5804d5da6d0499086eb5fca4edeee80","max":654,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b386e60f9f254580af4fd17eb015cb36","value":654}},"edc354a3a99c4d8db5e6aa0fbdb22e06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea9a8db825a6480faa878faa8dfc3757","placeholder":"​","style":"IPY_MODEL_29e7e36149d342138f2794341d503a7b","value":" 654/654 [00:00&lt;00:00, 42.0kB/s]"}},"c2ec7836a99746cc817d34c2c458c0eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3838dd11495409bbbb5b3a24a7a3eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ce40fdea0484a718a2fec5149b3419d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5804d5da6d0499086eb5fca4edeee80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b386e60f9f254580af4fd17eb015cb36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea9a8db825a6480faa878faa8dfc3757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29e7e36149d342138f2794341d503a7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37e59c91a04a4c40a837df71b891beda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b9e95d32d8d4c198fe4cccd9e744038","IPY_MODEL_9a74d3b8307748faa66c83da103bc5d5","IPY_MODEL_c6871bb3080945d2bcd95683a36cbfda"],"layout":"IPY_MODEL_6a2efd5dc54641e5b6b118d43c51913a"}},"7b9e95d32d8d4c198fe4cccd9e744038":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42d850508c7449be85a0173ef4d350d0","placeholder":"​","style":"IPY_MODEL_4cde1773ca9c4798b46b41cba9a8355d","value":"model.safetensors.index.json: 100%"}},"9a74d3b8307748faa66c83da103bc5d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8dc1d227aef45c99fcb1812a0de8790","max":23950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97f5f5e9dc5c4ecfa874d892534c5ab3","value":23950}},"c6871bb3080945d2bcd95683a36cbfda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f9ba032be6141248b7ff2efcca78fa0","placeholder":"​","style":"IPY_MODEL_10f26fae6dce45c2af20fd5175179268","value":" 23.9k/23.9k [00:00&lt;00:00, 1.96MB/s]"}},"6a2efd5dc54641e5b6b118d43c51913a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d850508c7449be85a0173ef4d350d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cde1773ca9c4798b46b41cba9a8355d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8dc1d227aef45c99fcb1812a0de8790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97f5f5e9dc5c4ecfa874d892534c5ab3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f9ba032be6141248b7ff2efcca78fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10f26fae6dce45c2af20fd5175179268":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5d6cea49a764935be8dd6ec34bdf46b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7b192cdc9f841909b5f282f6524afa0","IPY_MODEL_4f4f6e6eeaf543f287ed13b5c096877b","IPY_MODEL_008dfb1d55ef4a018fee8b4b32b0dc42"],"layout":"IPY_MODEL_9d5f8997e90b4222a594a41d345d5db2"}},"f7b192cdc9f841909b5f282f6524afa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbfaa87268974827831ac773c96c7c8f","placeholder":"​","style":"IPY_MODEL_0f5500befe6846dbbaa4ccbe3a873dbd","value":"Downloading shards:  25%"}},"4f4f6e6eeaf543f287ed13b5c096877b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c90cc2222320410da933075c0fc9f42a","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1e435c0824d4581bc7f61bc6051b354","value":1}},"008dfb1d55ef4a018fee8b4b32b0dc42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_108b6a6b318f45fd9b9d990fcf42a40d","placeholder":"​","style":"IPY_MODEL_4b35e45b2bab4c5fb0b60fa5b7be2612","value":" 1/4 [01:30&lt;04:30, 90.17s/it]"}},"9d5f8997e90b4222a594a41d345d5db2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbfaa87268974827831ac773c96c7c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f5500befe6846dbbaa4ccbe3a873dbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c90cc2222320410da933075c0fc9f42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e435c0824d4581bc7f61bc6051b354":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"108b6a6b318f45fd9b9d990fcf42a40d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b35e45b2bab4c5fb0b60fa5b7be2612":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5039bae15a2143df9bb97c0f445cb8ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7fb4bbfc8b24924954f9770bd919802","IPY_MODEL_6200ad8de2e146b981d39990a178715c","IPY_MODEL_0d1ce0a8d1c04080b7165230f11962d7"],"layout":"IPY_MODEL_1c3cda4872de4851980e61c776e64570"}},"a7fb4bbfc8b24924954f9770bd919802":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d2cdb27fa054fe88c7ea3b5a3dd03f6","placeholder":"​","style":"IPY_MODEL_a2e54f767fa94c94816c2ee5f8d2a850","value":"model-00001-of-00004.safetensors: 100%"}},"6200ad8de2e146b981d39990a178715c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c26eca24edcc4cbebcdd48ad92752e2f","max":4976698672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a040c934aef4eb69e02cac781b4535c","value":4976698672}},"0d1ce0a8d1c04080b7165230f11962d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fee8c55898a64e57a5d493942dfb22a4","placeholder":"​","style":"IPY_MODEL_7e3b4b4a3885429a9a1f4666b91efbe7","value":" 4.98G/4.98G [01:29&lt;00:00, 62.3MB/s]"}},"1c3cda4872de4851980e61c776e64570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d2cdb27fa054fe88c7ea3b5a3dd03f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e54f767fa94c94816c2ee5f8d2a850":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c26eca24edcc4cbebcdd48ad92752e2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a040c934aef4eb69e02cac781b4535c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fee8c55898a64e57a5d493942dfb22a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e3b4b4a3885429a9a1f4666b91efbe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41c86dc9764e42198866458248afc766":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90db6db147e74d80953539cf97e75821","IPY_MODEL_682e7a8be97a4eceae0c9cafd8e44059","IPY_MODEL_2df042a1435e42fe83014874d4652770"],"layout":"IPY_MODEL_b014e84c394d41808a847cc58220d80e"}},"90db6db147e74d80953539cf97e75821":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_911c229e111447ba9af801187264b4da","placeholder":"​","style":"IPY_MODEL_c82fcca6bd59402c8f02a7ecf87e4894","value":"model-00002-of-00004.safetensors:  34%"}},"682e7a8be97a4eceae0c9cafd8e44059":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d0d85b788384bc69a8320485ec3e5c9","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bd1988b53594b06ba37c484131868ea","value":1709178880}},"2df042a1435e42fe83014874d4652770":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc01773f6294db392668d13b308676b","placeholder":"​","style":"IPY_MODEL_ebf13447cd47430792ab10d75b5e07ae","value":" 1.71G/5.00G [00:29&lt;01:04, 50.7MB/s]"}},"b014e84c394d41808a847cc58220d80e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"911c229e111447ba9af801187264b4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82fcca6bd59402c8f02a7ecf87e4894":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d0d85b788384bc69a8320485ec3e5c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bd1988b53594b06ba37c484131868ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fc01773f6294db392668d13b308676b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebf13447cd47430792ab10d75b5e07ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"weO225X2EjZ2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAphU6E1lcvy","executionInfo":{"status":"ok","timestamp":1721190876352,"user_tz":-540,"elapsed":460,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"833a0716-97b4-421a-cdc1-82e72581a87c","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","onnx\n","onnxruntime\n","onnx2pytorch\n","tensorrt\n","vllm\n","lmdeploy\n","openvino\n","ipex_llm\n","optimum-intel[extras]\n","ray\n","konlpy"]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!VLLM_TARGET_DEVICE=cpu pip install -r requirements.txt"],"metadata":{"id":"cATRLnmdnFU3","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3-EU5iXPMhF","executionInfo":{"status":"ok","timestamp":1721212306185,"user_tz":-540,"elapsed":166079,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"50cfdd64-098b-47fe-b942-7b625ebbaf87"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","Cannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# Select Model"],"metadata":{"id":"uuZ4dVPUFWP5"}},{"cell_type":"code","source":["#@title Select Language Model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","\n","base_model_id = \"meta-llama/Meta-Llama-3-8B\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M\", \"google/gemma-2b-it\"] {allow-input: true}\n","\n","origin_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True,\n","    # quantization_config=bnb_config\n","    )\n","\n","processor = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    add_special_tokens=True,\n","    trust_remote_code=True)\n","processor.model_input_names=['input_ids', 'attention_mask']\n","if processor.pad_token is None:\n","    processor.pad_token = processor.eos_token\n","\n","processor.padding_side = \"right\"\n","processor.truncation_side = \"right\"\n","\n","@torch.inference_mode()\n","def inference(input_, model):\n","    model.eval()\n","    inputs = processor(input_,\n","                       return_tensors=\"pt\",\n","                       padding=\"max_length\",\n","                       truncation=True,\n","                       max_length=128)\n","    outputs = model.generate(**inputs,\n","                             max_new_tokens=50)\n","    return processor.batch_decode(outputs, skip_special_tokens=True)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["21cf7df09eb140749b63bb0ff10d3742","bf4a553195d34d1a84338eb5b77ec359","f8917c6f9fef495693583ae3e5348f83","edc354a3a99c4d8db5e6aa0fbdb22e06","c2ec7836a99746cc817d34c2c458c0eb","b3838dd11495409bbbb5b3a24a7a3eaa","4ce40fdea0484a718a2fec5149b3419d","b5804d5da6d0499086eb5fca4edeee80","b386e60f9f254580af4fd17eb015cb36","ea9a8db825a6480faa878faa8dfc3757","29e7e36149d342138f2794341d503a7b","37e59c91a04a4c40a837df71b891beda","7b9e95d32d8d4c198fe4cccd9e744038","9a74d3b8307748faa66c83da103bc5d5","c6871bb3080945d2bcd95683a36cbfda","6a2efd5dc54641e5b6b118d43c51913a","42d850508c7449be85a0173ef4d350d0","4cde1773ca9c4798b46b41cba9a8355d","f8dc1d227aef45c99fcb1812a0de8790","97f5f5e9dc5c4ecfa874d892534c5ab3","4f9ba032be6141248b7ff2efcca78fa0","10f26fae6dce45c2af20fd5175179268","f5d6cea49a764935be8dd6ec34bdf46b","f7b192cdc9f841909b5f282f6524afa0","4f4f6e6eeaf543f287ed13b5c096877b","008dfb1d55ef4a018fee8b4b32b0dc42","9d5f8997e90b4222a594a41d345d5db2","cbfaa87268974827831ac773c96c7c8f","0f5500befe6846dbbaa4ccbe3a873dbd","c90cc2222320410da933075c0fc9f42a","e1e435c0824d4581bc7f61bc6051b354","108b6a6b318f45fd9b9d990fcf42a40d","4b35e45b2bab4c5fb0b60fa5b7be2612","5039bae15a2143df9bb97c0f445cb8ab","a7fb4bbfc8b24924954f9770bd919802","6200ad8de2e146b981d39990a178715c","0d1ce0a8d1c04080b7165230f11962d7","1c3cda4872de4851980e61c776e64570","7d2cdb27fa054fe88c7ea3b5a3dd03f6","a2e54f767fa94c94816c2ee5f8d2a850","c26eca24edcc4cbebcdd48ad92752e2f","6a040c934aef4eb69e02cac781b4535c","fee8c55898a64e57a5d493942dfb22a4","7e3b4b4a3885429a9a1f4666b91efbe7","41c86dc9764e42198866458248afc766","90db6db147e74d80953539cf97e75821","682e7a8be97a4eceae0c9cafd8e44059","2df042a1435e42fe83014874d4652770","b014e84c394d41808a847cc58220d80e","911c229e111447ba9af801187264b4da","c82fcca6bd59402c8f02a7ecf87e4894","7d0d85b788384bc69a8320485ec3e5c9","9bd1988b53594b06ba37c484131868ea","8fc01773f6294db392668d13b308676b","ebf13447cd47430792ab10d75b5e07ae"]},"cellView":"form","id":"9W9RxgzzFX41","outputId":"ef0d316f-45e1-466e-dbcf-87dbef8680a1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21cf7df09eb140749b63bb0ff10d3742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e59c91a04a4c40a837df71b891beda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d6cea49a764935be8dd6ec34bdf46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5039bae15a2143df9bb97c0f445cb8ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c86dc9764e42198866458248afc766"}},"metadata":{}}]},{"cell_type":"code","source":["#@title Select Vision Model\n","\n","from transformers import AutoProcessor, Owlv2ForObjectDetection, Owlv2VisionConfig\n","import torch\n","\n","base_model_id = \"google/owlv2-base-patch16-ensemble\" # @param [\"google/owlv2-base-patch16-ensemble\", \"\"] {allow-input: true}\n","\n","processor = AutoProcessor.from_pretrained(base_model_id)\n","origin_model = Owlv2ForObjectDetection.from_pretrained(base_model_id)\n","\n","@torch.inference_mode()\n","def inference(input_, model):\n","    model.eval()\n","    inputs = processor(input_, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    return outputs"],"metadata":{"cellView":"form","id":"ifokOEXAGJev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantize Model\n"],"metadata":{"id":"3h222ZK1EV0M"}},{"cell_type":"code","source":["from ipex_llm.transformers import AutoModelForCausalLM\n","from transformers import AutoTokenizer\n","\n","ipex_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True)\n","\n","outputs = inference(input_=\"test 입력 처리 요구\",\n","                    model=ipex_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6k0sn_rfOD1w","executionInfo":{"status":"ok","timestamp":1721095897349,"user_tz":-540,"elapsed":7477,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"5f046ee3-6434-4d8a-e71f-725221a2770f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BartForCausalLM were not initialized from the model checkpoint at Gunulhona/tb_pretrained_sts and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 onnx qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import numpy as np\n","import torch\n","import onnx\n","import onnxruntime as ort\n","import random\n","import ray\n","from copy import deepcopy\n","import string\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","from optimum.onnxruntime import ORTModelForCausalLM, ORTConfig\n","\n","\n","@ray.remote\n","def make_texts():\n","    return \"\".join(random.sample('가나다라마바사아자차카타파하그느드르므브스으즈츠크트프흐구누두루무부수우주추쿠투푸후', 30))\n","\n","randomtexts = ray.get([ make_texts.remote() for i in range(10) ])\n","\n","\n","# Load your original model\n","original_model = origin_model\n","test_in = torch.randint(1000, (1, 128))\n","torch.onnx.export(original_model,\n","                  args={\n","                      \"input_ids\": test_in,\n","                      \"decoder_input_ids\": test_in\n","                      },\n","                  f=\"original_model.pt\")\n","\n","\n","# Quantize the model using ONNX\n","quantized_model = quantize_dynamic(\n","    model_input=\"original_model.pt\",\n","    model_output=\"quantized_model.onnx\",\n","    weight_type=onnx.TensorProto.INT8\n",")\n","\n","# Save the quantized model\n","# onnx.save(quantized_model, \"quantized_model.onnx\")\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","result = []\n","for t in randomtexts:\n","    outputs = inference(input_=t,\n","                        model=origin_model)\n","    result.append(outputs)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","print(result[0])\n","\n","# Measure execution time for the quantized model\n","start_time = time.time()\n","# Run inference with the quantized model\n","q_session = ort.InferenceSession(\"quantized_model.onnx\", providers=[\"CPUExecutionProvider\"])\n","\n","result = []\n","for t in randomtexts:\n","    preprocessed = processor(t, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n","    onnx_input= {\n","        #  \"input.1\": np.array([1]).astype(np.int64),\n","         \"input.1\": preprocessed[\"input_ids\"].astype(np.int64),\n","        #  \"input.2\": preprocessed[\"attention_mask\"].astype(np.int64)\n","     }\n","    result.append(q_session.run(None, input_feed=onnx_input)[0])\n","end_time = time.time()\n","quantized_execution_time = end_time - start_time\n","print(\"Quantized model execution time:\", quantized_execution_time)\n","print(result[0].shape)\n","config = deepcopy(original_model.config)\n","\n","q_model = ORTModelForCausalLM(q_session, config = original_model.config, use_io_binding=False, use_cache=False)\n","\n","@ray.remote\n","def ray_onnx(text):\n","    return inference(input_=text,\n","              model=q_model)\n","\n","start_time = time.time()\n","result = ray.get([ray_onnx.remote(ray.put(t)) for t in randomtexts])\n","end_time = time.time()\n","result = processor.batch_decode(result)\n","print(result[0])\n","\n","ray_quantized_execution_time = end_time - start_time\n","print(\"Ray + Quantized model execution time:\", ray_quantized_execution_time)\n","\n","# Compare execution times\n","speedup = original_execution_time / quantized_execution_time\n","print(\"Speedup:\", speedup)\n","speedup = original_execution_time / ray_quantized_execution_time\n","print(\"Ray Speedup:\", speedup)\n"],"metadata":{"id":"zyU0RmgXpreF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1721199719877,"user_tz":-540,"elapsed":98485,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"8291385d-4b37-4bfe-ae04-3df6e8a69153"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if input_shape[-1] > 1 or self.sliding_window is not None:\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if past_key_values_length > 0:\n","/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py:612: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n","WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original model execution time: 55.35991835594177\n","트마가으즈파우라누프카차자부그수흐쿠드타하츠브나푸투후주아사묠묠묠 하여스티벌Ø묠묠 방향묠묠쪂묠묠ØØØ韆韆Ø韆묠묠韆韆스티벌ØØ묠韆묠韆 고용노동韆韆韆묠 고용노동묠묠 경고韆韆 방향 방향Ø韆스티벌묠\n","Quantized model execution time: 2.5607028007507324\n","(1, 128, 30000)\n"]},{"output_type":"stream","name":"stderr","text":["ORTModelForCausalLM.export_feature is deprecated, and will be removed in optimum 2.0.\n","ORTModelForCausalLM.export_feature is deprecated, and will be removed in optimum 2.0.\n"]},{"output_type":"error","ename":"TypeError","evalue":"Could not serialize the function __main__.ray_onnx:\n=================================================================\nChecking Serializability of <function ray_onnx at 0x7d85d7adbeb0>\n=================================================================\n\u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle 'onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession' object\nDetected 2 global variables. Checking serializability...\n    Serializing 'inference' <function inference at 0x7d85d7a372e0>...\n    Serializing 'q_model' <optimum.onnxruntime.modeling_decoder.ORTModelForCausalLM object at 0x7d85d7a90e50>...\n    \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle 'onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession' object\n        Serializing '_cached_file' <function ORTModel._cached_file at 0x7d8a21830160>...\n        Serializing '_expand_inputs_for_generation' <function GenerationMixin._expand_inputs_for_generation at 0x7d8b025809d0>...\n        Serializing '_generate_regular_names_for_filename' <function ORTModel._generate_regular_names_for_filename at 0x7d8a218177f0>...\n        Serializing '_reorder_cache' <function ORTModelForCausalLM._reorder_cache at 0x7d8a21831000>...\n        Serializing 'infer_onnx_filename' <function ORTModel.infer_onnx_filename at 0x7d8a21817880>...\n        Serializing 'load_model' <function ORTModel.load_model at 0x7d8a218176d0>...\n        Serializing '_abc_impl' <_abc._abc_data object at 0x7d8a21834780>...\n        \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_abc._abc_data' object\n        WARNING: Did not find non-serializable object in <_abc._abc_data object at 0x7d8a21834780>. This may be an oversight.\n=================================================================\nVariable: \n\n\t\u001b[1mFailTuple(_abc_impl [obj=<_abc._abc_data object at 0x7d8a21834780>, parent=<optimum.onnxruntime.modeling_decoder.ORTModelForCausalLM object at 0x7d85d7a90e50>])\u001b[0m\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \nConsider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n=================================================================\nCheck https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\nIf you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n=================================================================\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36mpickle_dumps\u001b[0;34m(obj, error_msg)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot pickle 'onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession' object","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-7d2e03c6ff31>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mray_onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandomtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-7d2e03c6ff31>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mray_onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandomtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/remote_function.py\u001b[0m in \u001b[0;36m_remote_proxy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_remote_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_remote_proxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/util/tracing/tracing_helper.py\u001b[0m in \u001b[0;36m_invocation_remote_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;34m\"_ray_trace_ctx\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m\"_ray_trace_ctx\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/remote_function.py\u001b[0m in \u001b[0;36m_remote\u001b[0;34m(self, args, kwargs, **task_options)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# first driver. This is an argument for repickling the function,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# which we do here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             self._pickled_function = pickle_dumps(\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;34mf\"Could not serialize the function {self._function_descriptor.repr}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36mpickle_dumps\u001b[0;34m(obj, error_msg)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0minspect_serializability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{error_msg}:\\n{sio.getvalue()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Could not serialize the function __main__.ray_onnx:\n=================================================================\nChecking Serializability of <function ray_onnx at 0x7d85d7adbeb0>\n=================================================================\n\u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle 'onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession' object\nDetected 2 global variables. Checking serializability...\n    Serializing 'inference' <function inference at 0x7d85d7a372e0>...\n    Serializing 'q_model' <optimum.onnxruntime.modeling_decoder.ORTModelForCausalLM object at 0x7d85d7a90e50>...\n    \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle 'onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession' object\n        Serializing '_cached_file' <function ORTModel._cached_file at 0x7d8a21830160>...\n        Serializing '_expand_inputs_for_generation' <function GenerationMixin._expand_inputs_for_generation at 0x7d8b025809d0>...\n        Serializing '_generate_regular_names_for_filename' <function ORTModel._generate_regular_names_for_filename at 0x7d8a218177f0>...\n        Serializing '_reorder_cache' <function ORTModelForCausalLM._reorder_cache at 0x7d8a21831000>...\n        Serializing 'infer_onnx_filename' <function ORTModel.infer_onnx_filename at 0x7d8a21817880>...\n        Serializing 'load_model' <function ORTModel.load_model at 0x7d8a218176d0>...\n        Serializing '_abc_impl' <_abc._abc_data object at 0x7d8a21834780>...\n        \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_abc._abc_data' object\n        WARNING: Did not find non-serializable object in <_abc._abc_data object at 0x7d8a21834780>. This may be an oversight.\n=================================================================\nVariable: \n\n\t\u001b[1mFailTuple(_abc_impl [obj=<_abc._abc_data object at 0x7d8a21834780>, parent=<optimum.onnxruntime.modeling_decoder.ORTModelForCausalLM object at 0x7d85d7a90e50>])\u001b[0m\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \nConsider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n=================================================================\nCheck https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\nIf you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n=================================================================\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 tensorrt qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import ray\n","import tensorrt as trt\n","\n","# Load your original model\n","original_model = origin_model\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# TensorRT quantization and execution\n","logger = trt.Logger(trt.Logger.INFO)\n","builder = trt.Builder(logger)\n","network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n","parser = trt.OnnxParser(network, logger)\n","\n","with open(\"quantized_model.onnx\", \"rb\") as model:\n","    if not parser.parse(model.read()):\n","        for error in range(parser.num_errors):\n","            print(parser.get_error(error))\n","\n","# Build TensorRT engine\n","engine = builder.build_cuda_engine(network)\n","\n","# Measure execution time for the TensorRT quantized model\n","start_time = time.time()\n","# Run inference with the TensorRT engine\n","for _ in range(10):\n","    preprocessed = processor(\"test 입력 처리 요구\",\n","                             return_tensors=\"np\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=128)\n","    inputs[0].host = preprocessed[\"input_ids\"].astype(np.float32)  # Assuming input_ids is the input tensor name\n","    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n","end_time = time.time()\n","trt_quantized_execution_time = end_time - start_time\n","print(\"TensorRT quantized model execution time:\", trt_quantized_execution_time)\n","\n","@ray.remote\n","def ray_inference(text):\n","    preprocessed = processor(text,\n","                            return_tensors=\"np\",\n","                            padding=\"max_length\",\n","                            truncation=True,\n","                            max_length=128)\n","    inputs[0].host = preprocessed[\"input_ids\"].astype(np.float32)  # Assuming input_ids is the input tensor name\n","    return do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n","\n","\n","start_time = time.time()\n","for _ in range(10):\n","    text_id = ray.put(\"test 입력 처리 요구\")\n","    ray.get([ray_inference.remote(text_id)])\n","\n","end_time = time.time()\n","ray_trt_quantized_execution_time = end_time - start_time\n","print(\"Ray + TensorRT quantized model execution time:\", ray_trt_quantized_execution_time)\n","\n","\n","# Compare execution times\n","speedup_trt = original_execution_time / trt_quantized_execution_time\n","speedup_ray = original_execution_time / ray_trt_quantized_execution_time\n","print(\"Speedup with TensorRT quantization:\", speedup_trt)\n","print(\"Speedup with Ray + TensorRT :\", speedup_ray)\n"],"metadata":{"id":"TwhYub_Hp6sT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"60fe690b-3818-4962-c5cc-f811376966cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1364: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 vllm qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import time\n","import torch\n","import onnx\n","import tensorrt as trt\n","from vllm import LLM, SamplingParams\n","\n","# Load your original model\n","original_model = origin_model\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# vllm quantization and execution\n","# Initialize vLLM with the quantized model\n","llm = LLM(model=base_model_id)\n","\n","# Generate text using vLLM\n","prompts = [\"This is a prompt.\"]\n","sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n","start_time = time.time()\n","for _ in range(10):\n","    result = llm.generate(prompts, sampling_params)\n","end_time = time.time()\n","vllm_quantized_execution_time = end_time - start_time\n","print(\"vLLM quantized model execution time:\", vllm_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_vllm = original_execution_time / vllm_quantized_execution_time\n","print(\"Speedup with vLLM quantization:\", speedup_vllm)\n"],"metadata":{"id":"QPrvNnf_qGTZ","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"error","timestamp":1721051258458,"user_tz":-540,"elapsed":11730,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"63c91433-e5c4-487e-8f74-202a90f33b79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original model execution time: 9.05189323425293\n","INFO 07-15 13:47:37 config.py:1350] Downcasting torch.float32 to torch.float16.\n","INFO 07-15 13:47:37 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='Gunulhona/tb_pretrained_sts', speculative_config=None, tokenizer='Gunulhona/tb_pretrained_sts', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1026, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Gunulhona/tb_pretrained_sts, use_v2_block_manager=False, enable_prefix_caching=False)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-3aaf0a6a1928>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# vllm quantization and execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Initialize vLLM with the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Generate text using vLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         )\n\u001b[0;32m--> 149\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    150\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    415\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    241\u001b[0m             self.model_config)\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         self.model_executor = executor_class(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeculative_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeculative_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \"GPUExecutor only supports single GPU.\")\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_create_worker\u001b[0;34m(self, local_rank, rank, distributed_init_method)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mworker_class_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworker_class_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         )\n\u001b[0;32m---> 68\u001b[0;31m         wrapper.init_worker(**self._get_worker_kwargs(local_rank, rank,\n\u001b[0m\u001b[1;32m     69\u001b[0m                                                       distributed_init_method))\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\u001b[0m in \u001b[0;36minit_worker\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_module_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mworker_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, local_rank, rank, distributed_init_method, lora_config, multimodal_config, speculative_config, is_driver_worker, model_runner_cls)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mModelRunnerClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingModelRunner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mparallel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, lora_config, kv_cache_dtype, is_driver_worker, multimodal_config, return_hidden_states)\u001b[0m\n\u001b[1;32m    215\u001b[0m         num_attn_heads = self.model_config.get_num_attention_heads(\n\u001b[1;32m    216\u001b[0m             self.parallel_config)\n\u001b[0;32m--> 217\u001b[0;31m         self.attn_backend = get_attn_backend(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mnum_attn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\u001b[0m in \u001b[0;36mget_attn_backend\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size, is_blocksparse)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBlocksparseFlashAttentionBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\n\u001b[0m\u001b[1;32m     46\u001b[0m                                 \u001b[0msliding_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                 block_size)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\u001b[0m in \u001b[0;36mwhich_attn_to_use\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# FlashAttn in NVIDIA GPUs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mselected_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_Backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLASH_ATTN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Volta and Turing NVIDIA GPUs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \"\"\"\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 lmdeploy qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","# 단 lmdeploy는 cli가 아닌 lmdeploy 패키지 import 를 통해 실행하여야 함\n","\n","import time\n","import torch\n","import onnx\n","import lmdeploy\n","from onnx2pytorch import ConvertModel\n","from transformers import AutoConfig, AutoModelForCausalLM\n","from lmdeploy import turbomind as tm\n","from lmdeploy import pipeline, TurbomindEngineConfig, ChatTemplateConfig\n","\n","# Load your original model\n","original_model = origin_model\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# lmdeploy quantization\n","onnx_model = onnx.load(\"quantized_model.onnx\")\n","\n","\n","pytorch_model = ConvertModel(onnx_model)\n","config = AutoConfig.from_pretrained(\"bart\")  # 적절한 모델 구성으로 변경하세요\n","config.architectures = [\"BartModel\"]  # 모델 아키텍처에 맞게 조정하세요\n","\n","hf_model = AutoModelForCausalLM.from_config(config)\n","hf_model.load_state_dict(pytorch_model.state_dict())\n","hf_model.save_pretrained(\"onnx_hf_model\")\n","\n","tm_model = tm.TurboMind(model_path=\"onnx_hf_model\")\n","\n","tm.quantize(\n","    base_model_id,\n","    \"lmdeploy_quantized.tm\",\n","    w_bits=4, w_group_size=128)\n","quantized_tm_model = tm.TurboMind(model_path=quantized_model_path)\n","chat = quantized_tm_model.create_chat_session()\n","\n","# lmdeploy_pipe = pipeline(\n","#     base_model_id,\n","#     backend_config=TurbomindEngineConfig(model_format='hf', tp=4),\n","#     chat_template_config=ChatTemplateConfig(model_name='llama2'))\n","\n","# Measure execution time for the lmdeploy quantized model\n","# Run inference with the quantized model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    response, _ = quantized_chat.chat(\"This is a prompt.\")\n","end_time = time.time()\n","lmdeploy_quantized_execution_time = end_time - start_time\n","print(\"lmdeploy quantized model execution time:\", lmdeploy_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_lmdeploy = original_execution_time / lmdeploy_quantized_execution_time\n","print(\"Speedup with lmdeploy quantization:\", speedup_lmdeploy)\n"],"metadata":{"id":"VtG4Vcx7qMND","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"error","timestamp":1721052471767,"user_tz":-540,"elapsed":8081,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b2e71528-b093-42df-e716-74dd28ce674f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original model execution time: 7.303006410598755\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"Conversion not implemented for op_type=DequantizeLinear.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-8d69c0518f5d>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpytorch_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bart\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 적절한 모델 구성으로 변경하세요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"BartModel\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 모델 아키텍처에 맞게 조정하세요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnx2pytorch/convert/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, onnx_model, batch_dim, experimental, debug, enable_pruning)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Create mapping from node (identified by first output) to submodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         for op_id, op_name, op in convert_operations(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnx2pytorch/convert/operations.py\u001b[0m in \u001b[0;36mconvert_operations\u001b[0;34m(onnx_graph, opset_version, batch_dim, enable_pruning)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m    280\u001b[0m                     \u001b[0;34m\"Conversion not implemented for op_type={}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 )\n","\u001b[0;31mNotImplementedError\u001b[0m: Conversion not implemented for op_type=DequantizeLinear."]}]},{"cell_type":"code","source":["# prompt: 기존 모델 실행 시간과 openvino qunatization 실행 및 quantized 모델의 실행 시간 비교 하는 코드\n","\n","import numpy as np\n","import openvino as ov\n","from optimum.intel import OVModelForCausalLM\n","\n","# Load your original model\n","original_model = origin_model\n","\n","# Measure execution time for the original model\n","start_time = time.time()\n","# Run inference with the original model\n","for _ in range(10):\n","    outputs = inference(input_=\"test 입력 처리 요구\",\n","                        model=origin_model)\n","end_time = time.time()\n","original_execution_time = end_time - start_time\n","print(\"Original model execution time:\", original_execution_time)\n","\n","# Load the ONNX model\n","core = ov.Core()\n","model = core.read_model(\"quantized_model.onnx\")\n","model.reshape({model_input.any_name: ov.PartialShape([1, '?']) for model_input in model.inputs})\n","\n","# Quantize the model\n","tput = {'PERFORMANCE_HINT': 'THROUGHPUT'}\n","compiled_model = core.compile_model(model, 'CPU', tput)\n","ireqs = ov.AsyncInferQueue(compiled_model)\n","\n","ov_model = OVModelForCausalLM.from_pretrained(base_model_id, export=True)\n","\n","# Save the quantized model\n","# ov.serialize(quantized_model, \"quantized_model.xml\")\n","\n","# Load the quantized model\n","\n","# Measure execution time for the OpenVINO quantized model\n","start_time = time.time()\n","# Run inference with the OpenVINO quantized model\n","for _ in range(10):\n","    preprocessed = processor(\"test 입력 처리 요구\",\n","                             return_tensors=\"np\",\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=128)\n","    onnx_input= {\n","        #  \"input.1\": np.array([1]).astype(np.int64),\n","         \"input.1\": preprocessed[\"input_ids\"].astype(np.int64),\n","        #  \"input.2\": preprocessed[\"attention_mask\"].astype(np.int64)\n","     }\n","    ireqs.start_async(onnx_input)\n","    # result = inference(input_=\"test 입력 처리 요구\",\n","    #                    model=ov_model)\n","ireqs.wait_all()\n","end_time = time.time()\n","openvino_quantized_execution_time = end_time - start_time\n","print(\"OpenVINO quantized model execution time:\", openvino_quantized_execution_time)\n","\n","# Compare execution times\n","speedup_openvino = original_execution_time / openvino_quantized_execution_time\n","print(\"Speedup with OpenVINO quantization:\", speedup_openvino)\n"],"metadata":{"id":"Y7SVCYOfqeDn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721056022303,"user_tz":-540,"elapsed":87388,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"79e78289-5045-4d33-cadb-124888583fbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model execution time: 40.269423484802246\n"]},{"output_type":"stream","name":"stderr","text":["Framework not specified. Using pt to export the model.\n","Some weights of BartForCausalLM were not initialized from the model checkpoint at Gunulhona/tb_pretrained_sts and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_eos_token_id': 2}\n","Using framework PyTorch: 2.3.0+cu121\n","Overriding 1 configuration item(s)\n","\t- use_cache -> True\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if past_key_values_length > 0:\n","/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py:247: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n","/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py:254: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n","/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py:286: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_eos_token_id': 2}\n","Compiling the model to CPU ...\n"]},{"output_type":"stream","name":"stdout","text":["OpenVINO quantized model execution time: 3.899169921875\n","Speedup with OpenVINO quantization: 10.327691352686118\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Vip1W9ak4T9p"},"execution_count":null,"outputs":[]}]}