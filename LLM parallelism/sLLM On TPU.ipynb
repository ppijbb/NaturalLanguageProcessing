{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1FdWRxC-t_cy7vXm4pCUgnVmc30RiEcY5","authorship_tag":"ABX9TyN5axtoE/Mzpg8gM55Nv6Ak"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXQHi-cwGmh5","executionInfo":{"status":"ok","timestamp":1721009029067,"user_tz":-540,"elapsed":434,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"fec9127a-25f8-43e0-9dcb-c4d697452381"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["%%writefile requirements.txt\n","lightning\n","peft\n","trl\n","accelerate\n","fire\n","optimum\n","jax"]},{"cell_type":"code","source":["%%capture\n","!pip install -r requirements.txt"],"metadata":{"id":"axGJblGGHt28","executionInfo":{"status":"ok","timestamp":1721009046020,"user_tz":-540,"elapsed":16647,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import jax.tools.colab_tpu\n","import jax\n","jax.tools.colab_tpu.setup_tpu()\n","jax.local_devices()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWyeOBMIHvuh","executionInfo":{"status":"ok","timestamp":1721009670956,"user_tz":-540,"elapsed":320,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"6e6801c7-fb8d-4c84-a79a-0622127fb007"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["%%writefile data_module.py\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from transformers import GPT2Tokenizer\n","import lightning.pytorch as pl\n","\n","\n","class WikiTextDataModule(pl.LightningDataModule):\n","    def __init__(self, tokenizer_name='gpt2', max_length=512, batch_size=32):\n","        super().__init__()\n","        self.tokenizer_name = tokenizer_name\n","        self.max_length = max_length\n","        self.batch_size = batch_size\n","        self.tokenizer = None\n","\n","    def prepare_data(self):\n","        # 데이터셋 다운로드 (이 메소드는 한 번만 실행됨)\n","        load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n","        GPT2Tokenizer.from_pretrained(self.tokenizer_name)\n","\n","    def setup(self, stage=None):\n","        # 데이터셋 로드 및 전처리\n","        self.dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n","        self.tokenizer = GPT2Tokenizer.from_pretrained(self.tokenizer_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        def tokenize_function(examples):\n","            return self.tokenizer(examples[\"text\"], truncation=True, max_length=self.max_length, padding=\"max_length\")\n","\n","        self.tokenized_datasets = self.dataset.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=4,\n","            remove_columns=[\"text\"],\n","        )\n","\n","        # 데이터셋을 PyTorch 텐서로 변환\n","        self.tokenized_datasets.set_format(\"torch\")\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.tokenized_datasets[\"train\"],\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=4,\n","            pin_memory=True\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.tokenized_datasets[\"validation\"],\n","            batch_size=self.batch_size,\n","            num_workers=4,\n","            pin_memory=True\n","        )\n","\n","    def test_dataloader(self):\n","        return DataLoader(\n","            self.tokenized_datasets[\"test\"],\n","            batch_size=self.batch_size,\n","            num_workers=4,\n","            pin_memory=True\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUkwMOJ4JF2l","executionInfo":{"status":"ok","timestamp":1721009735255,"user_tz":-540,"elapsed":284,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"bc98d18a-efc0-445d-d7c3-4d4a7aafd347"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing data_module.py\n"]}]},{"cell_type":"code","source":["%%writefile llm_model.py\n","import os\n","import jax\n","import torch\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import lightning.pytorch as pl\n","from transformers import GPT2Config, GPT2LMHeadModel\n","\n","# XLA 설정\n","os.environ['XLA_USE_BF16'] = \"1\"\n","\n","class ParameterParallelLLM(pl.LightningModule):\n","    def __init__(self, vocab_size=50257, n_layer=12, n_head=12, n_embd=768):\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","        # 모델 설정\n","        config = GPT2Config(\n","            vocab_size=vocab_size,\n","            n_layer=n_layer,\n","            n_head=n_head,\n","            n_embd=n_embd\n","        )\n","        self.model = GPT2LMHeadModel(config)\n","\n","        # 모델 파라미터 병렬화\n","        self.parallelize_model()\n","\n","    def parallelize_model(self):\n","\n","        device_count = xm.xrt_world_size()\n","        layers_per_device = self.hparams.n_layer // device_count\n","\n","        # 임베딩 레이어를 첫 번째 디바이스에 할당\n","        self.model.transformer.wte.to(xm.xla_device(0))\n","        self.model.transformer.wpe.to(xm.xla_device(0))\n","\n","        # 트랜스포머 레이어를 여러 디바이스에 분산\n","        for i, layer in enumerate(self.model.transformer.h):\n","            device_id = (i // layers_per_device) % device_count\n","            layer.to(xm.xla_device(device_id))\n","\n","        # LM 헤드를 마지막 디바이스에 할당\n","        self.model.lm_head.to(xm.xla_device(device_count - 1))\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        # 입력을 첫 번째 디바이스로 이동\n","        input_ids = input_ids.to(xm.xla_device(0))\n","        if attention_mask is not None:\n","            attention_mask = attention_mask.to(xm.xla_device(0))\n","\n","        # 모델 실행\n","        outputs = self.model(input_ids, attention_mask=attention_mask)\n","\n","        # 출력을 마지막 디바이스에서 가져옴\n","        return outputs.to(xm.xla_device(xm.xrt_world_size() - 1))\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask = batch\n","        outputs = self(input_ids, attention_mask)\n","        loss = outputs.loss\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=5e-5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WL9rVhjyHwNJ","executionInfo":{"status":"ok","timestamp":1721009810265,"user_tz":-540,"elapsed":339,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"d11b4cc4-fd24-4d89-9daa-f717b1702e2c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting llm_model.py\n"]}]},{"cell_type":"code","source":["%%writefile tpu_trainer.py\n","from lightning.pytorch.plugins import TPUPrecisionPlugin\n","from data_module import WikiTextDataModule\n","from llm_model import ParameterParallelLLM\n","\n","def main():\n","    # 데이터 모듈 초기화\n","    dm = WikiTextDataModule(tokenizer_name='gpt2', max_length=512, batch_size=32)\n","\n","    # 모델 초기화 (이전에 정의한 ParameterParallelLLM 사용)\n","    model = ParameterParallelLLM(vocab_size=50257, n_layer=12, n_head=12, n_embd=768)\n","\n","    # 트레이너 설정\n","    trainer = pl.Trainer(\n","        max_epochs=10,\n","        accelerator='tpu',\n","        devices=8,  # TPU 코어 수\n","        num_nodes=1,\n","        strategy='ddp',\n","        precision='bf16',\n","        # plugins=[TPUPlugin(device='tpu')]\n","    )\n","\n","    # 학습 실행\n","    trainer.fit(model, dm)\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sb70zXLtJIIz","executionInfo":{"status":"ok","timestamp":1721009810698,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"2eba992f-6b10-4f37-ef34-808dd5b1bbfc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting tpu_trainer.py\n"]}]},{"cell_type":"code","source":["!python tpu_trainer.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nuw3q_HUK7xN","executionInfo":{"status":"ok","timestamp":1721009827830,"user_tz":-540,"elapsed":16725,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"66642d47-745b-462c-d427-865555380a0d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1721009824.614279    6548 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/dist-packages/torch_xla/lib/libtpu.so\n","I0000 00:00:1721009824.614360    6548 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n","I0000 00:00:1721009824.614375    6548 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n","E0715 02:17:04.720415604    7621 oauth2_credentials.cc:176]            Call to http server ended with error 404 [].\n","Traceback (most recent call last):\n","  File \"/content/tpu_trainer.py\", line 27, in <module>\n","    main()\n","  File \"/content/tpu_trainer.py\", line 10, in main\n","    model = ParameterParallelLLM(vocab_size=50257, n_layer=12, n_head=12, n_embd=768)\n","  File \"/content/llm_model.py\", line 27, in __init__\n","    self.parallelize_model()\n","  File \"/content/llm_model.py\", line 31, in parallelize_model\n","    device_count = xm.xrt_world_size()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/core/xla_model.py\", line 132, in xrt_world_size\n","    return runtime.world_size()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py\", line 95, in wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py\", line 150, in world_size\n","    if torch_xla._XLAC._xla_get_replication_devices_count() == 0:\n","RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: ioctl failed; [0000:00:04.0 PE0 C0 MC-1 TN0] Failed to set number of simple DMA addresses\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6s2JFPY6Last"},"execution_count":null,"outputs":[]}]}