{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1B76NAjB51NbQfhtHvTU7WbNTWISMvAhl","authorship_tag":"ABX9TyPIO5Ln83hiBC+tJ7mki3H2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAphU6E1lcvy","executionInfo":{"status":"ok","timestamp":1721017063752,"user_tz":-540,"elapsed":455,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b1ee3b97-7e52-49ae-d5e7-4abf8923d673"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["%%writefile requirements.txt\n","onnx\n","tensorrt\n","vllm\n","lmdeploy\n","openvino"]},{"cell_type":"code","source":["%%capture\n","!pip install -r requirements.txt"],"metadata":{"id":"cATRLnmdnFU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: onnx 로 하는  quantization 코드\n","\n","import onnx\n","\n","# Load your ONNX model\n","model = onnx.load(\"path/to/your/model.onnx\")\n","\n","# Quantize the model\n","quantized_model = onnx.quantize(model, quantization_mode=onnx.QuantizationMode.IntegerOps)\n","\n","# Save the quantized model\n","onnx.save(quantized_model, \"path/to/save/quantized_model.onnx\")\n"],"metadata":{"id":"GwI7dluanHpm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: tensorrt 로 하는 quantization 코드\n","\n","import tensorrt as trt\n","import torch\n","\n","# Load your PyTorch model\n","model = torch.load(\"path/to/your/model.pth\")\n","\n","# Convert the model to ONNX\n","dummy_input = torch.randn(1, 3, 224, 224)\n","torch.onnx.export(model, dummy_input, \"path/to/save/model.onnx\", verbose=True)\n","\n","# Load the ONNX model\n","TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n","EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n","with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n","    with open(\"path/to/save/model.onnx\", 'rb') as model:\n","        parser.parse(model.read())\n","\n","    # Quantize the model\n","    builder.int8_mode = True\n","    builder.int8_calibrator = MyCalibrator()  # Replace with your own calibrator\n","\n","    # Build the TensorRT engine\n","    engine = builder.build_cuda_engine(network)\n","\n","    # Serialize the engine to a file\n","    with open(\"path/to/save/engine.trt\", \"wb\") as f:\n","        f.write(engine.serialize())\n"],"metadata":{"id":"jR_wpUY7nZyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: vllm 으로 하는 quantization 코드\n","\n","from vllm import LLM\n","from vllm.quantization import quantize_model\n","\n","# Load the original model\n","llm = LLM(model=\"path/to/your/model\")\n","\n","# Quantize the model\n","quantized_model_dir = \"path/to/save/quantized_model\"\n","quantize_model(\n","  model=llm.model,\n","  output_dir=quantized_model_dir,\n","  quantization_config={\n","    \"bits\": 4,  # Number of bits for quantization\n","    \"group_size\": -1,  # Group size for quantization\n","    \"dtype\": \"int4\",  # Data type for quantization\n","  },\n",")\n","\n","# Load the quantized model\n","quantized_llm = LLM(model=quantized_model_dir)\n"],"metadata":{"id":"rSGm4FCHngky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: lmdeploy 로 하는 qunatization 코드\n","\n","# Quantize the model using lmdeploy\n","!lmdeploy quantize \\\n","  --model_path path/to/your/model \\\n","  --output_path path/to/save/quantized_model \\\n","  --quantization_config '{\"bits\": 4, \"group_size\": -1, \"dtype\": \"int4\"}'\n"],"metadata":{"id":"1RE_kRuTnj4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: openvino 로 하는 quantization 코드\n","import openvino as ov\n","\n","# Load the ONNX model\n","core = ov.Core()\n","model = core.read_model(\"path/to/save/model.onnx\")\n","\n","# Quantize the model\n","quantized_model = ov.quantize(model, {}, \"path/to/save/quantized_model.xml\")\n","\n","# Save the quantized model\n","ov.serialize(quantized_model, \"path/to/save/quantized_model.xml\")\n"],"metadata":{"id":"cGtR99gMnpvZ"},"execution_count":null,"outputs":[]}]}