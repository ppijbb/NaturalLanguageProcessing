{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ubuntu/kevin.jung/colossal/callback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/colossal/callback.py\n",
    "import psutil\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "\n",
    "def print_rank_0(*args, **kwargs):\n",
    "    if dist.get_rank() == 0:\n",
    "        print(*args, **kwargs)\n",
    "    dist.barrier()\n",
    "\n",
    "\n",
    "def get_cpu_mem():\n",
    "    return psutil.Process().memory_info().rss\n",
    "\n",
    "\n",
    "class MemoryMonitor(Callback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.max_cpu_mem = 0\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx) -> None:\n",
    "        self.max_cpu_mem = max(get_cpu_mem(), self.max_cpu_mem)\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module) -> None:\n",
    "        max_cuda_mem = torch.cuda.max_memory_allocated()\n",
    "        cuda_mem = torch.cuda.memory_allocated()\n",
    "        print_rank_0(f'CPU memory before training: {get_cpu_mem()/1024**2:.3f} MB')\n",
    "        print_rank_0(f'CUDA memory before training: {cuda_mem/1024**2:.3f} MB')\n",
    "        print_rank_0(f'Max CUDA memory before training: {max_cuda_mem/1024**2:.3f} MB')\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module) -> None:\n",
    "        max_cuda_mem = torch.cuda.max_memory_allocated()\n",
    "        print_rank_0(f'Max CPU memory: {self.max_cpu_mem/1024**2:.3f} MB')\n",
    "        print_rank_0(f'Max CUDA memory: {max_cuda_mem/1024**2:.3f} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ubuntu/kevin.jung/colossal/data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/colossal/data.py\n",
    "import torch\n",
    "\n",
    "__all__ = ['RandomDataloader']\n",
    "\n",
    "\n",
    "def get_data(batch_size, seq_len, vocab_size):\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "class RandomDataloader:\n",
    "    def __init__(self, n_steps: int, batch_size: int, seq_len: int = 1024, vocab_size: int = 50257) -> None:\n",
    "        self.n_steps = n_steps\n",
    "        self.cur_step = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.cur_step = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.cur_step >= self.n_steps:\n",
    "            raise StopIteration\n",
    "        self.cur_step += 1\n",
    "        return get_data(self.batch_size, self.seq_len, self.vocab_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_steps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ubuntu/kevin.jung/colossal/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/colossal/model.py\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2PreTrainedModel\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from colossalai.utils import colo_set_process_memory_fraction\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
    "from torch.optim import Adam, Optimizer\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable\n",
    "from contextlib import contextmanager\n",
    "__all__ = ['GPTLitModule', 'get_optimizer']\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def no_init_weights():\n",
    "    def dummy_fn(*args):\n",
    "        return\n",
    "    try:\n",
    "        old_init_weights = GPT2PreTrainedModel._init_weights\n",
    "        GPT2PreTrainedModel._init_weights = dummy_fn\n",
    "        yield\n",
    "    finally:\n",
    "        GPT2PreTrainedModel._init_weights = old_init_weights\n",
    "\n",
    "\n",
    "class GPTLMModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_layers=12, num_attention_heads=12, max_seq_len=1024, vocab_size=50257, checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.checkpoint = checkpoint\n",
    "        with no_init_weights():\n",
    "            self.model = GPT2LMHeadModel(GPT2Config(n_embd=hidden_size, n_layer=num_layers,\n",
    "                                                    n_head=num_attention_heads, n_positions=max_seq_len, n_ctx=max_seq_len, vocab_size=vocab_size))\n",
    "        if checkpoint:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Only return lm_logits\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=not self.checkpoint)[0]\n",
    "\n",
    "\n",
    "def gpt2_tiny(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=128, num_layers=4, num_attention_heads=4, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_small(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=768, num_layers=12, num_attention_heads=12, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_medium(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=1024, num_layers=24, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_large(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=1280, num_layers=36, num_attention_heads=20, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_xl(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=1600, num_layers=48, num_attention_heads=25, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_2B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=2048, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_3B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=2304, num_layers=48, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_4B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=2304, num_layers=64, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_6B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=4096, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_8B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=3072, num_layers=72, num_attention_heads=24, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_12B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=4096, num_layers=60, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_15B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=4096, num_layers=78, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_18B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=4096, num_layers=90, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_20B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=25, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_24B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_28B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=35, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_32B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_36B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=45, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_40B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=50, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt2_45B(checkpoint=True):\n",
    "    return GPTLMModel(hidden_size=8192, num_layers=56, num_attention_heads=16, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def gpt3(checkpoint=True):\n",
    "    return GPTLMModel(max_seq_len=2048, hidden_size=12288, num_layers=96, num_attention_heads=96, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def get_gpt_model(model_name: str, checkpoint: bool = True) -> nn.Module:\n",
    "    model_map = {\n",
    "        'gpt2_tiny': gpt2_tiny,\n",
    "        'gpt2_small': gpt2_small,\n",
    "        'gpt2_medium': gpt2_medium,\n",
    "        'gpt2_large': gpt2_large,\n",
    "        'gpt2_xl': gpt2_xl,\n",
    "        'gpt2_2B': gpt2_2B,\n",
    "        'gpt2_3B': gpt2_3B,\n",
    "        'gpt2_4B': gpt2_4B,\n",
    "        'gpt2_6B': gpt2_6B,\n",
    "        'gpt2_8B': gpt2_8B,\n",
    "        'gpt2_12B': gpt2_12B,\n",
    "        'gpt2_15B': gpt2_15B,\n",
    "        'gpt2_18B': gpt2_18B,\n",
    "        'gpt2_20B': gpt2_20B,\n",
    "        'gpt2_24B': gpt2_24B,\n",
    "        'gpt2_28B': gpt2_28B,\n",
    "        'gpt2_32B': gpt2_32B,\n",
    "        'gpt2_36B': gpt2_36B,\n",
    "        'gpt2_40B': gpt2_40B,\n",
    "        'gpt2_45B': gpt2_45B,\n",
    "        'gpt3': gpt3,\n",
    "    }\n",
    "    assert model_name in model_map\n",
    "    return model_map[model_name](checkpoint)\n",
    "\n",
    "\n",
    "class GPTLMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        return self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "\n",
    "def get_optimizer(strategy: str, **kwargs) -> Callable[[Iterable], Optimizer]:\n",
    "    assert strategy in ('ddp', 'deepspeed', 'colossal')\n",
    "    if strategy == 'ddp':\n",
    "        opt_cls = Adam\n",
    "    elif strategy == 'deepspeed':\n",
    "        offload = kwargs.pop('offload')\n",
    "        if offload:\n",
    "            opt_cls = DeepSpeedCPUAdam\n",
    "        else:\n",
    "            opt_cls = FusedAdam\n",
    "    else:\n",
    "        opt_cls = HybridAdam\n",
    "    return partial(opt_cls, **kwargs)\n",
    "\n",
    "\n",
    "class GPTLitModule(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, optimizer_init_fn: Callable[[Iterable], Optimizer],\n",
    "                 checkpoint: bool = True, cuda_mem_fraction: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.optimizer_init_fn = optimizer_init_fn\n",
    "        self.checkpoint = checkpoint\n",
    "        self.criterion = GPTLMLoss()\n",
    "        self.cuda_mem_fraction = cuda_mem_fraction\n",
    "\n",
    "    def configure_sharded_model(self) -> None:\n",
    "        self.model = get_gpt_model(self.model_name, self.checkpoint)\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint) -> None:\n",
    "        if not hasattr(self, 'model'):\n",
    "            self.configure_sharded_model()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_init_fn(self.model.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask = batch\n",
    "        logits = self.model(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, input_ids)\n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        if self.cuda_mem_fraction < 1.0:\n",
    "            colo_set_process_memory_fraction(self.cuda_mem_fraction)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ubuntu/kevin.jung/colossal/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/colossal/train.py\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "from data import RandomDataloader\n",
    "from model import GPTLitModule, get_optimizer\n",
    "from callback import MemoryMonitor\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from pytorch_lightning.strategies.deepspeed import DeepSpeedStrategy\n",
    "from pytorch_lightning.strategies.colossalai import ColossalAIStrategy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--tqdm_rate', type=int, default=2000)\n",
    "    parser.add_argument('--epochs', type=int, default=2)\n",
    "    parser.add_argument('--steps_per_epoch', type=int, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--model', default='gpt2_xl')\n",
    "    parser.add_argument('--np', type=int, default=1)\n",
    "    parser.add_argument('--no_activation_ckpt', action='store_true', default=False)\n",
    "    parser.add_argument('--opt_nvme_offload_frac', type=float, default=0.0)\n",
    "    parser.add_argument('--opt_nvme_offload_dir', default='./offload')\n",
    "    parser.add_argument('--seq_len', type=int, default=1024)\n",
    "    parser.add_argument('--placement_policy', default='cuda')\n",
    "    parser.add_argument('--opt_gpu_margin_rat', type=float, default=0.0)\n",
    "    parser.add_argument('--cuda_mem_frac', type=float, default=1.0)\n",
    "    parser.add_argument('--strategy', default='ddp', choices=['ddp', 'colossal', 'deepspeed'])\n",
    "    parser.add_argument('--offload', action='store_true', default=False)\n",
    "    args = parser.parse_args()\n",
    "    train_dataloader = RandomDataloader(args.steps_per_epoch, args.batch_size, args.seq_len)\n",
    "    optimizer_cfg = {'lr': args.lr}\n",
    "    if args.strategy == 'ddp':\n",
    "        trainer_cfg = {\n",
    "            'accelerator': 'gpu',\n",
    "            'precision': 16,\n",
    "            'strategy': DDPStrategy(static_graph=True)\n",
    "        }\n",
    "    elif args.strategy == 'colossal':\n",
    "        trainer_cfg = {\n",
    "            'accelerator': 'gpu',\n",
    "            'precision': 16,\n",
    "            'strategy': ColossalAIStrategy(\n",
    "                placement_policy=args.placement_policy,\n",
    "                gpu_margin_mem_ratio=args.opt_gpu_margin_rat,\n",
    "                initial_scale=32,\n",
    "                chunk_search_range= 64 * 1024**2,\n",
    "                chunk_search_n_grids= 4096,\n",
    "                min_chunk_size= 32 * 1024**2\n",
    "            )\n",
    "        }\n",
    "        optimizer_cfg['nvme_offload_dir'] = args.opt_nvme_offload_dir\n",
    "        optimizer_cfg['nvme_offload_fraction'] = args.opt_nvme_offload_frac\n",
    "    elif args.strategy == 'deepspeed':\n",
    "        trainer_cfg = {\n",
    "            'accelerator': 'gpu',\n",
    "            'precision': 16,\n",
    "            'strategy': DeepSpeedStrategy(\n",
    "                stage=3,\n",
    "                offload_parameters=args.offload,\n",
    "                offload_optimizer=args.offload,\n",
    "                initial_scale_power=5\n",
    "            )\n",
    "        }\n",
    "        optimizer_cfg['offload'] = args.offload\n",
    "    opt_init_fn = get_optimizer(args.strategy, **optimizer_cfg)\n",
    "    model = GPTLitModule(args.model, opt_init_fn, checkpoint=not args.no_activation_ckpt,\n",
    "                         cuda_mem_fraction=args.cuda_mem_frac)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        devices=args.np,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[\n",
    "            MemoryMonitor(),\n",
    "            TQDMProgressBar(refresh_rate=args.tqdm_rate)\n",
    "        ],\n",
    "        **trainer_cfg\n",
    "    )\n",
    "    trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ubuntu/kevin.jung/colossal/train_start\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ubuntu/kevin.jung/colossal/train_start\n",
    "# export CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "# export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\n",
    "export TOKENIZERS_PARALLELISM=\"0\"\n",
    "\n",
    "\n",
    "EXECUTEFILE=\"colossal/train.py\"    # needs custom trainer path\n",
    "EPOCHS=100                         # type=int       default=2\n",
    "TQDM_RATE=2000                     # type=int       default=2000\n",
    "LEARNING_RATE=5e-5                 # type=float     default=1e-3\n",
    "STRATEGY=\"colossal\"                # type=str       default='ddp'         choices=['ddp', 'colossal', 'deepspeed']\n",
    "ACCELERATOR=\"gpu\"                  # type=str       default=gpu\n",
    "NP=-1                              # type=int       default=1\n",
    "BATCHSIZE=1                        # type=int       default=1\n",
    "MODEL_NAME='gpt2_2B'               # type=str       default='gpt2_xl'     choices=['gpt2_tiny'~'gpt2_xl'~'gpt3']\n",
    "STEPS_PER_EPOCH=4                  # type=int       default=4\n",
    "NAC=false                          # type=bool      default=False         action='store_true'\n",
    "OFFLOAD=false                      # type=bool      default=False         action='store_true'\n",
    "OPT_NVME_OFFLAND_FRAC=0.0          # type=float     default=0.0\n",
    "OPT_NVME_OFFLAND_DIR='/data/opt/'  # type=str       default='/data/opt'\n",
    "SEQ_LEN=1024                       # type=int       default=1024\n",
    "PLACEMENT_POLICY='cuda'            # type=str       defualt='cuda'\n",
    "OPT_GPU_MARGIN_RAT=0.0             # type=float     defualt=0.0\n",
    "CUDA_MEMORY_FRAC=1.0               # type=float     defualt=1.0\n",
    "\n",
    "# >>> conda initialize >>>\n",
    "# !! Contents within this block are managed by 'conda init' !!\n",
    "__conda_setup=\"$('/home/ubuntu/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
    "if [ $? -eq 0 ]; then\n",
    "    eval \"$__conda_setup\"\n",
    "else\n",
    "    if [ -f \"/home/ubuntu/anaconda3/etc/profile.d/conda.sh\" ]; then\n",
    "        . \"/home/ubuntu/anaconda3/etc/profile.d/conda.sh\"\n",
    "    else\n",
    "        export PATH=\"/home/ubuntu/anaconda3/bin:$PATH\"\n",
    "    fi\n",
    "fi\n",
    "unset __conda_setup\n",
    "# <<< conda initialize <<<\n",
    "\n",
    "conda activate nlp_env\n",
    "\n",
    "python $EXECUTEFILE\\\n",
    "  --tqdm_rate $TQDM_RATE\\\n",
    "  --model $MODEL_NAME\\\n",
    "  --epochs $EPOCHS\\\n",
    "  --steps_per_epoch $STEPS_PER_EPOCH\\\n",
    "  --batch_size $BATCHSIZE\\\n",
    "  --seq_len $SEQ_LEN\\\n",
    "  --cuda_mem_frac $CUDA_MEMORY_FRAC\\\n",
    "  --np $NP\\\n",
    "  --strategy $STRATEGY\\\n",
    "  --placement_policy $PLACEMENT_POLICY\\\n",
    "  --lr $LEARNING_RATE \\\n",
    "  --no_activation_ckpt\\\n",
    "  --offload\\\n",
    "  --opt_nvme_offload_frac $OPT_NVME_OFFLAND_FRAC\\\n",
    "  --opt_nvme_offload_dir $OPT_NVME_OFFLAND_DIR\\\n",
    "  --opt_gpu_margin_rat $OPT_GPU_MARGIN_RAT\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('nlp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e90295cf50fa08954a711c27a0fd85ffb404c65ef479e602f0e36fbf546ee202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
