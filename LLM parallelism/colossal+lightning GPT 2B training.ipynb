{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_wD5iJRXbA1l","outputId":"6c800007-82fc-4ea9-b4ea-30fe0acdf43e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /home/ubuntu/kevin.jung/colossal/callback.py\n"]}],"source":["%%writefile /content/colossal/callback.py\n","import psutil\n","import torch\n","import torch.distributed as dist\n","from pytorch_lightning.callbacks import Callback\n","\n","\n","def print_rank_0(*args, **kwargs):\n","    if dist.get_rank() == 0:\n","        print(*args, **kwargs)\n","    dist.barrier()\n","\n","\n","def get_cpu_mem():\n","    return psutil.Process().memory_info().rss\n","\n","\n","class MemoryMonitor(Callback):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.max_cpu_mem = 0\n","\n","    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx) -> None:\n","        self.max_cpu_mem = max(get_cpu_mem(), self.max_cpu_mem)\n","\n","    def on_fit_start(self, trainer, pl_module) -> None:\n","        max_cuda_mem = torch.cuda.max_memory_allocated()\n","        cuda_mem = torch.cuda.memory_allocated()\n","        print_rank_0(f'CPU memory before training: {get_cpu_mem()/1024**2:.3f} MB')\n","        print_rank_0(f'CUDA memory before training: {cuda_mem/1024**2:.3f} MB')\n","        print_rank_0(f'Max CUDA memory before training: {max_cuda_mem/1024**2:.3f} MB')\n","\n","    def on_fit_end(self, trainer, pl_module) -> None:\n","        max_cuda_mem = torch.cuda.max_memory_allocated()\n","        print_rank_0(f'Max CPU memory: {self.max_cpu_mem/1024**2:.3f} MB')\n","        print_rank_0(f'Max CUDA memory: {max_cuda_mem/1024**2:.3f} MB')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CTgK7yjbA1o","outputId":"ba163ed9-0e28-4278-d5c9-562c4d148653"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /home/ubuntu/kevin.jung/colossal/data.py\n"]}],"source":["%%writefile /content/colossal/data.py\n","import torch\n","\n","__all__ = ['RandomDataloader']\n","\n","\n","def get_data(batch_size, seq_len, vocab_size):\n","    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n","    attention_mask = torch.ones_like(input_ids)\n","    return input_ids, attention_mask\n","\n","\n","class RandomDataloader:\n","    def __init__(self, n_steps: int, batch_size: int, seq_len: int = 1024, vocab_size: int = 50257) -> None:\n","        self.n_steps = n_steps\n","        self.cur_step = 0\n","        self.batch_size = batch_size\n","        self.seq_len = seq_len\n","        self.vocab_size = vocab_size\n","\n","    def __iter__(self):\n","        self.cur_step = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.cur_step >= self.n_steps:\n","            raise StopIteration\n","        self.cur_step += 1\n","        return get_data(self.batch_size, self.seq_len, self.vocab_size)\n","\n","    def __len__(self):\n","        return self.n_steps\n","\n","def get_data_s2s(batch_size, seq_len, vocab_size):\n","    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n","    attention_mask = torch.ones_like(input_ids)\n","    decoder_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n","    decoder_attention_mask = torch.ones_like(decoder_input_ids)\n","    labels = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n","    return input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels\n","\n","class RandomS2SDataloader(RandomDataloader):\n","    def __next__(self):\n","        if self.cur_step >= self.n_steps:\n","            raise StopIteration\n","        self.cur_step += 1\n","        return get_data_s2s(self.batch_size, self.seq_len, self.vocab_size)\n",""]},{"cell_type":"code","source":["%%writefile /content/colossal/dataloader.py\n","import os\n","import warnings\n","\n","import numpy as np\n","import pandas as pd\n","import pytorch_lightning as pl\n","import torch\n","\n","from datetime import datetime as dt\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from pytorch_lightning.trainer.supporters import CombinedLoader\n","\n","from copy import deepcopy\n","from scipy.stats import poisson\n","\n","__all__ = [\"LanguageDataModule\"]\n","\n","\n","class Pet_Dataset(Dataset):\n","    def __init__(self,\n","                 max_seq_len:int,\n","                 file_path:str,\n","                 tokenizer_path:str):\n","        self.file_path = file_path\n","        self.data = pd.read_csv(self.file_path).dropna()\n","        self.max_seq_len = max_seq_len\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","        self.masking_start, self.masking_end =self.tokenizer.encode(\"[]\")\n","\n","    def __len__(self):\n","        return self.data.__len__()\n","\n","    def _encode(self, text):\n","        tokens = [self.tokenizer.bos_token] + self.tokenizer.tokenize(text) + [self.tokenizer.eos_token]\n","        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = [1]*len(input_ids)\n","        if len(input_ids) < self.max_seq_len:\n","            while len(input_ids)<self.max_seq_len:\n","                input_ids+=[self.tokenizer.pad_token_id]\n","                attention_mask+=[0]\n","        else:\n","            input_ids = input_ids[:self.max_seq_len-1]+[self.tokenizer.eos_token_id]\n","            attention_mask = attention_mask[:self.max_seq_len]\n","        return input_ids, attention_mask\n","\n","    def _labeling(self, label):\n","        tokens = self.tokenizer.tokenize(label)+[self.tokenizer.eos_token]\n","        label_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        if len(label_ids) < self.max_seq_len:\n","            while len(label_ids)<self.max_seq_len:\n","                label_ids+=[-100]\n","        else:\n","            label_ids = label_ids[:self.max_seq_len-1] + [self.tokenizer.eos_token_id]\n","        return label_ids\n","\n","    def _masking(self, tokens):\n","        masked = tokens\n","        mask_idx=[]\n","        while self.masking_start in masked and self.masking_end in masked:\n","            start_idx = masked.index(self.masking_start)\n","            end_idx = masked.index(self.masking_end)+1\n","            if start_idx < end_idx:\n","                mask_idx +=[[start_idx,end_idx]]\n","                masked[start_idx:end_idx] = [self.tokenizer.mask_token_id]*len(masked[start_idx:end_idx])\n","            else: break\n","        return masked, mask_idx\n","\n","    def _label_masking(self, tokens, idx):\n","        masked = np.array(tokens)\n","        index,position =[],[]\n","        for i in idx:\n","            index+=range(i[0],i[1])\n","        for n,x in enumerate(tokens):\n","            position += [n not in index]\n","        masked[position] = [-100]\n","        return masked#.tolist()\n","\n","\n","class Masked_Dataset(Pet_Dataset):\n","    def _random_masking(self, input, mask=None, ratio=0.15):\n","        if mask is None:\n","            mask = self.tokenizer.mask_token_id\n","        input = np.array(input)\n","        rand = np.random.rand(input.size)\n","        mask_arr = (rand < ratio) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n","        input[mask_arr.nonzero()] = mask\n","        return input\n","\n","    def __getitem__(self, index):\n","        record = self.data.iloc[index]\n","        pattern, label = record[\"pattern\"], record[\"label\"]\n","        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n","        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n","        labels = self._labeling(pattern+label)\n","        encoder_input_ids, mask_idx = self._masking(encoder_input_ids)\n","        encoder_input_ids = self._random_masking(encoder_input_ids)\n","\n","        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n","                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n","                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n","                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n","                \"labels\":np.array(labels,dtype=np.int_)}\n","\n","\n","class Permutation_Dataset(Pet_Dataset):\n","    def _random_rotation(self, input):\n","        input = np.array(input)\n","        start_idx = np.where(input==self.tokenizer.bos_token_id)[0][0]\n","        end_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n","        np.random.shuffle(input[start_idx:end_idx])\n","        return input\n","\n","    def __getitem__(self, index):\n","        record = self.data.iloc[index]\n","        pattern, label = record[\"pattern\"], record[\"label\"]\n","        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n","        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n","        labels = self._labeling(pattern+label)\n","        encoder_input_ids = self._random_rotation(encoder_input_ids)\n","\n","        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n","                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n","                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n","                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n","                \"labels\":np.array(labels,dtype=np.int_)}\n","\n","\n","class Deletion_Dataset(Pet_Dataset):\n","    def _random_deletion(self, input, ratio=0.15):\n","        input = np.array(input)\n","        eos_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n","        rand = np.random.rand(input[:eos_idx].size)\n","        rand = np.append(rand,[1.]*input[eos_idx:].size)\n","        del_arr = (rand < ratio) * (input != self.tokenizer.bos_token_id) * (input != self.tokenizer.eos_token_id)\n","        input = np.delete(input, del_arr.nonzero())\n","        return np.int_(np.append(input,[self.tokenizer.pad_token_id]*del_arr.nonzero()[0].size))#.tolist()\n","\n","    def __getitem__(self, index):\n","        record = self.data.iloc[index]\n","        pattern, label = record[\"pattern\"], record[\"label\"]\n","        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n","        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n","        labels = self._labeling(pattern+label)\n","        encoder_input_ids = self._random_deletion(encoder_input_ids)\n","\n","        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n","                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n","                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n","                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n","                \"labels\":np.array(labels,dtype=np.int_)}\n","\n","\n","class Infilling_Dataset(Pet_Dataset):\n","    def _random_infilling(self, input, ratio=0.15, l=3):\n","        input = np.array(input)\n","        eos_idx = np.where(input==self.tokenizer.eos_token_id)[0][0]\n","        text_range = input[1:eos_idx]\n","        poi = poisson(l).pmf(text_range) > ratio\n","        infill = np.where(poi, np.array([self.tokenizer.mask_token_id]+[-100]*(poi.size-1)), text_range)\n","        infill = np.append(input[0], np.delete(infill,np.where(infill == -100)))\n","        infill = np.append(infill, [self.tokenizer.pad_token_id]*(np.count_nonzero(poi)-1))\n","        infill = np.append(infill, input[eos_idx:])\n","        if poi.max() == poi.min():\n","            if not len(infill) > len(input):\n","                infill = np.insert(infill, np.random.choice(len(text_range)),\n","                                   self.tokenizer.mask_token_id)[:len(input)]\n","            else:\n","                _size = int(len(text_range)*ratio)\n","                infill = np.insert(infill, np.random.choice(len(text_range),\n","                                                            size=_size), self.tokenizer.mask_token_id)[:len(input)-1]\n","                infill = np.append(infill, [self.tokenizer.eos_token_id])\n","        if infill.size == (self.max_seq_len-1):\n","            infill = np.append(infill, input[-1])\n","        return np.int_(infill)\n","\n","    def __getitem__(self, index):\n","        record = self.data.iloc[index]\n","        pattern, label = record[\"pattern\"], record[\"label\"]\n","        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n","        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n","        labels = self._labeling(pattern+label)\n","        encoder_input_ids = self._random_infilling(encoder_input_ids)\n","\n","        return {\"input_ids\":np.array(encoder_input_ids, dtype=np.int_),\n","                \"attention_mask\":np.array(encoder_attention_mask,dtype=np.float32),\n","                \"decoder_input_ids\":np.array(decoder_input_ids, dtype=np.int_),\n","                \"decoder_attention_mask\":np.array(decoder_attention_mask,dtype=np.float32),\n","                \"labels\":np.array(labels,dtype=np.int_)}\n","\n","class RandomMultiToeknFilling_Dataset(\n","    Masked_Dataset, Permutation_Dataset, Deletion_Dataset, Infilling_Dataset):\n","    def _random_pattern(self, input):\n","        _func = np.random.choice([self._random_masking,\n","                                  self._random_rotation,\n","                                  self._random_deletion,\n","                                  self._random_infilling], 1)[0]\n","        return _func(input=input)\n","\n","    def __getitem__(self, index):\n","        record = self.data.iloc[index]\n","        pattern, label = record[\"pattern\"], record[\"label\"]\n","        encoder_input_ids, encoder_attention_mask = self._encode(pattern)\n","        decoder_input_ids, decoder_attention_mask = self._encode(pattern+label)\n","        labels = self._labeling(pattern+label)\n","        encoder_input_ids = self._random_pattern(encoder_input_ids)\n","        input_ids = np.array(encoder_input_ids, dtype=np.int_)\n","        attention_mask = np.array(encoder_attention_mask,dtype=np.float32)\n","        decoder_input_ids = np.array(decoder_input_ids, dtype=np.int_)\n","        decoder_attention_mask = np.array(decoder_attention_mask,dtype=np.float32)\n","        labels = np.array(labels,dtype=np.int_)\n","        return input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels\n","\n","\n","class LanguageDataModule(pl.LightningDataModule):\n","    def __init__(self,\n","                 train_file:str,\n","                 val_file:str,\n","                 test_file:str,\n","                 tokenizer_path:str,\n","                 max_seq_len:int=1024,\n","                 batch_size:int=4,\n","                 num_workers:int=0,\n","                 pinned:bool=True):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.max_seq_len = max_seq_len\n","        self.train_file_path = train_file\n","        self.val_file_path = val_file\n","        self.test_file_path = test_file\n","        self.tokenizer_path = tokenizer_path\n","        self.num_workers = num_workers\n","        self.pinned = pinned\n","\n","    def _load_multiple(self, file_path, shuffle):\n","        return {\n","            \"Deletion Data\": DataLoader(\n","                Deletion_Dataset(\n","                    max_seq_len=self.max_seq_len,\n","                    file_path=file_path,\n","                    tokenizer_path=self.tokenizer_path),\n","                pin_memory=self.pinned,\n","                batch_size=self.batch_size,\n","                num_workers=self.num_workers,\n","                shuffle=shuffle),\n","            \"Permutation Data\": DataLoader(\n","                Permutation_Dataset(\n","                    max_seq_len=self.max_seq_len,\n","                    file_path=file_path,\n","                    tokenizer_path=self.tokenizer_path),\n","                pin_memory=self.pinned,\n","                batch_size=self.batch_size,\n","                num_workers=self.num_workers,\n","                shuffle=shuffle),\n","            \"Masked Data\": DataLoader(\n","                Masked_Dataset(\n","                    max_seq_len=self.max_seq_len,\n","                    file_path=file_path,\n","                    tokenizer_path=self.tokenizer_path),\n","                pin_memory=self.pinned,\n","                batch_size=self.batch_size,\n","                num_workers=self.num_workers,\n","                shuffle=shuffle),\n","            \"Infilling Data\": DataLoader(\n","                Infilling_Dataset(\n","                    max_seq_len=self.max_seq_len,\n","                    file_path=file_path,\n","                    tokenizer_path=self.tokenizer_path),\n","                pin_memory=self.pinned,\n","                batch_size=self.batch_size,\n","                num_workers=self.num_workers,\n","                shuffle=shuffle)\n","        }\n","\n","    def _load_random_token(self, file_path, shuffle):\n","        return DataLoader(\n","            RandomMultiToeknFilling_Dataset(\n","                max_seq_len=self.max_seq_len,\n","                file_path=file_path,\n","                tokenizer_path=self.tokenizer_path),\n","            pin_memory=self.pinned,\n","            batch_size=self.batch_size,\n","            num_workers=self.num_workers,\n","            shuffle=shuffle)\n","\n","    def setup(self, stage):\n","        # self.train = self._load_multiple(file_path=self.train_file_path, shuffle=True)\n","        # self.val = self._load_multiple(file_path=self.val_file_path, shuffle=False)\n","        # self.test = self._load_multiple(file_path=self.test_file_path, shuffle=False)\n","        self.train = self._load_random_token(file_path=self.train_file_path, shuffle=True)\n","        self.val = self._load_random_token(file_path=self.val_file_path, shuffle=False)\n","        self.test = self._load_random_token(file_path=self.test_file_path, shuffle=False)\n","\n","    def train_dataloader(self):\n","        return self.train\n","        # return CombinedLoader(self.train, mode=\"max_size_cycle\")\n","\n","    def val_dataloader(self):\n","        return self.val\n","        # return CombinedLoader(self.val, mode=\"max_size_cycle\")\n","\n","    def test_dataloader(self):\n","        return self.test\n","        # return CombinedLoader(self.test, mode=\"max_size_cycle\")"],"metadata":{"id":"bKeUBsPsbnwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_3TBZtkbA1o","outputId":"3d5218dc-6d53-4831-c5b1-a955a57a9158"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /home/ubuntu/kevin.jung/colossal/model.py\n"]}],"source":["%%writefile /content/colossal/colossal/model.py\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","from transformers import GPT2Config, GPT2LMHeadModel, GPT2PreTrainedModel\n","from colossalai.nn.optimizer import HybridAdam\n","from colossalai.utils import colo_set_process_memory_fraction\n","from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n","from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n","from torch.optim import Adam, Optimizer\n","from functools import partial\n","from typing import Callable, Iterable\n","from contextlib import contextmanager\n","__all__ = ['GPTLitModule', 'get_optimizer']\n","\n","\n","@contextmanager\n","def no_init_weights():\n","    def dummy_fn(*args):\n","        return\n","    try:\n","        old_init_weights = GPT2PreTrainedModel._init_weights\n","        GPT2PreTrainedModel._init_weights = dummy_fn\n","        yield\n","    finally:\n","        GPT2PreTrainedModel._init_weights = old_init_weights\n","\n","\n","class GPTLMModel(nn.Module):\n","    def __init__(self, hidden_size=768, num_layers=12, num_attention_heads=12, max_seq_len=1024, vocab_size=50257, checkpoint=False):\n","        super().__init__()\n","        self.checkpoint = checkpoint\n","        with no_init_weights():\n","            self.model = GPT2LMHeadModel(GPT2Config(n_embd=hidden_size, n_layer=num_layers,\n","                                                    n_head=num_attention_heads, n_positions=max_seq_len, n_ctx=max_seq_len, vocab_size=vocab_size))\n","        if checkpoint:\n","            self.model.gradient_checkpointing_enable()\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Only return lm_logits\n","        return self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=not self.checkpoint)[0]\n","\n","\n","def gpt2_tiny(checkpoint=True):\n","    return GPTLMModel(hidden_size=128, num_layers=4, num_attention_heads=4, checkpoint=checkpoint)\n","\n","\n","def gpt2_small(checkpoint=True):\n","    return GPTLMModel(hidden_size=768, num_layers=12, num_attention_heads=12, checkpoint=checkpoint)\n","\n","\n","def gpt2_medium(checkpoint=True):\n","    return GPTLMModel(hidden_size=1024, num_layers=24, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_large(checkpoint=True):\n","    return GPTLMModel(hidden_size=1280, num_layers=36, num_attention_heads=20, checkpoint=checkpoint)\n","\n","\n","def gpt2_xl(checkpoint=True):\n","    return GPTLMModel(hidden_size=1600, num_layers=48, num_attention_heads=25, checkpoint=checkpoint)\n","\n","\n","def gpt2_2B(checkpoint=True):\n","    return GPTLMModel(hidden_size=2048, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_3B(checkpoint=True):\n","    return GPTLMModel(hidden_size=2304, num_layers=48, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_4B(checkpoint=True):\n","    return GPTLMModel(hidden_size=2304, num_layers=64, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_6B(checkpoint=True):\n","    return GPTLMModel(hidden_size=4096, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_8B(checkpoint=True):\n","    return GPTLMModel(hidden_size=3072, num_layers=72, num_attention_heads=24, checkpoint=checkpoint)\n","\n","\n","def gpt2_12B(checkpoint=True):\n","    return GPTLMModel(hidden_size=4096, num_layers=60, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_15B(checkpoint=True):\n","    return GPTLMModel(hidden_size=4096, num_layers=78, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_18B(checkpoint=True):\n","    return GPTLMModel(hidden_size=4096, num_layers=90, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_20B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=25, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_24B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_28B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=35, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_32B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_36B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=45, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_40B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=50, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt2_45B(checkpoint=True):\n","    return GPTLMModel(hidden_size=8192, num_layers=56, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def gpt3(checkpoint=True):\n","    return GPTLMModel(max_seq_len=2048, hidden_size=12288, num_layers=96, num_attention_heads=96, checkpoint=checkpoint)\n","\n","\n","def get_gpt_model(model_name: str, checkpoint: bool = True) -> nn.Module:\n","    model_map = {\n","        'gpt2_tiny': gpt2_tiny,\n","        'gpt2_small': gpt2_small,\n","        'gpt2_medium': gpt2_medium,\n","        'gpt2_large': gpt2_large,\n","        'gpt2_xl': gpt2_xl,\n","        'gpt2_2B': gpt2_2B,\n","        'gpt2_3B': gpt2_3B,\n","        'gpt2_4B': gpt2_4B,\n","        'gpt2_6B': gpt2_6B,\n","        'gpt2_8B': gpt2_8B,\n","        'gpt2_12B': gpt2_12B,\n","        'gpt2_15B': gpt2_15B,\n","        'gpt2_18B': gpt2_18B,\n","        'gpt2_20B': gpt2_20B,\n","        'gpt2_24B': gpt2_24B,\n","        'gpt2_28B': gpt2_28B,\n","        'gpt2_32B': gpt2_32B,\n","        'gpt2_36B': gpt2_36B,\n","        'gpt2_40B': gpt2_40B,\n","        'gpt2_45B': gpt2_45B,\n","        'gpt3': gpt3,\n","    }\n","    assert model_name in model_map\n","    return model_map[model_name](checkpoint)\n","\n","\n","class GPTLMLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.loss = nn.CrossEntropyLoss()\n","\n","    def forward(self, logits, labels):\n","        shift_logits = logits[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","        # Flatten the tokens\n","        return self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","\n","def get_optimizer(strategy: str, **kwargs) -> Callable[[Iterable], Optimizer]:\n","    assert strategy in ('ddp', 'deepspeed', 'colossal')\n","    if strategy == 'ddp':\n","        opt_cls = Adam\n","    elif strategy == 'deepspeed':\n","        offload = kwargs.pop('offload')\n","        if offload:\n","            opt_cls = DeepSpeedCPUAdam\n","        else:\n","            opt_cls = FusedAdam\n","    else:\n","        opt_cls = HybridAdam\n","    return partial(opt_cls, **kwargs)\n","\n","\n","class GPTLitModule(pl.LightningModule):\n","    def __init__(self,\n","                 model_name: str,\n","                 optimizer_init_fn: Callable[[Iterable], Optimizer],\n","                 checkpoint: bool = True,\n","                 cuda_mem_fraction: float = 1.0,\n","                 model_checkpoint_dir: str = None) -> None:\n","        super().__init__()\n","        self.model_name = model_name\n","        self.optimizer_init_fn = optimizer_init_fn\n","        self.checkpoint = checkpoint\n","        self.criterion = GPTLMLoss()\n","        self.cuda_mem_fraction = cuda_mem_fraction\n","        self.model_checkpoint = model_checkpoint_dir\n","\n","    def configure_sharded_model(self) -> None:\n","        self.model = get_gpt_model(self.model_name, self.checkpoint)\n","\n","    def on_load_checkpoint(self, checkpoint) -> None:\n","        if not hasattr(self, 'model'):\n","            self.configure_sharded_model()\n","        if self.model_checkpoint:\n","            print(f\"Load Checkpoint from {self.model_checkpoint}\")\n","            self.model.model.load_state_dict(\n","                get_fp32_state_dict_from_zero_checkpoint(self.model_checkpoint))\n","\n","    def configure_optimizers(self):\n","        return self.optimizer_init_fn(self.model.parameters())\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask = batch\n","        logits = self.model(input_ids, attention_mask)\n","        loss = self.criterion(logits, input_ids)\n","        return loss\n","\n","    def on_fit_start(self) -> None:\n","        if self.cuda_mem_fraction < 1.0:\n","            colo_set_process_memory_fraction(self.cuda_mem_fraction)\n",""]},{"cell_type":"code","source":["%%writefile /content/colossal/s2s_model.py\n","import gc\n","import torch\n","import os\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","from transformers import BartForConditionalGeneration, BartModel, BartConfig, BartPretrainedModel\n","from colossalai.nn.optimizer import HybridAdam\n","from colossalai.utils import colo_set_process_memory_fraction\n","\n","import deepspeed\n","from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n","from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n","from deepspeed.accelerator import get_accelerator\n","from torch.optim import Adam, Optimizer\n","from functools import partial\n","from typing import Callable, Iterable\n","from contextlib import contextmanager\n","__all__ = ['S2SLitModule', 'get_optimizer']\n","\n","\n","@contextmanager\n","def no_init_weights():\n","    def dummy_fn(*args):\n","        return\n","    try:\n","        old_init_weights = BartPretrainedModel._init_weights\n","        BartPretrainedModel._init_weights = dummy_fn\n","        yield\n","    finally:\n","        BartPretrainedModel._init_weights = old_init_weights\n","\n","\n","class S2SLMModel(nn.Module):\n","    def __init__(self,\n","                 hidden_size:int=768,\n","                 num_layers:int=12,\n","                 num_attention_heads:int=12,\n","                 prompt_layers:int=3,\n","                 max_seq_len:int=1024,\n","                 vocab_size:int=64512, # 50257\n","                 checkpoint:bool=False):\n","        super().__init__()\n","        self.checkpoint = checkpoint\n","        with no_init_weights():\n","            self.model = BartForConditionalGeneration(\n","                BartConfig(\n","                    d_model=hidden_size,\n","                    encoder_layers=prompt_layers,\n","                    decoder_layers=num_layers,\n","                    encoder_head=num_attention_heads,\n","                    decoder_head=num_attention_heads,\n","                    max_position_embeddings=max_seq_len,\n","                    n_ctx=max_seq_len,\n","                    vocab_size=vocab_size))\n","        if checkpoint:\n","            self.model.gradient_checkpointing_enable()\n","\n","    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n","        # Only return lm_logits\n","        return self.model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            use_cache=not self.checkpoint)[0]\n","\n","\n","def s2s_tiny(checkpoint=True):\n","    return S2SLMModel(hidden_size=128, num_layers=4, num_attention_heads=4, checkpoint=checkpoint)\n","\n","\n","def s2s_small(checkpoint=True):\n","    return S2SLMModel(hidden_size=768, num_layers=12, num_attention_heads=12, checkpoint=checkpoint)\n","\n","\n","def s2s_medium(checkpoint=True):\n","    return S2SLMModel(hidden_size=1024, num_layers=24, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_large(checkpoint=True):\n","    return S2SLMModel(hidden_size=1280, num_layers=36, num_attention_heads=20, checkpoint=checkpoint)\n","\n","\n","def s2s_xl(checkpoint=True):\n","    return S2SLMModel(hidden_size=1600, num_layers=48, num_attention_heads=25, checkpoint=checkpoint)\n","\n","\n","def s2s_2B(checkpoint=True):\n","    return S2SLMModel(hidden_size=2048, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_9B(checkpoint=True):\n","    return S2SLMModel(hidden_size=2048, num_layers=178, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_3B(checkpoint=True):\n","    return S2SLMModel(hidden_size=2304, num_layers=48, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_4B(checkpoint=True):\n","    return S2SLMModel(hidden_size=2304, num_layers=64, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_6B(checkpoint=True):\n","    return S2SLMModel(hidden_size=4096, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_8B(checkpoint=True):\n","    return S2SLMModel(hidden_size=3072, num_layers=72, num_attention_heads=24, checkpoint=checkpoint)\n","\n","\n","def s2s_12B(checkpoint=True):\n","    return S2SLMModel(hidden_size=4096, num_layers=60, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_15B(checkpoint=True):\n","    return S2SLMModel(hidden_size=4096, num_layers=78, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_18B(checkpoint=True):\n","    return S2SLMModel(hidden_size=4096, num_layers=90, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_20B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=25, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_24B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=30, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_28B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=35, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_32B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=40, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_36B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=45, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_40B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=50, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_45B(checkpoint=True):\n","    return S2SLMModel(hidden_size=8192, num_layers=56, num_attention_heads=16, checkpoint=checkpoint)\n","\n","\n","def s2s_LLM(checkpoint=True):\n","    return S2SLMModel(max_seq_len=2048, hidden_size=12288, num_layers=96, num_attention_heads=96, checkpoint=checkpoint)\n","\n","\n","def get_s2s_model(model_name: str, checkpoint: bool = True) -> nn.Module:\n","    model_map = {\n","        's2s_tiny': s2s_tiny,\n","        's2s_small': s2s_small,\n","        's2s_medium': s2s_medium,\n","        's2s_large': s2s_large,\n","        's2s_xl': s2s_xl,\n","        's2s_2B': s2s_2B,\n","        's2s_9.2B': s2s_9B,\n","        's2s_3B': s2s_3B,\n","        's2s_4B': s2s_4B,\n","        's2s_6B': s2s_6B,\n","        's2s_8B': s2s_8B,\n","        's2s_12B': s2s_12B,\n","        's2s_15B': s2s_15B,\n","        's2s_18B': s2s_18B,\n","        's2s_20B': s2s_20B,\n","        's2s_24B': s2s_24B,\n","        's2s_28B': s2s_28B,\n","        's2s_32B': s2s_32B,\n","        's2s_36B': s2s_36B,\n","        's2s_40B': s2s_40B,\n","        's2s_45B': s2s_45B,\n","        's2s_LLM': s2s_LLM,\n","    }\n","    assert model_name in model_map\n","    # print(f\"Training model is {model_name}\")\n","    return model_map[model_name](checkpoint)\n","\n","\n","class GPTLMLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.loss = nn.CrossEntropyLoss()\n","\n","    def forward(self, logits, labels):\n","        shift_logits = logits[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","        # Flatten the tokens\n","        return self.loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","\n","def get_optimizer(strategy: str, **kwargs) -> Callable[[Iterable], Optimizer]:\n","    assert strategy in ('ddp', 'deepspeed', 'colossal')\n","    if strategy == 'ddp':\n","        opt_cls = Adam\n","    elif strategy == 'deepspeed':\n","        offload = kwargs.pop('offload')\n","        if offload:\n","            opt_cls = DeepSpeedCPUAdam\n","        else:\n","            opt_cls = FusedAdam\n","    else:\n","        opt_cls = HybridAdam\n","    return partial(opt_cls, **kwargs)\n","\n","\n","class S2SLitModule(pl.LightningModule):\n","    def __init__(self,\n","                 model_name: str,\n","                 optimizer_init_fn: Callable[[Iterable], Optimizer],\n","                 checkpoint: bool = True,\n","                 cuda_mem_fraction: float = 1.0,\n","                 model_checkpoint_dir: str = None) -> None:\n","        super().__init__()\n","        self.model_name = model_name\n","        self.optimizer_init_fn = optimizer_init_fn\n","        self.checkpoint = checkpoint\n","        self.criterion = GPTLMLoss()\n","        self.cuda_mem_fraction = cuda_mem_fraction\n","        self.model_checkpoint = model_checkpoint_dir\n","\n","    def _save_in_hub_(self)->None:\n","        # print(self.model.model)\n","        self.model.model.save_pretrained(\n","            save_directory=\"/content/llm_checkpoint/\",#\n","            # use_temp_dir=False,\n","            push_to_hub=True,\n","            max_shard_size=\"124MB\",\n","            # safe_serialization=True,\n","            repo_id=os.getenv(\"MODEL_SAVE_REPO\"),\n","            use_auth_token=os.getenv(\"HUGGINGFACE_AUTO_TOKEN\"))\n","\n","    def __memory_clean__(self)->None:\n","        get_accelerator().empty_cache()\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    def configure_sharded_model(self) -> None:\n","        self.model = get_s2s_model(\n","            model_name=self.model_name,\n","            checkpoint=self.checkpoint)\n","\n","    # def on_save_checkpoint(self, checkpoint)->None:\n","    #     self._save_in_hub_()\n","\n","    def on_load_checkpoint(self, checkpoint) -> None:\n","        if not hasattr(self, 'model'):\n","            self.configure_sharded_model()\n","        if self.model_checkpoint:\n","            print(f\"Load Checkpoint from {self.model_checkpoint}\")\n","            self.model.model.load_state_dict(\n","                get_fp32_state_dict_from_zero_checkpoint(self.model_checkpoint))\n","\n","    def configure_optimizers(self):\n","        return self.optimizer_init_fn(self.model.parameters())\n","\n","    def training_step(self, batch, batch_idx):\n","        if type(batch) is dict:\n","            loss_list= torch.empty(0,device=torch.cuda.current_device())\n","            for loader in batch:\n","                input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels = batch[loader].values()\n","                logits = self.model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        decoder_input_ids=decoder_input_ids,\n","                        decoder_attention_mask=decoder_attention_mask)\n","                loss = self.criterion(logits, labels)\n","                loss_list = torch.cat((loss_list, loss.view(-1)))\n","            loss = loss_list.sum()\n","            return loss\n","        else:\n","            input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels = batch\n","            logits = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask)\n","            loss = self.criterion(logits, input_ids)\n","            return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        if type(batch) is dict:\n","            loss_list= torch.empty(0,device=torch.cuda.current_device())\n","            for loader in batch:\n","                input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels = batch[loader].values()\n","                logits = self.model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        decoder_input_ids=decoder_input_ids,\n","                        decoder_attention_mask=decoder_attention_mask)\n","                loss = self.criterion(logits, labels)\n","                loss_list = torch.cat((loss_list, loss.view(-1)))\n","            loss = loss_list.sum()\n","        else:\n","            input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels = batch\n","            logits = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask)\n","            loss = self.criterion(logits, labels)\n","\n","    def on_training_batch_end(self, outputs, batch, batch_idx):\n","        self.__memory_clean__()\n","\n","    def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n","        self.__memory_clean__()\n","\n","    def on_fit_start(self) -> None:\n","        if self.cuda_mem_fraction < 1.0:\n","            colo_set_process_memory_fraction(self.cuda_mem_fraction)\n",""],"metadata":{"id":"gLPCwb-9cDCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Yore_jvbA1p","outputId":"bf0f145e-3052-4db1-b277-0c54674d80a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /home/ubuntu/kevin.jung/colossal/train.py\n"]}],"source":["%%writefile /content/colossal/colossal/train.py\n","import pytorch_lightning as pl\n","import argparse\n","import warnings\n","import logging\n","from data import RandomDataloader, RandomS2SDataloader\n","from dataloader import LanguageDataModule\n","from model import GPTLitModule, get_optimizer\n","from s2s_model import S2SLitModule\n","from callback import MemoryMonitor\n","from pytorch_lightning.callbacks import TQDMProgressBar, ModelCheckpoint\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.strategies.deepspeed import DeepSpeedStrategy\n","from pytorch_lightning.strategies.colossalai import ColossalAIStrategy\n","# from pytorch_lightning.plugins.deepspeed import Deepspeed\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--tqdm_rate', type=int, default=2000)\n","    parser.add_argument('--epochs', type=int, default=2)\n","    parser.add_argument('--steps_per_epoch', type=int, default=4)\n","    parser.add_argument('--batch_size', type=int, default=1)\n","    parser.add_argument('--lr', type=float, default=1e-3)\n","    parser.add_argument('--model', default='gpt2_xl')\n","    parser.add_argument('--np', type=int, default=1)\n","    parser.add_argument('--no_activation_ckpt', action='store_true', default=False)\n","    parser.add_argument('--opt_nvme_offload_frac', type=float, default=0.0)\n","    parser.add_argument('--opt_nvme_offload_dir', default='./offload')\n","    parser.add_argument('--seq_len', type=int, default=1024)\n","    parser.add_argument('--placement_policy', default='cuda')\n","    parser.add_argument('--opt_gpu_margin_rat', type=float, default=0.0)\n","    parser.add_argument('--cuda_mem_frac', type=float, default=1.0)\n","    parser.add_argument('--strategy', default='ddp', choices=['ddp', 'colossal', 'deepspeed'])\n","    parser.add_argument('--offload', action='store_true', default=False)\n","    parser.add_argument('--model_checkpoint_dir', default='/data/llm_checkpoint/')\n","    parser.add_argument('--model_zero_ckpt_dir', default=None)\n","    args = parser.parse_args()\n","\n","    if \"gpt\" in args.model:\n","        lit_module = GPTLitModule\n","        train_dataloader = RandomDataloader(args.steps_per_epoch, args.batch_size, args.seq_len)\n","        data_module = None\n","    else:\n","        lit_module = S2SLitModule\n","        train_dataloader = None # RandomS2SDataloader(args.steps_per_epoch, args.batch_size, args.seq_len)\n","        data_module = LanguageDataModule(\n","            train_file=\"/data/kevin.jung/Train.csv\",\n","            val_file=\"/data/kevin.jung/Dev_s.csv\",\n","            test_file=\"/data/kevin.jung/Test.csv\",\n","            tokenizer_path=\"Gunulhona/tb_tokenizer_big\",\n","            max_seq_len=args.seq_len,\n","            batch_size=args.batch_size)\n","\n","    optimizer_cfg = {\n","        'lr': args.lr\n","        }\n","\n","    if args.strategy == 'ddp':\n","        trainer_cfg = {\n","            'accelerator': 'gpu',\n","            'precision': 16,\n","            'strategy': DDPStrategy(static_graph=True)\n","        }\n","\n","    elif args.strategy == 'colossal':\n","        trainer_cfg = {\n","            'accelerator': 'gpu',\n","            'precision': 16,\n","            'strategy': ColossalAIStrategy(\n","                placement_policy=args.placement_policy,\n","                gpu_margin_mem_ratio=args.opt_gpu_margin_rat,\n","                initial_scale=32,\n","                chunk_search_range= 64 * 1024**2,\n","                chunk_search_n_grids= 4096,\n","                min_chunk_size= 32 * 1024**2)\n","            }\n","\n","        optimizer_cfg['nvme_offload_dir'] = args.opt_nvme_offload_dir\n","        optimizer_cfg['nvme_offload_fraction'] = args.opt_nvme_offload_frac\n","\n","    elif args.strategy == 'deepspeed':\n","        trainer_cfg = {\n","            'accelerator': 'gpu',\n","            'precision': 16,\n","            'strategy': DeepSpeedStrategy(\n","                stage=3,\n","                offload_parameters=args.offload,\n","                offload_optimizer=args.offload,\n","                initial_scale_power=5,\n","                load_full_weights=True,\n","                logging_batch_size_per_gpu=args.batch_size,\n","                logging_level=logging.ERROR) # 로그에 warning 너무 많이 쌓여서 추가\n","            }\n","\n","        optimizer_cfg['offload'] = args.offload\n","\n","    opt_init_fn = get_optimizer(args.strategy, **optimizer_cfg)\n","\n","    model = lit_module(\n","        model_name=args.model,\n","        optimizer_init_fn=opt_init_fn,\n","        checkpoint=not args.no_activation_ckpt,\n","        cuda_mem_fraction=args.cuda_mem_frac,\n","        model_checkpoint_dir=args.model_zero_ckpt_dir)\n","\n","    trainer = pl.Trainer(\n","        max_epochs=args.epochs,\n","        devices=args.np,\n","        enable_checkpointing=True,\n","        callbacks=[\n","            MemoryMonitor(),\n","            TQDMProgressBar(\n","                refresh_rate=args.tqdm_rate),\n","            ModelCheckpoint(\n","                dirpath=args.model_checkpoint_dir,\n","                mode=\"min\",\n","                monitor=\"loss\",\n","                filename=\"llm-{epoch:02d}-{val_loss:.4f}.ckpt\",\n","                every_n_train_steps=1,\n","                save_last=True)],\n","        fast_dev_run=False,\n","        profiler=\"advanced\",\n","        **trainer_cfg)\n","\n","    trainer.fit(\n","        model=model,\n","        train_dataloaders=train_dataloader,\n","        datamodule=data_module)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtA7JoKjbA1p","outputId":"ae2bf6cc-9d80-4f65-c99b-d6fae7950dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /home/ubuntu/kevin.jung/colossal/train_start\n"]}],"source":["%%writefile /content/colossal/colossal/train_start\n","# export CUDA_LAUNCH_BLOCKING=\"1\"\n","# export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\n","export TOKENIZERS_PARALLELISM=\"0\"\n","\n","\n","EXECUTEFILE=\"colossal/train.py\"                         # needs custom trainer path\n","EPOCHS=100                                              # type=int       default=2\n","TQDM_RATE=2000                                          # type=int       default=2000\n","LEARNING_RATE=5e-5                                      # type=float     default=1e-3\n","STRATEGY=\"colossal\"                                     # type=str       default='ddp'         choices=['ddp', 'colossal', 'deepspeed']\n","ACCELERATOR=\"gpu\"                                       # type=str       default=gpu\n","NP=-1                                                   # type=int       default=1\n","BATCHSIZE=1                                             # type=int       default=1\n","MODEL_NAME='gpt2_2B'                                    # type=str       default='gpt2_xl'     choices=['gpt2_tiny'~'gpt2_xl'~'gpt3']\n","STEPS_PER_EPOCH=4                                       # type=int       default=4\n","NAC=false                                               # type=bool      default=False         action='store_true'\n","OFFLOAD=false                                           # type=bool      default=False         action='store_true'\n","OPT_NVME_OFFLAND_FRAC=0.0                               # type=float     default=0.0\n","OPT_NVME_OFFLAND_DIR='/data/opt/'                       # type=str       default='/data/opt'\n","SEQ_LEN=1024                                            # type=int       default=1024\n","PLACEMENT_POLICY='cuda'                                 # type=str       defualt='cuda'\n","OPT_GPU_MARGIN_RAT=0.0                                  # type=float     defualt=0.0\n","CUDA_MEMORY_FRAC=1.0                                    # type=float     defualt=1.0\n","MODEL_ZERO_CKPT_DIR='/data/llm_checkpoint/last.ckpt'    # type=str               default=None\n","\n","python $EXECUTEFILE\\\n","  --tqdm_rate $TQDM_RATE\\\n","  --model $MODEL_NAME\\\n","  --epochs $EPOCHS\\\n","  --steps_per_epoch $STEPS_PER_EPOCH\\\n","  --batch_size $BATCHSIZE\\\n","  --seq_len $SEQ_LEN\\\n","  --cuda_mem_frac $CUDA_MEMORY_FRAC\\\n","  --np $NP\\\n","  --strategy $STRATEGY\\\n","  --placement_policy $PLACEMENT_POLICY\\\n","  --lr $LEARNING_RATE \\\n","  --no_activation_ckpt\\\n","  --offload\\\n","  --opt_nvme_offload_frac $OPT_NVME_OFFLAND_FRAC\\\n","  --opt_nvme_offload_dir $OPT_NVME_OFFLAND_DIR\\\n","  --opt_gpu_margin_rat $OPT_GPU_MARGIN_RAT\\\n","  --model_zero_ckpt_dir $MODEL_ZERO_CKPT_DIR\n",""]},{"cell_type":"code","source":["# file upload to huggingface\n","from huggingface_hub import HfApi\n","\n","api=HfApi()\n","\n","api.upload_folder(\n","    repo_id=\"REPOID\",\n","    folder_path=\"/content/llm_checkpoint/last.ckpt/\",\n","    repo_type=\"model\",\n","    token='HUGGINFACE_AUTH_TOKEN')"],"metadata":{"id":"tNHbccXDdBbv"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('nlp_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"e90295cf50fa08954a711c27a0fd85ffb404c65ef479e602f0e36fbf546ee202"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}