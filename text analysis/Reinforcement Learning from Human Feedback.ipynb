{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Weight&Biases 기제 글](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx) 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017년 OpenAI에서는 사람의 피드백을 포함한 강화학습 기법을 적용한 논문  [\"Deep Reinforcement Learning from Human Preferences.\"](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)를 발표했다.\n",
    "\n",
    "사람의 피드백을 포함하는 루프를 통하여 문서 요약 성능을 올리기 위한 접근을 시발점으로 InstructGPT와 지금의 ChatGPT를 개발했다.\n",
    "\n",
    "모델 학습 중 사람의 피드백이 합쳐지는 방법을 정렬과정이라고 명명하기로 한다.\n",
    "\n",
    "학습 모델의 성능이 높아질수록, 목표에 대한 정렬과정이 친인간적인 가치를 생산하는데 중요한 역할이 된다.\n",
    "\n",
    "(OpenAI에서 연구했던 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
