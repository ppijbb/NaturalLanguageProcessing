{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA(Latent Semantic Anaylsis; 잠재의미 분석)\n",
    "토픽 모델링에 아이디어를 주는 알고리즘\n",
    "\n",
    "BoW, DTM, TF-IDF 등 기본적 단어 빈도수 기반 수치화 방법은 단어의 의미를 고려하지 못함\n",
    "\n",
    "DTM의 잠재된(Latent) 의미를 이끌어내는 방법(Latent Semantic Analysis)가 재안\n",
    "\n",
    "LSI(Latent Semantic Indexing)이라고 하는 경우도 있음\n",
    "\n",
    "선형대수학의 특이값 분해(Singular Value Decomposition)을 기반함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA(Latent Dirichlet Allocation:; 잠재 디클레리 할당)\n",
    "토픽 모델링의 대표적 알고리즘\n",
    "\n",
    "LDA문서들은 토픽들의 혼합으로 구성되어 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정\n",
    "\n",
    "데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추적하는 방법\n",
    "\n",
    "사용자가 토픽의 개수를 정하면 LDA에서 토픽이 전체 문서에서 분포하여 있다고 가정\n",
    "\n",
    "모든 단어를 토픽(k개)에 할당\n",
    "\n",
    "단어 중 토픽이 잘못 할당 된 경우에도 기준에 따라서 단어에 토픽을 재할당\n",
    "\n",
    "> LSA는 DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음\\\n",
    "> LDA는 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
