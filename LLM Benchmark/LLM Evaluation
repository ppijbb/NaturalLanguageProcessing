{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVWOlO439ue9pE8jDVzKc+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"LNmjT98_TnZF","executionInfo":{"status":"ok","timestamp":1736832944323,"user_tz":-540,"elapsed":32180,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"outputs":[],"source":["%%capture\n","#@title Installation\n","!git clone https://github.com/EleutherAI/lm-evaluation-harness\n","!pip install -e lm-evaluation-harness"]},{"cell_type":"code","source":["%%sh\n","#@title Evalutate with EleutherAI lm evaluation harness\n","MODEL_NAME=\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\" # @param {\"type\":\"string\",\"placeholder\":\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\"}\n","BATCH_SIZE = 1 # @param {\"type\":\"integer\",\"placeholder\":\"1\"}\n","\n","lm_eval \\\n","    --model hf \\\n","    --model_args pretrained=$MODEL_NAME \\\n","    --tasks lambada_openai,hellaswag \\\n","    --batch_size $BATCH_SIZE \\\n","    --device cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"cellView":"form","id":"MROO9wIZTtsg","executionInfo":{"status":"error","timestamp":1736834329007,"user_tz":-540,"elapsed":15556,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"e3d9a40e-cc63-437f-f698-c78f2e4edf46"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["sh: 3: BATCH_SIZE: not found\n","2025-01-14 05:58:41.630052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-14 05:58:41.657195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-14 05:58:41.665406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-14 05:58:41.688960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-14 05:58:43.381901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2] [--model_args MODEL_ARGS]\n","               [--num_fewshot N] [--batch_size auto|auto:N|N] [--max_batch_size N]\n","               [--device DEVICE] [--output_path DIR|DIR/file.json] [--limit N|0<N<1]\n","               [--use_cache DIR] [--cache_requests {true,refresh,delete}] [--check_integrity]\n","               [--write_out] [--log_samples] [--system_instruction SYSTEM_INSTRUCTION]\n","               [--apply_chat_template [APPLY_CHAT_TEMPLATE]] [--fewshot_as_multiturn]\n","               [--show_config] [--include_path DIR] [--gen_kwargs GEN_KWARGS]\n","               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG] [--wandb_args WANDB_ARGS]\n","               [--hf_hub_log_args HF_HUB_LOG_ARGS] [--predict_only] [--seed SEED]\n","               [--trust_remote_code]\n","lm_eval: error: argument --batch_size/-b: expected one argument\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"Command 'b'#@title Evalutate with EleutherAI lm evaluation harness\\nMODEL_NAME=\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\" # @param {\"type\":\"string\",\"placeholder\":\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\"}\\nBATCH_SIZE = 1 # @param {\"type\":\"integer\",\"placeholder\":\"1\"}\\n\\nlm_eval \\\\\\n    --model hf \\\\\\n    --model_args pretrained=$MODEL_NAME \\\\\\n    --tasks lambada_openai,hellaswag \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --device cpu\\n'' returned non-zero exit status 2.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9e695b5fbacf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#@title Evalutate with EleutherAI lm evaluation harness\\nMODEL_NAME=\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\" # @param {\"type\":\"string\",\"placeholder\":\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\"}\\nBATCH_SIZE = 1 # @param {\"type\":\"integer\",\"placeholder\":\"1\"}\\n\\nlm_eval \\\\\\n    --model hf \\\\\\n    --model_args pretrained=$MODEL_NAME \\\\\\n    --tasks lambada_openai,hellaswag \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --device cpu\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'#@title Evalutate with EleutherAI lm evaluation harness\\nMODEL_NAME=\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\" # @param {\"type\":\"string\",\"placeholder\":\"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\"}\\nBATCH_SIZE = 1 # @param {\"type\":\"integer\",\"placeholder\":\"1\"}\\n\\nlm_eval \\\\\\n    --model hf \\\\\\n    --model_args pretrained=$MODEL_NAME \\\\\\n    --tasks lambada_openai,hellaswag \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --device cpu\\n'' returned non-zero exit status 2."]}]},{"cell_type":"code","source":["# @title # 🧐 LLM AutoEval\n","\n","# @markdown > 🗣️ [Large Language Model Course](https://github.com/mlabonne/llm-course)\n","\n","# @markdown ❤️ Created by [@maximelabonne](https://twitter.com/maximelabonne).\n","# @markdown * This notebook allows you to **automatically evaluate your LLMs** using RunPod (please consider using my [referral link](https://runpod.io?ref=9nvk2srl)).\n","# @markdown * The results are automatically uploaded to [GitHub Gist](https://gist.github.com/) and the pod is destroyed (you can safely close this tab).\n","# @markdown * For further details, see the project on 💻 [GitHub](https://github.com/mlabonne/llm-autoeval).\n","\n","!pip install -qqq runpod --progress-bar off\n","\n","import runpod\n","from google.colab import userdata\n","\n","# @markdown ---\n","\n","# @markdown ## 🔍 Evaluation\n","MODEL_ID = \"Gunulhona/Gemma-System-9B-MoRA-SimPO-no-adapter\" # @param {type:\"string\"}\n","BENCHMARK = \"nous\" # @param [\"nous\", \"eq-bench\", \"openllm\", \"lighteval\"]\n","\n","# @markdown For lighteval, select tasks as specified in the [readme](https://github.com/huggingface/lighteval?tab=readme-ov-file#usage) or in the list of [recommended tasks](https://github.com/huggingface/lighteval/blob/main/tasks_examples/recommended_set.txt).\n","\n","LIGHTEVAL_TASK = \"leaderboard|truthfulqa:mc|0|0,leaderboard|gsm8k|0|0\" # @param {type:\"string\"}\n","\n","# @markdown ---\n","\n","# @markdown ## ☁️ Cloud GPU\n","\n","GPU = \"NVIDIA GeForce RTX 3090\" # @param [\"NVIDIA A100 80GB PCIe\", \"NVIDIA A100-SXM4-80GB\", \"NVIDIA A30\", \"NVIDIA A40\", \"NVIDIA GeForce RTX 3070\", \"NVIDIA GeForce RTX 3080\", \"NVIDIA GeForce RTX 3080 Ti\", \"NVIDIA GeForce RTX 3090\", \"NVIDIA GeForce RTX 3090 Ti\", \"NVIDIA GeForce RTX 4070 Ti\", \"NVIDIA GeForce RTX 4080\", \"NVIDIA GeForce RTX 4090\", \"NVIDIA H100 80GB HBM3\", \"NVIDIA H100 PCIe\", \"NVIDIA L4\", \"NVIDIA L40\", \"NVIDIA RTX 4000 Ada Generation\", \"NVIDIA RTX 4000 SFF Ada Generation\", \"NVIDIA RTX 5000 Ada Generation\", \"NVIDIA RTX 6000 Ada Generation\", \"NVIDIA RTX A2000\", \"NVIDIA RTX A4000\", \"NVIDIA RTX A4500\", \"NVIDIA RTX A5000\", \"NVIDIA RTX A6000\", \"Tesla V100-FHHL-16GB\", \"Tesla V100-PCIE-16GB\", \"Tesla V100-SXM2-16GB\", \"Tesla V100-SXM2-32GB\"]\n","NUMBER_OF_GPUS = 1 # @param {type:\"slider\", min:1, max:8, step:1}\n","CONTAINER_DISK = 75 # @param {type:\"slider\", min:50, max:500, step:25}\n","CLOUD_TYPE = \"COMMUNITY\" # @param [\"COMMUNITY\", \"SECURE\"]\n","REPO = \"https://github.com/mlabonne/llm-autoeval.git\" # @param {type:\"string\"}\n","TRUST_REMOTE_CODE = False # @param {type:\"boolean\"}\n","PRIVATE_GIST = True # @param {type:\"boolean\"}\n","DEBUG = False # @param {type:\"boolean\"}\n","\n","# @markdown ---\n","\n","# @markdown ## 🔑 Tokens\n","# @markdown Enter the name of your tokens in the Secrets tab.\n","RUNPOD_TOKEN = \"runpod\" # @param {type:\"string\"}\n","GITHUB_TOKEN = \"github\" # @param {type:\"string\"}\n","HF_TOKEN = \"HF_TOKEN\" # @param {type:\"string\"}\n","\n","# Environment variables\n","runpod.api_key = userdata.get(RUNPOD_TOKEN)\n","GITHUB_API_TOKEN = userdata.get(GITHUB_TOKEN)\n","HUGGINGFACE_TOKEN = userdata.get(HF_TOKEN)\n","\n","# Create a pod\n","pod = runpod.create_pod(\n","    name=f\"Eval {MODEL_ID.split('/')[-1]} on {BENCHMARK.capitalize()}\",\n","    image_name=\"runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04\",\n","    gpu_type_id=GPU,\n","    cloud_type=CLOUD_TYPE,\n","    gpu_count=NUMBER_OF_GPUS,\n","    volume_in_gb=0,\n","    container_disk_in_gb=CONTAINER_DISK,\n","    template_id=\"au6nz6emhk\",\n","    env={\n","        \"BENCHMARK\": BENCHMARK,\n","        \"MODEL_ID\": MODEL_ID,\n","        \"REPO\": REPO,\n","        \"TRUST_REMOTE_CODE\": TRUST_REMOTE_CODE,\n","        \"PRIVATE_GIST\": PRIVATE_GIST,\n","        \"DEBUG\": DEBUG,\n","        \"GITHUB_API_TOKEN\": GITHUB_API_TOKEN,\n","        \"HUGGINGFACE_TOKEN\": HUGGINGFACE_TOKEN,\n","        \"LIGHT_EVAL_TASK\": LIGHTEVAL_TASK,\n","        \"NUMBER_OF_GPUS\": NUMBER_OF_GPUS\n","    }\n",")\n","\n","print(\"Pod started: https://www.runpod.io/console/pods\")"],"metadata":{"id":"elyxjYI_rY5W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d80047d-2ef1-4e8f-b46a-3f8dc63db31c","executionInfo":{"status":"ok","timestamp":1712267500364,"user_tz":-60,"elapsed":39502,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pod started: https://www.runpod.io/console/pods\n"]}]}]}