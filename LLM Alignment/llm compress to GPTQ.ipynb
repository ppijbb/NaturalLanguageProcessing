{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11dNvmHVvIzA9o74RtAYgyriSiy2N-5KM","authorship_tag":"ABX9TyPuSxHXq8cLrzcjBD/BcQYO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8hd80cw009j","executionInfo":{"status":"ok","timestamp":1770681678837,"user_tz":-540,"elapsed":3158,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"628d8003-be8b-40b7-850b-7b128f4a1325"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m54 packages\u001b[0m \u001b[2min 1.28s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m7 packages\u001b[0m \u001b[2min 766ms\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 767ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m7 packages\u001b[0m \u001b[2min 147ms\u001b[0m\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.13.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.3.7\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mllmcompressor\u001b[0m\u001b[2m==0.9.0.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-ml-py\u001b[0m\u001b[2m==13.590.44\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==5.0.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n"]}],"source":["!uv pip install llmcompressor compressed-tensors"]},{"cell_type":"code","source":["%%writefiel\n","import os\n","from pathlib import Path\n","from typing import Optional\n","\n","import torch  # type: ignore\n","from dotenv import load_dotenv\n","from huggingface_hub import HfApi  # type: ignore\n","from llmcompressor import oneshot  # type: ignore\n","from llmcompressor.modifiers.quantization import GPTQModifier\n","from llmcompressor.utils import dispatch_for_generation\n","from transformers import AutoProcessor, Gemma3ForConditionalGeneration  # type: ignore\n","\n","# .env 파일 로드\n","load_dotenv()\n","\n","# Gemma3 공식 예제: https://github.com/vllm-project/llm-compressor/blob/main/examples/multimodal_vision/gemma3_example.py\n","# AWQ는 Gemma3에서 알려진 이슈(linear weight / NaN) 있음 → GPTQ 사용 권장 (issue #2102)\n","DATASET_ID = \"flickr30k\"\n","NUM_CALIBRATION_SAMPLES = 768\n","MAX_SEQUENCE_LENGTH = 2048\n","DATASET_SPLIT = \"test\"\n","\n","\n","def calc_state_dict_params(module: torch.nn.Module) -> int:\n","    \"\"\"\n","    주어진 모듈의 state_dict 항목들을 순회하여 파라미터(요소) 수를 합산합니다.\n","    Tensor 항목은 .numel()로 계산하고, 다른 객체에 대해선 .numel()가 있으면 사용합니다.\n","    안전하게 예외를 처리하여 실패 시 해당 항목은 건너뜁니다.\n","    \"\"\"\n","    total = 0\n","    try:\n","        sd = module.state_dict()\n","    except Exception:\n","        # state_dict 접근 불가 시 0 반환\n","        return 0\n","    for v in sd.values():\n","        try:\n","            if isinstance(v, torch.Tensor):\n","                total += int(v.numel())\n","            else:\n","                # 다른 객체가 numel을 제공하면 사용\n","                if hasattr(v, \"numel\"):\n","                    total += int(v.numel())\n","                # 그 외는 무시\n","        except Exception:\n","            # 안전하게 진행\n","            pass\n","    return total\n","\n","\n","def format_params(n: int) -> str:\n","    \"\"\"\n","    파라미터 수를 사람이 읽기 쉬운 문자열로 변환.\n","    예: \"1,234,567 params (1.23M)\"\n","    \"\"\"\n","\n","    def abbrev(num: int) -> str:\n","        if num >= 1_000_000_000:\n","            return f\"{num / 1_000_000_000:.2f}B\"\n","        if num >= 1_000_000:\n","            return f\"{num / 1_000_000:.2f}M\"\n","        if num >= 1_000:\n","            return f\"{num / 1_000:.2f}K\"\n","        return str(num)\n","\n","    return f\"{n:,} params ({abbrev(n)})\"\n","\n","\n","def _resolve_chat_template_path() -> Optional[Path]:\n","    \"\"\"프로젝트 내 chat_template 파일 경로 반환 (있을 경우).\"\"\"\n","    candidates: list[Path] = [\n","        Path(__file__).resolve().parent.parent\n","        / \"template\"\n","        / \"chat_template\",\n","    ]\n","    env_path = os.getenv(\"CHAT_TEMPLATE_PATH\", \"\").strip()\n","    if env_path:\n","        candidates.append(Path(env_path))\n","    for p in candidates:\n","        if p.is_file():\n","            return p\n","    return None\n","\n","\n","def quantize_gemma3(\n","    model_name: str,\n","    save_directory: str,\n","    bits: int = 4,\n","    group_size: int = 128,\n","):\n","    \"\"\"\n","    Gemma3를 GPTQ(W4A16)로 양자화합니다.\n","    AWQ는 Gemma3에서 'linear has no attribute weight' / NaN 이슈가 있어 공식 예제대로 GPTQ 사용.\n","    \"\"\"\n","    # 1. 원본 모델·프로세서 로드\n","    print(f\"Loading model: {model_name}\")\n","    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n","    template_path = _resolve_chat_template_path()\n","    if template_path is not None:\n","        processor.chat_template = template_path.read_text()\n","\n","    model = Gemma3ForConditionalGeneration.from_pretrained(\n","        model_name,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        low_cpu_mem_usage=False,\n","    )\n","\n","    orig_params = 0\n","    try:\n","        orig_params = calc_state_dict_params(model)\n","        print(\"Model parameters (before quantization):\", format_params(orig_params))\n","    except Exception as e:\n","        print(f\"Could not compute original model parameter count: {e}\")\n","\n","    # 2. GPTQ 양자화 (Gemma3 공식 예제와 동일 레시피)\n","    print(\"Starting GPTQ quantization (Gemma3 recommended)...\")\n","    recipe = [\n","        GPTQModifier(\n","            targets=\"Linear\",\n","            scheme=\"W4A16\",\n","            ignore=[\n","                \"lm_head\",\n","                r\"re:model\\.vision_tower.*\",\n","                r\"re:model\\.multi_modal_projector.*\",\n","            ],\n","        ),\n","    ]\n","\n","    oneshot(\n","        model=model,\n","        processor=processor,\n","        dataset=DATASET_ID,\n","        splits={\"calibration\": f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\"},\n","        recipe=recipe,\n","        shuffle_calibration_samples=False,\n","        max_seq_length=MAX_SEQUENCE_LENGTH,\n","        num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n","        trust_remote_code_model=True,\n","    )\n","\n","    # 3. 생성용 디스패치 후 저장 (순서 중요: dispatch → 테스트 → save)\n","    dispatch_for_generation(model)\n","    model.eval()\n","\n","    try:\n","        quant_params = calc_state_dict_params(model)\n","        print(\"Model parameters (after quantization):\", format_params(quant_params))\n","        if orig_params:\n","            print(\n","                f\"Quantized: {format_params(orig_params)} -> {format_params(quant_params)}\"\n","            )\n","    except Exception as e:\n","        print(f\"Could not compute quantized parameter count: {e}\")\n","\n","    # 4. 저장 전 생성 테스트 (NaN/Inf 방지 확인)\n","    try:\n","        with torch.no_grad():\n","            print(\"##### [ Test Generation (before save) ] #####\")\n","            inputs = processor.apply_chat_template(\n","                [\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": [\n","                            {\n","                                \"type\": \"text\",\n","                                \"text\": \"안녕하세요. 테스트입니다. 짧게 답해주세요.\",\n","                            }\n","                        ],\n","                    }\n","                ],\n","                return_tensors=\"pt\",\n","                padding=False,\n","                truncation=True,\n","                max_length=MAX_SEQUENCE_LENGTH,\n","                tokenize=True,\n","                add_special_tokens=False,\n","                return_dict=True,\n","                add_generation_prompt=False,\n","            )\n","            generated = model.generate(\n","                input_ids=inputs[\"input_ids\"].to(model.device),\n","                attention_mask=inputs[\"attention_mask\"].to(model.device),\n","                max_new_tokens=10,\n","                do_sample=False,\n","                disable_compile=True,\n","            )\n","            out = processor.batch_decode(generated.cpu(), skip_special_tokens=True)\n","            if not out or \"nan\" in out.lower() or \"inf\" in out.lower():\n","                raise ValueError(\"Generation produced NaN/Inf or empty output.\")\n","            print(out)\n","    except Exception as e:\n","        print(f\"Generation test failed: {e}\")\n","        return None\n","\n","    model.save_pretrained(save_directory, save_compressed=True)\n","    processor.save_pretrained(save_directory)\n","    return model\n","\n","\n","@torch.inference_mode\n","def model_test(save_directory: str) -> None:\n","    \"\"\"양자화된 Gemma3 로드 후 생성 테스트. NaN/Inf 방지를 위해 동일 클래스·dtype·dispatch 사용.\"\"\"\n","    print(\"##### [ Test Generation (load from disk) ] #####\")\n","    model = Gemma3ForConditionalGeneration.from_pretrained(\n","        save_directory,\n","        device_map=\"auto\",\n","        torch_dtype=torch.bfloat16,\n","        trust_remote_code=True,\n","    )\n","    dispatch_for_generation(model)\n","    model.eval()\n","\n","    processor = AutoProcessor.from_pretrained(save_directory, trust_remote_code=True)\n","    inputs = processor.apply_chat_template(\n","        [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\n","                        \"type\": \"text\",\n","                        \"text\": \"안녕하세요. 테스트입니다. 짧게 답해주세요.\",\n","                    }\n","                ],\n","            }\n","        ],\n","        return_tensors=\"pt\",\n","        padding=False,\n","        truncation=True,\n","        max_length=MAX_SEQUENCE_LENGTH,\n","        tokenize=True,\n","        add_special_tokens=False,\n","        return_dict=True,\n","        add_generation_prompt=False,\n","    )\n","    generated = model.generate(\n","        input_ids=inputs[\"input_ids\"].to(model.device),\n","        attention_mask=inputs[\"attention_mask\"].to(model.device),\n","        max_new_tokens=10,\n","        do_sample=False,\n","        disable_compile=True,\n","    )\n","    out = processor.decode(generated.cpu(), skip_special_tokens=True)\n","    print(generated.shape)\n","    print(out)\n","\n","\n","def upload(\n","    save_directory: str,\n","    hf_repo_name: str,\n","    token: Optional[str] = None,\n","):  # 4. Hugging Face Hub에 업로드\n","    print(f\"Uploading to Hugging Face Hub: {hf_repo_name}\")\n","\n","    # Hugging Face 토큰 설정\n","    if token is None:\n","        token = os.getenv(\"HUGGINGFACE_TOKEN\")\n","\n","    if token is None:\n","        raise ValueError(\n","            \"Hugging Face token is required. Provide it as an argument or set HUGGINGFACE_TOKEN environment variable.\"\n","        )\n","\n","    # 레포지토리 생성 및 업로드\n","    api = HfApi(token=token)\n","\n","    # 레포지토리가 존재하지 않으면 생성\n","    try:\n","        api.create_repo(repo_id=hf_repo_name, private=False, exist_ok=True)\n","    except Exception as e:\n","        print(f\"Repository creation failed (may already exist): {e}\")\n","\n","    # 파일 업로드\n","    api.upload_folder(\n","        folder_path=save_directory, repo_id=hf_repo_name, repo_type=\"model\", token=token\n","    )\n","\n","    print(f\"Successfully uploaded quantized model to {hf_repo_name}\")\n","\n","\n","if __name__ == \"__main__\":\n","    MODEL_NAME = os.getenv(\"QUANTIZE_MODEL_NAME\", \"Gunulhona/Gemma-3-27B-v2\")\n","    SAVE_DIR = os.getenv(\"QUANTIZE_SAVE_DIR\", \"quantized_model\")\n","    HF_REPO_NAME = os.getenv(\"QUANTIZE_HF_REPO\", \"Gunulhona/Gemma-3-27B-v2-w4a16\")\n","\n","    model = quantize_gemma3(\n","        model_name=MODEL_NAME,\n","        save_directory=SAVE_DIR,\n","        bits=4,\n","        group_size=128,\n","    )\n","    if model is not None:\n","        model_test(save_directory=SAVE_DIR)\n","        token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n","        if token:\n","            upload(save_directory=SAVE_DIR, hf_repo_name=HF_REPO_NAME, token=token)\n","        else:\n","            print(\"HF_TOKEN not set; skipping upload.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"dzAjkg7y2cev","executionInfo":{"status":"error","timestamp":1770682384125,"user_tz":-540,"elapsed":6257,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"ace07ab2-9fc7-4a10-8067-4a8396502df0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model: Gunulhona/Gemma-3-27B-v2\n"]},{"output_type":"error","ename":"NameError","evalue":"name '__file__' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-286800465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mHF_REPO_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QUANTIZE_HF_REPO\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Gunulhona/Gemma-3-27B-v2-w4a16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     model = quantize_gemma3(\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-286800465.py\u001b[0m in \u001b[0;36mquantize_gemma3\u001b[0;34m(model_name, save_directory, bits, group_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtemplate_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_chat_template_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtemplate_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-286800465.py\u001b[0m in \u001b[0;36m_resolve_chat_template_path\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m\"\"\"프로젝트 내 chat_template 파일 경로 반환 (있을 경우).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     candidates: list[Path] = [\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;34m/\u001b[0m \u001b[0;34m\"template\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"]}]}]}