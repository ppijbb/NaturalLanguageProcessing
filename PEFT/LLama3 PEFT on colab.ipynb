{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyMo6Iv5dRj5pQCvukEmJ7Wx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi6ZScMwq0Lj","executionInfo":{"status":"ok","timestamp":1719965246178,"user_tz":-540,"elapsed":1194,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"f94a386f-7bce-417e-a29a-804414754c8a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jul  3 00:07:25 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]\n","deepspeed\n","colossalai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1719977228325,"user_tz":-540,"elapsed":306,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"5be7cdb0-851e-43db-d8f2-ea31ce09f309"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!CUDA_EXT=1 DS_BUILD=1 pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf","executionInfo":{"status":"ok","timestamp":1719977307986,"user_tz":-540,"elapsed":79380,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login   --add-to-git-credential\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iCRpjtCdPPRx","executionInfo":{"status":"ok","timestamp":1719977468194,"user_tz":-540,"elapsed":18689,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"781f3a25-a421-4f53-d1d9-efceb5bb9eef"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown      LLama3-8B => OOM\n","#@markdown      Mistral-7B => OOM\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"meta-llama/Meta-Llama-3-8B\" # @param [\"Gunulhona/tb_pretrained_sts\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1719980524294,"user_tz":-540,"elapsed":309,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"00d02900-8a1d-485c-c182-ed1c1fc86f4a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 85))\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 90))\n","\n","\n","def preprocess_function(sample, max_source_length, max_target_length, padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","dataset = dataset.map(preprocess_function,\n","                      batched=True,\n","                      remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                      fn_kwargs={\n","                          \"max_source_length\": max_source_length,\n","                           \"max_target_length\": max_source_length\n","                          },)\n","\n","if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    dataset = dataset.map(lambda x: x,\n","                          batched=True,\n","                          remove_columns=[\"token_type_ids\"], )\n"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1719977478877,"user_tz":-540,"elapsed":297,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d580f48b-e5c5-4d2d-fc94-c17bd53d0008","cellView":"form"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"adamw_hf\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1719977481360,"user_tz":-540,"elapsed":312,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"010f25cc-36f3-4fb0-dc1a-111ad7cc8dec","cellView":"form"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iX1Vp8-2PQN8","executionInfo":{"status":"ok","timestamp":1719965581403,"user_tz":-540,"elapsed":136994,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"2fbbebfc-5f3f-4066-e580-c14f6f7d81a1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-03 00:10:50.796469: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-07-03 00:10:50.850356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-03 00:10:50.850417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-03 00:10:50.852926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-03 00:10:50.860938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-03 00:10:52.046736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","config.json: 100% 1.07k/1.07k [00:00<00:00, 5.73MB/s]\n","model.safetensors: 100% 495M/495M [00:01<00:00, 409MB/s]\n","Some weights of BartForCausalLM were not initialized from the model checkpoint at Gunulhona/tb_pretrained_sts and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","tokenizer_config.json: 100% 39.2k/39.2k [00:00<00:00, 38.1MB/s]\n","tokenizer.json: 100% 1.05M/1.05M [00:00<00:00, 4.69MB/s]\n","special_tokens_map.json: 100% 692/692 [00:00<00:00, 4.66MB/s]\n","Downloading builder script: 100% 3.36k/3.36k [00:00<00:00, 17.3MB/s]\n","Downloading readme: 100% 7.04k/7.04k [00:00<00:00, 18.8MB/s]\n","Downloading data: 100% 2.94M/2.94M [00:00<00:00, 73.6MB/s]\n","Generating train split: 100% 14732/14732 [00:00<00:00, 18364.54 examples/s]\n","Generating test split: 100% 819/819 [00:00<00:00, 3131.93 examples/s]\n","Generating validation split: 100% 818/818 [00:00<00:00, 3154.04 examples/s]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 21.9MB/s]\n","Map:   0% 0/15551 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Map: 100% 15551/15551 [00:02<00:00, 6271.06 examples/s]\n","Map: 100% 15551/15551 [00:00<00:00, 18063.94 examples/s]\n","Map: 100% 14732/14732 [00:13<00:00, 1126.52 examples/s]\n","Map: 100% 819/819 [00:00<00:00, 1155.38 examples/s]\n","Map: 100% 818/818 [00:00<00:00, 1163.45 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","  0% 10/58928 [00:01<1:36:09, 10.21it/s]\n","  0% 0/819 [00:00<?, ?it/s]\u001b[A\n","  1% 9/819 [00:00<00:09, 87.80it/s]\u001b[A\n","  2% 18/819 [00:00<00:10, 74.67it/s]\u001b[A\n","  3% 26/819 [00:00<00:11, 69.87it/s]\u001b[A\n","  4% 34/819 [00:00<00:12, 64.80it/s]\u001b[A\n","  5% 41/819 [00:01<00:27, 28.15it/s]\u001b[A\n","  6% 46/819 [00:01<00:35, 21.96it/s]\u001b[A\n","  6% 50/819 [00:01<00:43, 17.78it/s]\u001b[A\n","  6% 53/819 [00:02<00:47, 16.09it/s]\u001b[A\n","  7% 56/819 [00:02<00:53, 14.34it/s]\u001b[A\n","  7% 58/819 [00:02<00:54, 14.04it/s]\u001b[A\n","  7% 60/819 [00:03<01:24,  9.03it/s]\u001b[A\n","  8% 63/819 [00:03<01:12, 10.43it/s]\u001b[A\n","  8% 65/819 [00:03<01:12, 10.46it/s]\u001b[A\n","  8% 68/819 [00:03<01:20,  9.33it/s]\u001b[A\n","  9% 70/819 [00:04<01:21,  9.15it/s]\u001b[A\n","  9% 72/819 [00:04<01:21,  9.16it/s]\u001b[A\n","  9% 74/819 [00:04<01:11, 10.39it/s]\u001b[A\n","  9% 76/819 [00:05<01:58,  6.28it/s]\u001b[A\n"," 10% 79/819 [00:05<01:33,  7.87it/s]\u001b[A\n"," 10% 81/819 [00:05<02:18,  5.34it/s]\u001b[A\n"," 10% 83/819 [00:06<01:53,  6.49it/s]\u001b[A\n"," 11% 86/819 [00:06<02:11,  5.59it/s]\u001b[A\n"," 11% 88/819 [00:06<01:49,  6.65it/s]\u001b[A\n"," 11% 91/819 [00:07<02:04,  5.84it/s]\u001b[A\n"," 11% 92/819 [00:07<02:07,  5.71it/s]\u001b[A\n"," 12% 96/819 [00:08<02:11,  5.50it/s]\u001b[A\n"," 12% 98/819 [00:08<01:52,  6.40it/s]\u001b[A\n"," 12% 100/819 [00:09<02:16,  5.26it/s]\u001b[A\n"," 12% 101/819 [00:09<02:13,  5.39it/s]\u001b[A\n"," 13% 104/819 [00:09<01:58,  6.04it/s]\u001b[A\n"," 13% 105/819 [00:10<02:24,  4.93it/s]\u001b[A\n"," 13% 108/819 [00:11<02:45,  4.30it/s]\u001b[A\n","  0% 10/58928 [00:13<1:36:09, 10.21it/s]\n"," 14% 113/819 [00:11<02:39,  4.43it/s]\u001b[A\n"," 14% 116/819 [00:12<02:56,  3.98it/s]\u001b[A\n"," 15% 119/819 [00:13<02:53,  4.04it/s]\u001b[A\n"," 15% 122/819 [00:14<02:51,  4.06it/s]\u001b[A\n"," 15% 125/819 [00:15<02:52,  4.02it/s]\u001b[A\n"," 16% 128/819 [00:15<02:53,  3.97it/s]\u001b[A\n"," 16% 131/819 [00:16<02:55,  3.93it/s]\u001b[A\n"," 16% 134/819 [00:17<02:57,  3.85it/s]\u001b[A\n"," 17% 137/819 [00:18<02:59,  3.80it/s]\u001b[A\n"," 17% 140/819 [00:19<03:02,  3.72it/s]\u001b[A\n"," 17% 143/819 [00:19<03:04,  3.66it/s]\u001b[A\n"," 18% 146/819 [00:20<03:07,  3.58it/s]\u001b[A\n"," 18% 148/819 [00:21<03:09,  3.53it/s]\u001b[A\n"," 18% 150/819 [00:22<03:12,  3.48it/s]\u001b[A\n"," 19% 152/819 [00:22<03:14,  3.43it/s]\u001b[A\n"," 19% 154/819 [00:23<03:17,  3.37it/s]\u001b[A\n"," 19% 156/819 [00:23<03:20,  3.31it/s]\u001b[A\n"," 19% 158/819 [00:24<03:23,  3.25it/s]\u001b[A\n"," 20% 160/819 [00:25<03:26,  3.20it/s]\u001b[A\n"," 20% 162/819 [00:25<03:29,  3.14it/s]\u001b[A\n"," 20% 164/819 [00:26<03:31,  3.10it/s]\u001b[A\n"," 20% 166/819 [00:27<03:33,  3.06it/s]\u001b[A\n"," 21% 168/819 [00:27<03:35,  3.02it/s]\u001b[A\n"," 21% 170/819 [00:28<03:37,  2.98it/s]\u001b[A\n"," 21% 172/819 [00:29<03:39,  2.94it/s]\u001b[A\n"," 21% 174/819 [00:29<03:41,  2.91it/s]\u001b[A\n"," 21% 176/819 [00:30<03:43,  2.88it/s]\u001b[A\n"," 22% 178/819 [00:31<03:45,  2.84it/s]\u001b[A\n"," 22% 180/819 [00:32<03:46,  2.82it/s]\u001b[A\n"," 22% 182/819 [00:32<03:48,  2.78it/s]\u001b[A\n"," 22% 184/819 [00:33<03:50,  2.76it/s]\u001b[A\n"," 23% 186/819 [00:34<03:52,  2.73it/s]\u001b[A\n"," 23% 188/819 [00:35<03:53,  2.70it/s]\u001b[A\n"," 23% 190/819 [00:35<03:55,  2.67it/s]\u001b[A\n"," 23% 192/819 [00:36<03:57,  2.64it/s]\u001b[A\n"," 24% 194/819 [00:37<04:00,  2.60it/s]\u001b[A\n"," 24% 195/819 [00:37<04:03,  2.57it/s]\u001b[A\n"," 24% 196/819 [00:38<04:05,  2.54it/s]\u001b[A\n"," 24% 197/819 [00:38<04:07,  2.51it/s]\u001b[A\n"," 24% 198/819 [00:39<04:09,  2.49it/s]\u001b[A\n"," 24% 199/819 [00:39<04:11,  2.46it/s]\u001b[A\n"," 24% 200/819 [00:39<04:13,  2.44it/s]\u001b[A\n"," 25% 201/819 [00:40<04:15,  2.42it/s]\u001b[A\n"," 25% 202/819 [00:40<04:16,  2.41it/s]\u001b[A\n"," 25% 203/819 [00:41<04:17,  2.39it/s]\u001b[A\n"," 25% 204/819 [00:41<04:18,  2.38it/s]\u001b[A\n"," 25% 205/819 [00:42<04:19,  2.36it/s]\u001b[A\n"," 25% 206/819 [00:42<04:20,  2.35it/s]\u001b[A\n"," 25% 207/819 [00:42<04:21,  2.34it/s]\u001b[A\n"," 25% 208/819 [00:43<04:22,  2.33it/s]\u001b[A\n"," 26% 209/819 [00:43<04:23,  2.31it/s]\u001b[A\n"," 26% 210/819 [00:44<04:24,  2.30it/s]\u001b[A\n"," 26% 211/819 [00:44<04:25,  2.29it/s]\u001b[A\n"," 26% 212/819 [00:45<04:25,  2.28it/s]\u001b[A\n"," 26% 213/819 [00:45<04:26,  2.27it/s]\u001b[A\n"," 26% 214/819 [00:46<04:27,  2.26it/s]\u001b[A\n"," 26% 215/819 [00:46<04:28,  2.25it/s]\u001b[A\n"," 26% 216/819 [00:46<04:29,  2.24it/s]\u001b[A\n"," 26% 217/819 [00:47<04:29,  2.23it/s]\u001b[A\n"," 27% 218/819 [00:47<04:30,  2.22it/s]\u001b[A\n"," 27% 219/819 [00:48<04:31,  2.21it/s]\u001b[A\n"," 27% 220/819 [00:48<04:32,  2.20it/s]\u001b[A\n"," 27% 221/819 [00:49<04:33,  2.19it/s]\u001b[A\n"," 27% 222/819 [00:49<04:33,  2.18it/s]\u001b[A\n"," 27% 223/819 [00:50<04:34,  2.17it/s]\u001b[A\n"," 27% 224/819 [00:50<04:35,  2.16it/s]\u001b[A\n"," 27% 225/819 [00:51<04:36,  2.14it/s]\u001b[A\n"," 28% 226/819 [00:51<04:37,  2.14it/s]\u001b[A\n"," 28% 227/819 [00:52<04:38,  2.12it/s]\u001b[A\n"," 28% 228/819 [00:52<04:38,  2.12it/s]\u001b[A\n"," 28% 229/819 [00:52<04:39,  2.11it/s]\u001b[A\n"," 28% 230/819 [00:53<04:39,  2.11it/s]\u001b[A\n"," 28% 231/819 [00:53<04:40,  2.10it/s]\u001b[A\n"," 28% 232/819 [00:54<04:40,  2.09it/s]\u001b[A\n"," 28% 233/819 [00:54<04:41,  2.08it/s]\u001b[A\n"," 29% 234/819 [00:55<04:41,  2.08it/s]\u001b[A\n"," 29% 235/819 [00:55<04:42,  2.07it/s]\u001b[A\n"," 29% 236/819 [00:56<04:43,  2.06it/s]\u001b[A\n"," 29% 237/819 [00:56<04:44,  2.05it/s]\u001b[A\n"," 29% 238/819 [00:57<04:44,  2.04it/s]\u001b[A\n"," 29% 239/819 [00:57<04:45,  2.03it/s]\u001b[A\n"," 29% 240/819 [00:58<04:46,  2.02it/s]\u001b[A\n"," 29% 241/819 [00:58<04:46,  2.02it/s]\u001b[A\n"," 30% 242/819 [00:59<04:47,  2.01it/s]\u001b[A\n"," 30% 243/819 [00:59<04:47,  2.00it/s]\u001b[A\n"," 30% 244/819 [01:00<04:48,  1.99it/s]\u001b[A\n"," 30% 245/819 [01:00<04:48,  1.99it/s]\u001b[A\n"," 30% 246/819 [01:01<04:49,  1.98it/s]\u001b[A\n"," 30% 247/819 [01:01<04:50,  1.97it/s]\u001b[A\n"," 30% 248/819 [01:02<04:51,  1.96it/s]\u001b[A\n"," 30% 249/819 [01:02<04:52,  1.95it/s]\u001b[A\n"," 31% 250/819 [01:03<04:52,  1.94it/s]\u001b[A\n"," 31% 251/819 [01:03<04:53,  1.94it/s]\u001b[A\n"," 31% 252/819 [01:04<04:53,  1.93it/s]\u001b[A\n"," 31% 253/819 [01:05<04:54,  1.92it/s]\u001b[A\n"," 31% 254/819 [01:05<04:54,  1.92it/s]\u001b[A\n"," 31% 255/819 [01:06<04:55,  1.91it/s]\u001b[A\n"," 31% 256/819 [01:06<04:55,  1.90it/s]\u001b[A\n"," 31% 257/819 [01:07<04:56,  1.89it/s]\u001b[A\n"," 32% 258/819 [01:07<04:56,  1.89it/s]\u001b[A\n"," 32% 259/819 [01:08<04:57,  1.88it/s]\u001b[A\n"," 32% 260/819 [01:08<04:57,  1.88it/s]\u001b[A\n"," 32% 261/819 [01:09<04:58,  1.87it/s]\u001b[A\n"," 32% 262/819 [01:09<04:59,  1.86it/s]\u001b[A\n"," 32% 263/819 [01:10<04:59,  1.86it/s]\u001b[A\n"," 32% 264/819 [01:10<05:00,  1.85it/s]\u001b[A\n"," 32% 265/819 [01:11<05:00,  1.84it/s]\u001b[A\n"," 32% 266/819 [01:11<05:01,  1.84it/s]\u001b[A\n"," 33% 267/819 [01:12<05:01,  1.83it/s]\u001b[A\n"," 33% 268/819 [01:13<05:02,  1.82it/s]\u001b[A\n"," 33% 269/819 [01:13<05:03,  1.81it/s]\u001b[A\n"," 33% 270/819 [01:14<05:04,  1.81it/s]\u001b[A\n"," 33% 271/819 [01:14<05:04,  1.80it/s]\u001b[A\n"," 33% 272/819 [01:15<05:05,  1.79it/s]\u001b[A\n"," 33% 273/819 [01:15<05:05,  1.79it/s]\u001b[A\n"," 33% 274/819 [01:16<05:05,  1.78it/s]\u001b[A\n"," 34% 275/819 [01:17<05:06,  1.78it/s]\u001b[A\n"," 34% 276/819 [01:17<05:06,  1.77it/s]\u001b[A\n"," 34% 277/819 [01:18<05:07,  1.76it/s]\u001b[A\n"," 34% 278/819 [01:18<05:07,  1.76it/s]\u001b[A\n"," 34% 279/819 [01:19<05:08,  1.75it/s]\u001b[A\n"," 34% 280/819 [01:19<05:08,  1.75it/s]\u001b[A\n"," 34% 281/819 [01:20<05:09,  1.74it/s]\u001b[A\n"," 34% 282/819 [01:21<05:09,  1.73it/s]\u001b[A\n"," 35% 283/819 [01:21<05:10,  1.73it/s]\u001b[A\n"," 35% 284/819 [01:22<05:10,  1.72it/s]\u001b[A\n"," 35% 285/819 [01:22<05:11,  1.72it/s]\u001b[A\n"," 35% 286/819 [01:23<05:11,  1.71it/s]\u001b[A\n"," 35% 287/819 [01:23<05:12,  1.71it/s]\u001b[A\n"," 35% 288/819 [01:24<05:12,  1.70it/s]\u001b[A\n"," 35% 289/819 [01:25<05:12,  1.70it/s]\u001b[ATraceback (most recent call last):\n","  File \"/content/train.py\", line 83, in <module>\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1876, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2311, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2721, in _maybe_log_save_evaluate\n","    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3572, in evaluate\n","    output = eval_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3779, in evaluation_loop\n","    all_preds.add(logits)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 326, in add\n","    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 140, in nested_concat\n","    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 99, in torch_pad_and_concatenate\n","    return torch.cat((tensor1, tensor2), dim=0)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.66 GiB. GPU \n","  0%|          | 10/58928 [01:28<144:26:52,  8.83s/it]\n","\n","                                     \u001b[A"]}]},{"cell_type":"markdown","source":["## Training code to Lightning module"],"metadata":{"id":"d5KC2KSMllGB"}},{"cell_type":"code","source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","\n","import lightning as L\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, data_collator, train_batch_size, eval_batch_size,training_args,):\n","        super().__init__()\n","        self.train_dataset = dataset[\"train\"]\n","        self.val_dataset = dataset[\"validation\"]\n","        self.test_dataset = dataset[\"test\"]\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSF7QaH1zhQ9","executionInfo":{"status":"ok","timestamp":1719977485656,"user_tz":-540,"elapsed":299,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"c44399e4-ce9d-4f9a-d203-cddb54adc08f","cellView":"form"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_datamodule.py\n"]}]},{"cell_type":"code","source":["#@title Lightning Model\n","%%writefile l_model.py\n","\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, data_collator, training_args):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.training_args = training_args\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss,\n","                 prog_bar=True, on_step=True, on_epoch=False)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss,\n","                 prog_bar=True, on_step=True, on_epoch=False)\n","\n","    # def configure_optimizers(self):\n","    #     return torch.optim.AdamW(self.model.parameters(),\n","    #                              lr=self.training_args.learning_rate)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agrktrAylkal","executionInfo":{"status":"ok","timestamp":1719978731454,"user_tz":-540,"elapsed":333,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"77fddafb-8abb-4352-f1a7-cd076e47d2ed","cellView":"form"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_model.py\n"]}]},{"cell_type":"code","source":["#@title Trainer\n","%%writefile l_trainer.py\n","\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","\n","if __name__ == \"__main__\":\n","    L.pytorch.cli_lightning_logo()\n","    training_args = LightningArgumentParser()\n","    cli = LightningCLI(model_class=LLamaFTLightningModule,\n","                       datamodule_class=FTDataModule,\n","                       seed_everything_default=42,)\n","    # cli.add_arguments_to_parser(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZzH2TmzqP-","executionInfo":{"status":"ok","timestamp":1719980478870,"user_tz":-540,"elapsed":304,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"69ac6f64-0e19-41fe-9a20-33155343fa90","cellView":"form"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_trainer.py\n"]}]},{"cell_type":"code","source":["#@title Start Training\n","#@markdown 실험 결과\n","#@markdown\n","#@markdown * batch_size <b>2</b> 넘기는 경우 OOM\n","%%shell\n","\n","python l_trainer.py fit \\\n","    --trainer.max_epochs 4 \\\n","    --data.train_batch_size 2 \\\n","    --data.eval_batch_size 2 \\\n","    --optimizer torch.optim.AdamW \\\n","    --optimizer.lr 2e-5\n"],"metadata":{"id":"RYALPHD3PyBD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"879058db-001a-41b4-c990-b42acdcd9b3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-03 04:43:19.062288: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-07-03 04:43:19.114207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-03 04:43:19.114271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-03 04:43:19.115724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-03 04:43:19.123645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-03 04:43:20.342051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.34s/it]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","Seed set to 42\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name  | Type             | Params | Mode\n","--------------------------------------------------\n","0 | model | LlamaForCausalLM | 8.0 B  | eval\n","--------------------------------------------------\n","3.4 M     Trainable params\n","8.0 B     Non-trainable params\n","8.0 B     Total params\n","32,134.676Total estimated model params size (MB)\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","Epoch 0:  34% 2520/7366 [39:41<1:16:19,  1.06it/s, v_num=6, train_loss=7.720]"]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}