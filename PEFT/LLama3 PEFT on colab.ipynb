{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyPngLqvT9jV7W9tHg/I1OyO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi6ZScMwq0Lj","executionInfo":{"status":"ok","timestamp":1720136458261,"user_tz":-540,"elapsed":1021,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"8c1c5cfc-19b8-460e-9d8d-8c6a7318c771"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul  4 23:40:57 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   52C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]\n","deepspeed\n","colossalai\n","wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1720587432373,"user_tz":-540,"elapsed":4,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"6a87f12d-b8fc-4afb-c7b7-3559c24dc7ee"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!CUDA_EXT=1 DS_BUILD=1 pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf","executionInfo":{"status":"ok","timestamp":1720587616767,"user_tz":-540,"elapsed":183829,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login --add-to-git-credential\n"],"metadata":{"id":"iCRpjtCdPPRx","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720587968250,"user_tz":-540,"elapsed":351488,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b0eb4172-3486-4db8-f8bf-8ada0dd71fcd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["#@title Weight and Bias Train Logger Login\n","#@markdown weight and bias 로그인\n","!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"3i2AiD0181GO","executionInfo":{"status":"ok","timestamp":1720587980700,"user_tz":-540,"elapsed":5540,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"3548027a-5eb9-4883-efed-ade2016993ad"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown  |모델       | Normal   | DeepSpeed |\n","#@markdown  |---        | ---      | ---       |\n","#@markdown  |Llama3-8B  |  X       |   O       |\n","#@markdown  |Mistral-7B |  X       |   O       |\n","#@markdown  |Llama3-70B |  X       |   X       |\n","\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"Qwen/Qwen2-1.5B-Instruct\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1720589993687,"user_tz":-540,"elapsed":479,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"bfb5a586-5312-4582-ca53-73a0a852a856"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 85))\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 90))\n","\n","\n","def preprocess_function(sample, max_source_length, max_target_length, padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"],\n","                       max_length=max_target_length,\n","                       padding=padding,\n","                       truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else 1) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","dataset = dataset.map(preprocess_function,\n","                      batched=True,\n","                      remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                      fn_kwargs={\n","                          \"max_source_length\": max_source_length,\n","                           \"max_target_length\": max_source_length\n","                          },)\n","\n","if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    dataset = dataset.map(lambda x: x,\n","                          batched=True,\n","                          remove_columns=[\"token_type_ids\"], )\n"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1720587984571,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66b64b2f-0712-482b-a273-faca8b7a5da3","cellView":"form"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"adamw_hf\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1720587985865,"user_tz":-540,"elapsed":688,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"316cebd6-459b-429b-a797-8e8acfb49f6f","cellView":"form"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"id":"iX1Vp8-2PQN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training code to Lightning module"],"metadata":{"id":"d5KC2KSMllGB"}},{"cell_type":"code","source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","\n","import lightning as L\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, data_collator, train_batch_size, eval_batch_size,training_args,):\n","        super().__init__()\n","        self.train_dataset = dataset[\"train\"]\n","        self.val_dataset = dataset[\"validation\"]\n","        self.test_dataset = dataset[\"test\"]\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          num_workers=8,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSF7QaH1zhQ9","executionInfo":{"status":"ok","timestamp":1720587987913,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"9e64a552-76a8-4bcd-bb29-241cf3382d3d","cellView":"form"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_datamodule.py\n"]}]},{"cell_type":"code","source":["#@title Lightning Model\n","%%writefile l_model.py\n","\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from bitsandbytes.optim import AdamW, Lion\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","from finetuning_datasets import dataset\n","from torchmetrics.functional.text.rouge import rouge_score\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, data_collator, learning_rate: float = 2e-5):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.learning_rate = learning_rate\n","\n","    def _get_rouge_score(self, predictions, labels):\n","        generated_tokens = predictions.argmax(dim=-1)\n","        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        return rouge_score(preds=decoded_preds, target=decoded_labels)\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss,\n","                 prog_bar=True, on_step=True, on_epoch=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"train_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=True, on_epoch=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss,\n","                 prog_bar=True, on_step=True, on_epoch=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"val_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=False, on_epoch=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = Lion(params=self.model.parameters(),\n","                         lr=self.learning_rate,\n","                         weight_decay=0.01,\n","                         optim_bits=32,)\n","        scheduler = CosineAnnealingWarmRestarts(optimizer,\n","                                                T_0=10,\n","                                                T_mult=2,\n","                                                eta_min=0.00001)\n","        # scheduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"min\")\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"monitor\": \"val_loss\",\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","\n","            },\n","        }\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agrktrAylkal","executionInfo":{"status":"ok","timestamp":1720587988537,"user_tz":-540,"elapsed":1,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"45c935bd-c163-4308-9095-ada92c9ffb6c","cellView":"form"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_model.py\n"]}]},{"cell_type":"code","source":["#@title Trainer\n","%%writefile l_trainer.py\n","\n","import os\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n","from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","\n","\n","if __name__ == \"__main__\":\n","    L.pytorch.cli_lightning_logo()\n","    training_args = LightningArgumentParser()\n","    cli = LightningCLI(\n","        model_class=LLamaFTLightningModule,\n","        datamodule_class=FTDataModule,\n","        seed_everything_default=42,\n","        trainer_defaults={\n","            \"reload_dataloaders_every_n_epochs\": 1,\n","            \"strategy\": \"deepspeed\",\n","            \"precision\": \"bf16-mixed\",\n","            \"profiler\": \"PassThroughProfiler\",\n","            \"logger\": [WandbLogger(project=\"LLM-Finetuning\"),],\n","            \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=5), LearningRateMonitor()]\n","        },\n","        save_config_callback=None)\n","    # cli.add_arguments_to_parser(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZzH2TmzqP-","executionInfo":{"status":"ok","timestamp":1720587989741,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"fba723be-ba22-41c5-ca30-43de275ba292","cellView":"form"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_trainer.py\n"]}]},{"cell_type":"code","source":["#@title Start Training\n","#@markdown 실험 결과\n","#@markdown\n","#@markdown * batch_size <b>2</b> 넘기는 경우 OOM\n","#@markdown * DeepSpeed의 경우 GPU Ram 20GB로 7B finetuning 가능\n","#@markdown * DeepSpeed의 경우, 7B L4 GPU에서 사용 가능\n","#@markdown * 70B의 경우 RAM에서 Weight 가져오다 OOM\n","%%shell\n","\n","python l_trainer.py fit \\\n","    --trainer.max_epochs 4 \\\n","    --model.learning_rate 5e-5 \\\n","    --data.train_batch_size 2 \\\n","    --data.eval_batch_size 2\n","\n","#    --trainer.fast_dev_run 1\\"],"metadata":{"id":"RYALPHD3PyBD","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","outputId":"7d8e3ca9-f875-4b35-def3-91fe2ee8cd80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-10 05:40:02.264868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-10 05:40:02.264921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-10 05:40:02.266283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-10 05:40:03.300207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","config.json: 100% 660/660 [00:00<00:00, 4.04MB/s]\n","model.safetensors: 100% 3.09G/3.09G [01:46<00:00, 29.0MB/s]\n","generation_config.json: 100% 242/242 [00:00<00:00, 1.81MB/s]\n","tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 9.94MB/s]\n","vocab.json: 100% 2.78M/2.78M [00:00<00:00, 3.27MB/s]\n","merges.txt: 100% 1.67M/1.67M [00:00<00:00, 2.39MB/s]\n","tokenizer.json: 100% 7.03M/7.03M [00:01<00:00, 5.72MB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Map: 100% 15551/15551 [00:01<00:00, 7968.07 examples/s]\n","Map: 100% 15551/15551 [00:00<00:00, 19837.17 examples/s]\n","Map: 100% 14732/14732 [00:07<00:00, 2048.81 examples/s]\n","Map: 100% 819/819 [00:00<00:00, 2045.11 examples/s]\n","Map: 100% 818/818 [00:00<00:00, 2097.50 examples/s]\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","Seed set to 42\n","/usr/local/lib/python3.10/dist-packages/jsonargparse/_typehints.py:1439: JsonargparseWarning: \n","    Unable to serialize instance <lightning.pytorch.loggers.wandb.WandbLogger object at 0x7888521290f0>\n","\n","  warning(val)\n","[2024-07-10 05:42:17,157] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","[rank: 0] Seed set to 42\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevintb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240710_054219-wm9ojnp1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-blaze-15\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/wm9ojnp1\u001b[0m\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-10 05:42:22,770] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type             | Params | Mode\n","--------------------------------------------------\n","0 | model | Qwen2ForCausalLM | 1.5 B  | eval\n","--------------------------------------------------\n","1.1 M     Trainable params\n","1.5 B     Non-trainable params\n","1.5 B     Total params\n","6,179.215 Total estimated model params size (MB)\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Sanity Checking DataLoader 0: 100% 2/2 [00:03<00:00,  2.00s/it]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","Epoch 0:  10% 720/7366 [18:26<2:50:12,  1.54s/it, v_num=jnp1, train_loss_step=0.930, train_rouge1_fmeasure_step=0.0455, train_rouge1_precision_step=0.250, train_rouge1_recall_step=0.025, train_rouge2_fmeasure_step=0.000, train_rouge2_precision_step=0.000, train_rouge2_recall_step=0.000, train_rougeL_fmeasure_step=0.0455, train_rougeL_precision_step=0.250, train_rougeL_recall_step=0.025, train_rougeLsum_fmeasure_step=0.0455, train_rougeLsum_precision_step=0.250, train_rougeLsum_recall_step=0.025]   "]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}