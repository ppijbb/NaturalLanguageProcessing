{"cells":[{"cell_type":"markdown","metadata":{"id":"nXC6xmOlFtyz"},"source":["# 1. Package Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1721792055936,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"vi6ZScMwq0Lj","outputId":"0f67c01a-97d2-45d5-d130-bbfb0ab60b07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Jul 24 03:34:15 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1722382914422,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"KpkmrFpqYISS","outputId":"fc8d371d-d53c-4a99-f87e-b7be954ee2f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","peft>=0.12.0\n","accelerator\n","transformers>=4.39.0\n","datasets\n","fire\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters,onnxruntime]\n","onnx==1.16.0\n","onnxruntime>=1.18.1\n","trl~=0.9.6\n","lightning~=2.1.4\n","jsonargparse[signatures]\n","deepspeed>=0.14.4\n","colossalai>=0.4.1\n","wandb~=0.17.5\n","tensorrt>=10.2.0\n","ray[all]>=2.33.0\n","protobuf>=4.25.4\n","tensorboardx>=2.6.2.2\n","opentelemetry-proto>=1.26.0\n","--extra-index-url https://pypi.nvidia.com\n","nvidia-modelopt"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qt3RP1onC5Uf"},"outputs":[],"source":["#@title Install Packages\n","%%capture\n","!BUILD_EXT=1 pip install colossalai>=0.4.1\n","!CUDA_EXT=1 DS_BUILD=1 pip install --pre -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1008336,"status":"ok","timestamp":1722387462538,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"iCRpjtCdPPRx","outputId":"ce3c2ded-ca4c-4abe-e1da-1f9f6ea834b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","from google.colab import userdata\n","import os\n","\n","os.environ['HF_WRITE_TOKEN'] = userdata.get('HF_WRITE_TOKEN')\n","\n","!huggingface-cli login --add-to-git-credential --token $HF_WRITE_TOKEN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7291,"status":"ok","timestamp":1722387469824,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"3i2AiD0181GO","outputId":"f43de794-6025-4070-80f2-00ce8ae65ab5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["#@title Weight and Bias Train Logger Login\n","#@markdown weight and bias 로그인\n","from google.colab import userdata\n","import os\n","\n","os.environ['WANDB_TOKEN'] = userdata.get('WANDB_TOKEN')\n","\n","!huggingface-cli login --add-to-git-credential --with-token\n","\n","!wandb login $WANDB_TOKEN"]},{"cell_type":"markdown","metadata":{"id":"IfgXZtBZFyVE"},"source":["# 2. Load Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1722400872185,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"RO8x9zxrFb1-","outputId":"5ff747ab-4fa0-4743-c36a-636adae8ea95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting peft_model.py\n"]}],"source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown  |모델       | Normal   | DeepSpeed |\n","#@markdown  |---        | ---      | ---       |\n","#@markdown  |Llama3-8B  |  X       |   O       |\n","#@markdown  |Mistral-7B |  X       |   O       |\n","#@markdown  |Llama3-70B |  X       |   X       |\n","\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM, LoraConfig\n","from peft import (inject_adapter_in_model, prepare_model_for_kbit_training,\n","                  get_peft_model, replace_lora_weights_loftq)\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers.utils import PaddingStrategy\n","from transformers.tokenization_utils_base import TruncationStrategy\n","from datasets import load_dataset\n","from random import randint\n","\n","hf_model_list = [\n","    # \"Gunulhona/tb_pretrained_sts\",\n","    # \"Gunulhona/tb_pretrained\",\n","    # \"google/flan-t5-xxl\",\n","    # \"meta-llama/Meta-Llama-3.1-8B\",\n","    # \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    # \"mistralai/Mistral-7B-Instruct-v0.3\",\n","    # \"mistralai/Mistral-Nemo-Instruct-2407\"\n","    # \"Qwen/Qwen2-7B-Instruct\",\n","    # \"google/gemma-7b\",\n","    # \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",\n","    # \"EleutherAI/polyglot-ko-12.8b\",\n","    # \"vilm/vulture-40b\",\n","    # \"tiiuae/falcon-11B\",\n","    # \"tiiuae/falcon-7b-instruct\",\n","    # \"arcee-ai/Arcee-Spark\",\n","    # \"apple/DCLM-7B-8k\",\n","    # \"SciPhi/Triplex\",\n","    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    # \"bigscience/bloomz-7b1-mt\",\n","    \"bigscience/bloomz-1b1\",\n","    \"Qwen/Qwen2-1.5B-Instruct\",\n","    \"Qwen/Qwen2-0.5B-Instruct\",\n","    \"OuteAI/Lite-Oute-1-65M\",\n","    \"OuteAI/Lite-Mistral-150M-v2-Instruct\",\n","    \"google/gemma-2b-it\",\n","    \"jjhsnail0822/danube-ko-1.8b-base\",\n","    \"OpenBuddy/openbuddy-stablelm-3b-v13\",\n","    \"daekeun-ml/phi-2-ko-v0.1\",\n","    \"microsoft/Phi-3-mini-128k-instruct\",\n","    \"HuggingFaceTB/SmolLM-1.7B\",\n","    \"HuggingFaceTB/SmolLM-360M\",\n","    \"HuggingFaceTB/SmolLM-135M\",\n","    \"numind/NuExtract\",\n","    ]\n","base_model_id = \"Qwen/Qwen2-1.5B-Instruct\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M\", \"google/gemma-2b-it\"] {allow-input: true}\n","\n","def get_model(model_name:str,\n","              init_lora_weights: str = \"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","              rank: int = 8,\n","              lora_alpha: int = 32,\n","              lora_dropout: float = 0.05,\n","              use_dora: bool = False,\n","              use_rslora: bool = False,\n","              fan_in_fan_out: bool = False,):\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_8bit=True,\n","        bnb_8bit_quant_type=\"nf8\",\n","        bnb_8bit_compute_dtype=torch.float16\n","    )eawsdf123123123\n","\n","\n","    peft_model = AutoModelForCausalLM.from_pretrained(\n","        base_model_id,\n","        trust_remote_code=True,\n","        # quantization_config=bnb_config\n","        )\n","\n","    peft_model = prepare_model_for_kbit_training(peft_model)\n","\n","    # adapter configuration\n","    lora_config = LoraConfig(\n","        target_modules=[\"q_proj\", \"k_proj\"],\n","        init_lora_weights=init_lora_weights,\n","        r=rank,\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        inference_mode=False,\n","        use_dora=use_dora,\n","        use_rslora=use_rslora,\n","        fan_in_fan_out=fan_in_fan_out,\n","        task_type=\"CAUSAL_LM\",\n","    )\n","\n","    # peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","    inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","    peft_model = get_peft_model(peft_model, lora_config)\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        base_model_id,\n","        add_special_tokens=True,\n","        trust_remote_code=True)\n","    tokenizer.model_input_names=['input_ids', 'attention_mask']\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    tokenizer.padding_side = \"right\"\n","    tokenizer.truncation_side = \"right\"\n","    return {\n","        \"model\": peft_model,\n","        \"tokenizer\": tokenizer\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"wK7h7vdKX0r7"},"source":["#3. Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1722387473617,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"AQd5nc9cYixp","outputId":"32aba23b-5b07-4dbd-86ea-b6d613a47ea3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}],"source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","\n","from evaluate import load\n","from finetuning_datafunctions import formatting, preprocess_function, SumDataCallator\n","\n","\n","dataset_path = \"jonathankang/MEDICAL-DIALOG-SUMMARY\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\", \"ChuGyouk/Ko-MTS-Dialog\", \"har1/MTS_Dialogue-Clinical_Note\", \"316usman/research_clinical_visit_note_summarization_corpus_mts\", \"jonathankang/MEDICAL-DIALOG-SUMMARY\"] {allow-input: true}\n","def get_dataset(dataset_name: str,\n","                tokenizer):\n","    raw_dataset = load_dataset(\n","    dataset_path,\n","    trust_remote_code=True,\n","    revision=\"main\",  # tag name, or branch name, or commit hash\n","    )\n","\n","    metric = load(\"rouge\")\n","    full_dataset = concatenate_datasets([raw_dataset[\"train\"], raw_dataset[\"test\"]])\n","    tokenized_inputs = full_dataset.map(\n","        lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","        batched=True,\n","        remove_columns=[\"dialogue\", \"summary\"])\n","\n","    input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","    # take 85 percentile of max length for better utilization\n","    max_source_length = int(np.percentile(input_lenghts, 100)) + int(np.percentile(input_lenghts, 10))\n","    max_source_length = min(4096, max_source_length)\n","\n","    tokenized_targets = full_dataset.map(\n","        lambda x: tokenizer(x[\"summary\"], truncation=True),\n","        batched=True,\n","        remove_columns=[\"dialogue\", \"summary\"])\n","    target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","    # take 90 percentile of max length for better utilization\n","    max_target_length = int(np.percentile(target_lenghts, 100)) + int(np.percentile(target_lenghts, 10))\n","    max_target_length = min(4096, max_target_length)\n","\n","    dataset = raw_dataset.map(\n","        preprocess_function,\n","        batched=True,\n","        remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","        fn_kwargs={\n","            \"tokenizer\": tokenizer,\n","            \"max_source_length\": max_source_length,\n","            \"max_target_length\": max_target_length\n","            },)\n","    # dataset = raw_dataset\n","    # if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    #     dataset = dataset.map(lambda x: x,\n","    #                           batched=True,\n","    #                           remove_columns=[\"token_type_ids\"], )\n","    return {\n","        \"dataset\":dataset,\n","        \"metric\": metric,\n","        \"max_source_length\": max_source_length\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1722387474130,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"_6hq8eyYLvCn","outputId":"8ddb13c1-53a0-48ee-88ab-06bac7353036"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datafunctions.py\n"]}],"source":["#@title Data Modules\n","#@markdown Data Module Functions\n","%%writefile finetuning_datafunctions.py\n","import numpy as np\n","import torch\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","\n","from evaluate import load\n","\n","\n","def formatting(sample,\n","               max_source_length,\n","               max_target_length,\n","               tokenizer,\n","               padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    model_inputs, labels = [], []\n","    for dialogue, summary in zip(sample[\"dialogue\"], sample[\"summary\"]):\n","        chat_template = [\n","            # {\n","            #     \"role\": \"system\",\n","            #     \"content\": \"You are a friendly chatbot who always responds with summary\",\n","            # },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Summarize the following dialogue\\n\\n{dialogue}\"\n","            },\n","\n","        ]\n","        label_template = [{\n","                \"role\": \"assistant\",\n","                \"content\": f\"{summary}\"\n","        }]\n","\n","        chat_message = tokenizer.apply_chat_template(conversation=chat_template,\n","                                                     tokenize=False,\n","                                                     add_generateion_prompt=False, )\n","        bot_message = tokenizer.apply_chat_template(conversation=label_template,\n","                                                    tokenize=False,\n","                                                    add_generateion_prompt=False, )\n","        model_inputs.append(chat_message)\n","        labels.append(bot_message)\n","\n","    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    # model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    # labels = tokenizer(text_target=sample[\"summary\"],\n","    #                    max_length=max_target_length,\n","    #                    padding=padding,\n","    #                    truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    # if padding == \"max_length\":\n","    #     labels[\"input_ids\"] = [\n","    #         [(l if l != tokenizer.pad_token_id else 1) for l in label] for label in labels[\"input_ids\"]\n","    #     ]\n","    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs, labels\n","\n","def preprocess_function(sample, max_source_length, max_target_length, tokenizer):\n","    templated_text, labels = formatting(sample, max_source_length, max_target_length, tokenizer)\n","\n","    return {\n","        \"input_ids\": templated_text,\n","        \"labels\": labels\n","    }\n","\n","class CallatorOutput:\n","    def __init__(self, input_ids, attention_mask, labels):\n","        self._input_ids = input_ids\n","        self._attention_mask = attention_mask\n","        self._labels = labels\n","\n","    def __len__(self,):\n","        return len(self._input_ids)\n","\n","    def __getitem__(self, key):\n","        match key:\n","            case \"input_ids\":\n","                return self._input_ids\n","            case \"attention_mask\":\n","                return self._attention_mask\n","            case \"labels\":\n","                return self._labels\n","            case _:\n","                raise KeyError(f\"Key {key} not found\")\n","\n","    def __setitem__(self, key, value):\n","        match key:\n","            case \"input_ids\":\n","                self._input_ids = value\n","            case \"attention_mask\":\n","                self._attention_mask = value\n","            case \"labels\":\n","                self._labels = value\n","            case _:\n","                raise KeyError(f\"Key {key} not found\")\n","\n","    def __iter__(self):\n","        return iter(self.__dict__.keys())\n","\n","    def to_dict(self):\n","        return {\n","            \"input_ids\": self[\"input_ids\"],\n","            \"attention_mask\": self[\"attention_mask\"],\n","            \"labels\": self[\"labels\"]\n","        }\n","    def items(self):\n","        return self.to_dict().items()\n","\n","    def keys(self):\n","        return self.to_dict().keys()\n","\n","    def values(self):\n","        return self.to_dict().values()\n","\n","    @property\n","    def input_ids(self):\n","        return self._input_ids\n","\n","    @input_ids.setter\n","    def input_ids(self, value):\n","        self._input_ids = value\n","\n","    @property\n","    def attention_mask(self):\n","        return self._attention_mask\n","\n","    @attention_mask.setter\n","    def attention_mask(self, value):\n","        self._attention_mask = value\n","\n","    @property\n","    def labels(self):\n","        return self._labels\n","\n","    @labels.setter\n","    def labels(self, value):\n","        self._labels = value\n","\n","\n","\n","class SumDataCallator(DataCollatorForLanguageModeling):\n","    def __init__(self, tokenizer, max_length,):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    def _tokenizing(self, text):\n","        return self.tokenizer(text,\n","                              truncation=True,\n","                              padding=\"max_length\",\n","                              max_length=self.max_length,\n","                              return_tensors=\"pt\")\n","\n","    def __call__(self, batch):\n","        input_text = []\n","        labels = []\n","        for b in batch:\n","            input_text += [b[\"input_ids\"]]\n","            labels += [b[\"labels\"]]\n","        input_tokens = self._tokenizing(input_text)\n","        label_tokens = self._tokenizing(labels)\n","\n","        return CallatorOutput(**{\n","            \"input_ids\": input_tokens['input_ids'].to(self.device),\n","            \"attention_mask\": input_tokens['attention_mask'].to(self.device),\n","            \"labels\": label_tokens['input_ids'].to(self.device),\n","        })\n","        # raise Exception(\"STOP\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3C1Fs6eeX4Ri"},"source":["#4. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1722387478857,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"wnRyfucpWeK5","outputId":"73cb153a-3a39-4925-d121-e9559be30692"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}],"source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","from trl import SFTTrainer, SFTConfig\n","from ignite.metrics import Rouge\n","\n","from peft_model import get_model, base_model_id\n","from finetuning_datasets import get_dataset, dataset_path\n","from finetuning_datafunctions import SumDataCallator, formatting, preprocess_function\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_data = get_model(model_name=base_model_id)\n","peft_model = model_data[\"model\"]\n","tokenizer = model_data[\"tokenizer\"]\n","\n","dataset_data = get_dataset(dataset_name=dataset_path)\n","dataset = dataset_data[\"dataset\"]\n","metric = dataset_data[\"metric\"]\n","max_source_length = dataset_data[\"max_source_length\"]\n","\n","peft_model.to(device)\n","\n","class ColabTrainer(SFTTrainer):\n","    pass\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=500):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","\n","# data_collator = DataCollatorWithPadding(\n","#     tokenizer=tokenizer,\n","#     padding=True,\n","#     max_length=max_source_length,\n","#     return_tensors=\"pt\")\n","data_collator = SumDataCallator(tokenizer, max_length=max_source_length)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    # eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    warmup_steps=0.03,\n","    gradient_accumulation_steps=4,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"paged_adamw_8bit\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = SFTTrainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n","    peft_config=lora_config,\n","    # formatting_func=formatting,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1722387478858,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"uysQB-wlnmt3","outputId":"e781a83c-0fe8-4546-95dd-430634c5dcee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing deepspeed_config.yaml\n"]}],"source":["#@title deepspeed config yaml file saving\n","#@markdown deep speed config for transformers trainer\n","%%writefile deepspeed_config.yaml\n","compute_environment: LOCAL_MACHINE\n","debug: false\n","deepspeed_config:\n","  deepspeed_multinode_launcher: standard\n","  gradient_accumulation_steps: 4\n","  offload_optimizer_device: none\n","  offload_param_device: none\n","  zero3_init_flag: true\n","  zero3_save_16bit_model: true\n","  zero_stage: 2\n","distributed_type: DEEPSPEED\n","downcast_bf16: 'yes'\n","machine_rank: 0\n","main_training_function: main\n","mixed_precision: bf16\n","num_machines: 1\n","num_processes: 1\n","rdzv_backend: static\n","same_network: true\n","tpu_env: []\n","tpu_use_cluster: false\n","tpu_use_sudo: false\n","use_cpu: false"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2435976,"status":"ok","timestamp":1721803108168,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"iX1Vp8-2PQN8","outputId":"cbc0be16-1f79-4cce-fa8b-09297c99bc1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-07-24 05:58:03,310] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n","2024-07-24 05:58:13.001962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-24 05:58:13.002009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-24 05:58:13.130675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-24 05:58:15.598996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[2024-07-24 05:58:20,094] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[2024-07-24 05:58:52,462] [INFO] [comm.py:637:init_distributed] cdb=None\n","[2024-07-24 05:58:52,462] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","/usr/local/lib/python3.10/dist-packages/accelerate/utils/dataclasses.py:1017: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n","  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n","/usr/local/lib/python3.10/dist-packages/accelerate/utils/dataclasses.py:1017: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n","  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","[2024-07-24 05:58:54,005] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","  0% 0/7364 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","{'train_runtime': 2368.469, 'train_samples_per_second': 24.88, 'train_steps_per_second': 3.109, 'train_loss': 1.3368511962890626, 'epoch': 0.27}\n","  7% 500/7364 [39:28<9:01:54,  4.74s/it]\n"]},{"data":{"text/plain":[]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","#@title Huggingface Deepspeed Trainer\n","#@markdown env variables and accelerate trainer\n","export CUDA_VISIBLE_DEVICES=0\n","export CUDA_LAUNCH_BLOCKING=1\n","export TORCH_USE_CUDA_DSA=0\n","export HF_DATASETS_CACHE='/content/hf_cache/'\n","accelerate launch --config_file \"deepspeed_config.yaml\" train.py\n","# python train.py"]},{"cell_type":"markdown","metadata":{"id":"d5KC2KSMllGB"},"source":["## Training code to Lightning module"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1722387481682,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"YSF7QaH1zhQ9","outputId":"7fc70dfc-03f9-4a45-c3f8-30cb41c37bd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_datamodule.py\n"]}],"source":["%%writefile l_datamodule.py\n","#@title Lightning Data Moudle\n","#@markdown Lightning Data Loading Modules\n","\n","import lightning as L\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForLanguageModeling\n","from finetuning_datafunctions import SumDataCallator\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self,\n","                 train_dataset,\n","                 val_dataset,\n","                 test_dataset,\n","                 data_collator:DataCollatorForLanguageModeling,\n","                 train_batch_size: int = 1,\n","                 eval_batch_size:int = 1,\n","                 training_args: dict = {}):\n","        super().__init__()\n","        self.train_dataset = train_dataset\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        self.data_collator = data_collator\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          num_workers=0,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":486,"status":"ok","timestamp":1722406214945,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"agrktrAylkal","outputId":"43cbe31f-301e-4265-d2d8-6e18cab7ca39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_model.py\n"]}],"source":["%%writefile l_model.py\n","#@title Lightning Model\n","#@markdown Lightning Modules and Training step\n","\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from bitsandbytes.optim import AdamW, Lion, PagedAdamW, PagedLion, LARS\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","from torchmetrics.functional.text.rouge import rouge_score\n","\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self,\n","                 #data_collator,\n","                 peft_model,\n","                 tokenizer,\n","                 learning_rate: float = 2e-5 ):\n","        super().__init__()\n","        self.save_hyperparameters(ignore=['peft_model'])\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        # self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.learning_rate = learning_rate\n","\n","    def _get_rouge_score(self, predictions, labels):\n","        generated_tokens = predictions.argmax(dim=-1)\n","        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        return rouge_score(preds=decoded_preds, target=decoded_labels)\n","\n","    def _log(self, log_name, value, batch_size):\n","        self.log(\n","            log_name,\n","            value if value.device == self.model.device else value.to(self.model.device),\n","            prog_bar=True,\n","            on_step=True,\n","            on_epoch=True,\n","            batch_size=batch_size,\n","            sync_dist=True)\n","\n","    def _batch_device_correction(self, batch):\n","        for k, v in batch.items():\n","            if v.device != self.model.device:\n","                batch[k] = v.to(self.model.device)\n","        return batch\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        # print(input_ids.shape, input_ids.min(), input_ids.max())\n","        return self.model(**{\n","            \"input_ids\":input_ids,\n","            \"attention_mask\":attention_mask,\n","            \"labels\": labels\n","            })\n","\n","    def training_step(self, batch, batch_idx):\n","        batch = self._batch_device_correction(batch)\n","        outputs = self(input_ids=batch.input_ids,\n","                       attention_mask=batch.attention_mask,\n","                       labels=batch.labels)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","        loss = outputs.loss\n","        self._log(\"train_loss\", loss, self.trainer.datamodule.train_batch_size,)\n","        for k, v in rouge_score.items():\n","            self._log(f\"train_{k}\", v, self.trainer.datamodule.train_batch_size,)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        batch = self._batch_device_correction(batch)\n","        outputs = self(input_ids=batch.input_ids,\n","                       attention_mask=batch.attention_mask,\n","                       labels=batch.labels)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","        val_loss = outputs.loss\n","        self._log(\"val_loss\", val_loss, self.trainer.datamodule.eval_batch_size,)\n","        for k, v in rouge_score.items():\n","            self._log(f\"val_{k}\", v, self.trainer.datamodule.eval_batch_size,)\n","\n","    def configure_optimizers(self):\n","        optimizer = PagedLion(params=self.model.parameters(),\n","                              lr=self.learning_rate,\n","                              weight_decay=0.01,\n","                              optim_bits=32,)\n","        scheduler = CosineAnnealingWarmRestarts(optimizer,\n","                                                T_0=10,\n","                                                T_mult=2,\n","                                                eta_min=0.00001)\n","        # scheduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"min\")\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"monitor\": \"val_loss\",\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","\n","            },\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722406308530,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"vFZzH2TmzqP-","outputId":"9abd9882-1260-4e18-ffc4-62ac2d2ec6a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_trainer.py\n"]}],"source":["%%writefile l_trainer.py\n","#@title Trainer\n","#@markdown Lightning cli trainer\n","\n","import os\n","import lightning as L\n","from lightning.pytorch import Trainer\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n","from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, TQDMProgressBar\n","\n","import ray\n","from ray.train.lightning import (RayTrainReportCallback, RayDeepSpeedStrategy,\n","                                 RayLightningEnvironment, prepare_trainer)\n","from ray.train.torch import TorchTrainer\n","\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import get_model, base_model_id\n","from finetuning_datasets import get_dataset, dataset_path\n","from finetuning_datafunctions import SumDataCallator\n","\n","\n","# is Lightning able?\n","L.pytorch.cli_lightning_logo()\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","\n","\n","def l2ray_trainer():\n","    model_data = get_model(model_name=base_model_id)\n","    peft_model = model_data[\"model\"]\n","    tokenizer = model_data[\"tokenizer\"]\n","\n","    dataset_data = get_dataset(dataset_name=dataset_path, tokenizer=tokenizer)\n","    dataset = dataset_data[\"dataset\"]\n","    metric = dataset_data[\"metric\"]\n","    max_source_length = dataset_data[\"max_source_length\"]\n","    trainer = Trainer(\n","        fast_dev_run=True,\n","        max_epochs=10,\n","        devices=\"auto\",\n","        accelerator=\"auto\",\n","        accumulate_grad_batches=4,\n","        precision=\"bf16-mixed\",\n","        default_root_dir=f\"{os.getcwd()}/checkpoints\",\n","        strategy=RayDeepSpeedStrategy(\n","            stage=2,\n","            ),\n","        plugins=[\n","            RayLightningEnvironment()\n","            ],\n","        callbacks=[\n","            RayTrainReportCallback(),\n","            EarlyStopping(monitor=\"val_loss\"),\n","            LearningRateMonitor(),\n","            TQDMProgressBar(refresh_rate=5),\n","            ModelCheckpoint(\n","                save_top_k=3,\n","                monitor='val_loss',\n","                mode='min',\n","                save_weights_only=True,  # 가중치만 저장\n","                save_on_train_epoch_end=True,\n","                dirpath=\"checkpoint\",\n","                filename=f'{base_model_id}_PEFT_ckpt'\n","                )\n","        ],\n","        logger=[\n","            WandbLogger(project=\"LLM-Finetuning\"),\n","            TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n","        ],\n","    )\n","    trainer = prepare_trainer(trainer=trainer)\n","    trainer.fit(\n","        model=LLamaFTLightningModule(\n","            peft_model=peft_model,\n","            tokenizer=tokenizer,),\n","        datamodule=FTDataModule(\n","            train_dataset=dataset[\"train\"],\n","            val_dataset=dataset[\"test\"],\n","            test_dataset=dataset[\"test\"],\n","            data_collator=SumDataCallator(tokenizer, max_length=max_source_length),\n","            train_batch_size=2,\n","            eval_batch_size=1))\n","\n","\n","ray_trainer = TorchTrainer(\n","    l2ray_trainer,\n","    scaling_config=ray.train.ScalingConfig(\n","        num_workers=1,\n","        use_gpu=True,\n","        resources_per_worker={ \"CPU\": 8, \"GPU\": 1, },\n","        # trainer_resources={ \"CPU\": 8, \"GPU\": 1 },\n","        accelerator_type=\"L4\",\n","\n","        ),\n","    run_config=ray.train.RunConfig(\n","        checkpoint_config=ray.train.CheckpointConfig(\n","            num_to_keep=3,\n","            checkpoint_score_attribute=\"val_loss\",\n","            checkpoint_score_order=\"min\",\n","        ),\n","    )\n",")\n","ray.init(\n","    num_cpus=8,\n","    ignore_reinit_error=True,\n",")\n","result = ray_trainer.fit()\n","\n","print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1098,"status":"ok","timestamp":1722406534563,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"xCN4ffHxtqDo","outputId":"21a3f636-8d56-458d-c1c8-9ed94e7a6437"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_sweep.py\n"]}],"source":["%%writefile l_sweep.py\n","#@title Wandb Sweep\n","#@markdown Lightning cli Sweep\n","\n","import os\n","import lightning as L\n","from lightning.pytorch import Trainer\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n","from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, TQDMProgressBar\n","\n","import ray\n","from ray.train.lightning import (RayTrainReportCallback, RayDeepSpeedStrategy,\n","                                 RayLightningEnvironment, prepare_trainer)\n","from ray.train.torch import TorchTrainer\n","\n","from transformers import DataCollatorForSeq2Seq\n","import wandb\n","import gc\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import get_model, hf_model_list\n","from finetuning_datasets import get_dataset, dataset_path\n","from finetuning_datafunctions import SumDataCallator\n","\n","\n","# is Lightning able?\n","L.pytorch.cli_lightning_logo()\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","\n","os.environ[\"WANDB_PROJECT_NAME\"] = \"LLM-Finetuning\"\n","\n","def l2ray_trainer():\n","    wandb.init(\n","        project=\"LLM-Finetuning\")\n","    config = wandb.config\n","\n","    model_data = get_model(\n","        model_name=config.model_name,\n","        learning_rate=config.lr,\n","        lora_rank=config.lora_rank,\n","        lora_alpha=config.lora_alpha,\n","        lora_dropout=config.lora_dropout,\n","        init_lora_weights=config.init_lora_weights\n","        )\n","    peft_model = model_data[\"model\"]\n","    tokenizer = model_data[\"tokenizer\"]\n","\n","    dataset_data = get_dataset(dataset_name=dataset_path, tokenizer=tokenizer)\n","    dataset = dataset_data[\"dataset\"]\n","    metric = dataset_data[\"metric\"]\n","    max_source_length = dataset_data[\"max_source_length\"]\n","    trainer = Trainer(\n","        devices=\"auto\",\n","        accelerator=\"auto\",\n","        max_epochs=config.epochs,\n","        accumulate_grad_batches=config.accumulate_grad_batches,\n","        gradient_clip_val=config.gradient_clip_val,\n","        precision=\"bf16-mixed\",\n","        strategy=\"deepspeed\",\n","        # enable_checkpointing=False,\n","        # strategy=RayDeepSpeedStrategy(\n","        #     stage=2,\n","        #     contiguous_memory_optimization=False\n","        #     ),\n","        plugins=[\n","            # RayLightningEnvironment()\n","        ],\n","        callbacks=[\n","            # RayTrainReportCallback(),\n","            EarlyStopping(monitor=\"val_loss\"),\n","            TQDMProgressBar(refresh_rate=5),\n","            LearningRateMonitor()\n","        ],\n","        logger=[\n","            WandbLogger(project=os.getenv(\"WANDB_PROJECT_NAME\")),\n","            TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n","        ],\n","    )\n","    # trainer = prepare_trainer(trainer=trainer)\n","    trainer.fit(\n","        model=LLamaFTLightningModule(\n","            peft_model=peft_model,\n","            tokenizer=tokenizer,),\n","        datamodule=FTDataModule(\n","            train_dataset=dataset[\"train\"].shard(num_shards=10, index=0),\n","            val_dataset=dataset[\"test\"].shard(num_shards=10, index=0),\n","            test_dataset=dataset[\"test\"].shard(num_shards=10, index=1),\n","            data_collator=SumDataCallator(tokenizer, max_length=max_source_length),\n","            train_batch_size=2,\n","            eval_batch_size=1))\n","    gc.collect()\n","    del trainer\n","\n","\n","def ray_wrapped_trainer():\n","    ray_trainer = TorchTrainer(\n","        l2ray_trainer,\n","        scaling_config=ray.train.ScalingConfig(\n","            num_workers=1,\n","            use_gpu=True,\n","            resources_per_worker={ \"CPU\": 8, \"GPU\": 1, },\n","            # trainer_resources={ \"CPU\": 8, \"GPU\": 1 },\n","            accelerator_type=\"L4\",\n","            ),\n","        run_config=ray.train.RunConfig(\n","            checkpoint_config=ray.train.CheckpointConfig(\n","                num_to_keep=1,\n","                checkpoint_score_attribute=\"val_loss\",\n","                checkpoint_score_order=\"min\",\n","            ),\n","        )\n","    )\n","    ray.init(\n","        num_cpus=8,\n","        ignore_reinit_error=True,\n","    )\n","    result = ray_trainer.fit()\n","\n","wandb.agent(\n","    sweep_id=wandb.sweep(\n","        {\n","            \"method\": \"random\",\n","            \"name\": \"sweep\",\n","            \"metric\": {\n","                \"goal\": \"maximize\",\n","                \"name\": \"val_rouge1_fmeasure_epoch\"\n","                },\n","            \"parameters\": {\n","                \"model_name\": {\n","                    \"values\": hf_model_list\n","                    },\n","                \"epochs\": {\n","                    \"values\": [1]\n","                    },\n","                \"lr\": {\n","                    \"max\": 5e-4,\n","                    \"min\": 5e-5\n","                    },\n","                \"accumulate_grad_batches\": {\n","                    \"min\": 1,\n","                    \"max\": 8\n","                    },\n","                \"gradient_clip_val\": {\n","                    \"min\": 0.1\n","                    \"max\": 1\n","                    },\n","                \"lora_rank\":{\n","                    \"values\": [2, 4, 8, 16, 32]\n","                },\n","                \"lora_alpha\":{\n","                    \"values\": [16, 32, 64, 128]\n","                },\n","                \"lora_dropout\":{\n","                    \"min\": 0.05\n","                    \"max\": 0.1\n","                },\n","                \"init_lora_weights \":{\n","                    \"values\": [\"gaussian\", \"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False]\n","                }\n","            },\n","        },\n","        project=os.getenv(\"WANDB_PROJECT_NAME\"),\n","    ),\n","    function=l2ray_trainer,\n","    # function=ray_wrapped_trainer,\n","    count=len(hf_model_list),\n","    project=os.getenv(\"WANDB_PROJECT_NAME\"))\n","\n","# print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91056,"status":"ok","timestamp":1722406402587,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"RYALPHD3PyBD","outputId":"7d193c1d-8aa8-45d4-9286-0e9dcd8b8ec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-31 06:11:58.013647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-31 06:11:58.013714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-31 06:11:58.015656: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-31 06:11:59.255351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = _posixsubprocess.fork_exec(\n","/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = _posixsubprocess.fork_exec(\n","2024-07-31 06:12:02,276\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n","\n","View detailed results here: /root/ray_results/TorchTrainer_2024-07-31_06-12-03\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-07-31_06-12-00_956022_117191/artifacts/2024-07-31_06-12-03/TorchTrainer_2024-07-31_06-12-03/driver_artifacts`\n","\u001b[36m(TrainTrainable pid=117922)\u001b[0m 2024-07-31 06:12:09.250146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(TrainTrainable pid=117922)\u001b[0m 2024-07-31 06:12:09.250206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(TrainTrainable pid=117922)\u001b[0m 2024-07-31 06:12:09.251496: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(TrainTrainable pid=117922)\u001b[0m 2024-07-31 06:12:10.372547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(TrainTrainable pid=117922)\u001b[0m The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","\n","Training started without custom configuration.\n","\u001b[36m(TorchTrainer pid=117922)\u001b[0m Started distributed worker processes: \n","\u001b[36m(TorchTrainer pid=117922)\u001b[0m - (node_id=083929af302e03fa106cdf07266326ce4a923150d65f7f6734cd4a9c, ip=172.28.0.12, pid=118050) world_rank=0, local_rank=0, node_rank=0\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 2024-07-31 06:12:17.857786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 2024-07-31 06:12:17.857847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 2024-07-31 06:12:17.859242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 2024-07-31 06:12:18.995354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m   warnings.warn(\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m [2024-07-31 06:12:30,699] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m [rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m [2024-07-31 06:12:34,774] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m   | Name  | Type                 | Params\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m -----------------------------------------------\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 0 | model | PeftModelForCausalLM | 1.5 B \n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m -----------------------------------------------\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 1.1 M     Trainable params\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 1.5 B     Non-trainable params\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 1.5 B     Total params\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m 6,183.574 Total estimated model params size (MB)\n","Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] \n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m /usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m /usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.61it/s, train_loss_step=10.50, train_rouge1_fmeasure_step=0.0201, train_rouge1_precision_step=0.0102, train_rouge1_recall_step=0.857, train_rouge2_fmeasure_step=0.0101, train_rouge2_precision_step=0.00509, train_rouge2_recall_step=0.462, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00933, train_rougeL_recall_step=0.786, train_rougeLsum_fmeasure_step=0.0201, train_rougeLsum_precision_step=0.0102, train_rougeLsum_recall_step=0.857]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m \n","Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\u001b[A\n","Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.39it/s, train_loss_step=10.50, train_rouge1_fmeasure_step=0.0201, train_rouge1_precision_step=0.0102, train_rouge1_recall_step=0.857, train_rouge2_fmeasure_step=0.0101, train_rouge2_precision_step=0.00509, train_rouge2_recall_step=0.462, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00933, train_rougeL_recall_step=0.786, train_rougeLsum_fmeasure_step=0.0201, train_rougeLsum_precision_step=0.0102, train_rougeLsum_recall_step=0.857, val_loss_step=10.60, val_rouge1_fmeasure_step=0.0432, val_rouge1_precision_step=0.0225, val_rouge1_recall_step=0.674, val_rouge2_fmeasure_step=0.0135, val_rouge2_precision_step=0.007, val_rouge2_recall_step=0.246, val_rougeL_fmeasure_step=0.0263, val_rougeL_precision_step=0.0137, val_rougeL_recall_step=0.433, val_rougeLsum_fmeasure_step=0.0408, val_rougeLsum_precision_step=0.0212, val_rougeLsum_recall_step=0.650, val_loss_epoch=10.60, val_rouge1_fmeasure_epoch=0.0432, val_rouge1_precision_epoch=0.0225, val_rouge1_recall_epoch=0.674, val_rouge2_fmeasure_epoch=0.0135, val_rouge2_precision_epoch=0.007, val_rouge2_recall_epoch=0.246, val_rougeL_fmeasure_epoch=0.0263, val_rougeL_precision_epoch=0.0137, val_rougeL_recall_epoch=0.433, val_rougeLsum_fmeasure_epoch=0.0408, val_rougeLsum_precision_epoch=0.0212, val_rougeLsum_recall_epoch=0.650]\n","\n","Training finished iteration 1 at 2024-07-31 06:13:15. Total running time: 1min 12s\n","╭─────────────────────────────────────────────────────╮\n","│ Training result                                     │\n","├─────────────────────────────────────────────────────┤\n","│ checkpoint_dir_name               checkpoint_000000 │\n","│ time_this_iter_s                           63.74066 │\n","│ time_total_s                               63.74066 │\n","│ training_iteration                                1 │\n","│ epoch                                             0 │\n","│ step                                              0 │\n","│ train_loss                                 10.50942 │\n","│ train_loss_epoch                           10.50942 │\n","│ train_loss_step                            10.50942 │\n","│ train_rouge1_fmeasure                       0.02012 │\n","│ train_rouge1_fmeasure_epoch                 0.02012 │\n","│ train_rouge1_fmeasure_step                  0.02012 │\n","│ train_rouge1_precision                      0.01018 │\n","│ train_rouge1_precision_epoch                0.01018 │\n","│ train_rouge1_precision_step                 0.01018 │\n","│ train_rouge1_recall                         0.85714 │\n","│ train_rouge1_recall_epoch                   0.85714 │\n","│ train_rouge1_recall_step                    0.85714 │\n","│ train_rouge2_fmeasure                       0.01008 │\n","│ train_rouge2_fmeasure_epoch                 0.01008 │\n","│ train_rouge2_fmeasure_step                  0.01008 │\n","│ train_rouge2_precision                      0.00509 │\n","│ train_rouge2_precision_epoch                0.00509 │\n","│ train_rouge2_precision_step                 0.00509 │\n","│ train_rouge2_recall                         0.46154 │\n","│ train_rouge2_recall_epoch                   0.46154 │\n","│ train_rouge2_recall_step                    0.46154 │\n","│ train_rougeL_fmeasure                       0.01844 │\n","│ train_rougeL_fmeasure_epoch                 0.01844 │\n","│ train_rougeL_fmeasure_step                  0.01844 │\n","│ train_rougeL_precision                      0.00933 │\n","│ train_rougeL_precision_epoch                0.00933 │\n","│ train_rougeL_precision_step                 0.00933 │\n","│ train_rougeL_recall                         0.78571 │\n","│ train_rougeL_recall_epoch                   0.78571 │\n","│ train_rougeL_recall_step                    0.78571 │\n","│ train_rougeLsum_fmeasure                    0.02012 │\n","│ train_rougeLsum_fmeasure_epoch              0.02012 │\n","│ train_rougeLsum_fmeasure_step               0.02012 │\n","│ train_rougeLsum_precision                   0.01018 │\n","│ train_rougeLsum_precision_epoch             0.01018 │\n","│ train_rougeLsum_precision_step              0.01018 │\n","│ train_rougeLsum_recall                      0.85714 │\n","│ train_rougeLsum_recall_epoch                0.85714 │\n","│ train_rougeLsum_recall_step                 0.85714 │\n","│ val_loss                                   10.61591 │\n","│ val_loss_epoch                             10.61591 │\n","│ val_rouge1_fmeasure                         0.04315 │\n","│ val_rouge1_fmeasure_epoch                   0.04315 │\n","│ val_rouge1_precision                        0.02248 │\n","│ val_rouge1_precision_epoch                  0.02248 │\n","│ val_rouge1_recall                           0.67368 │\n","│ val_rouge1_recall_epoch                     0.67368 │\n","│ val_rouge2_fmeasure                          0.0135 │\n","│ val_rouge2_fmeasure_epoch                    0.0135 │\n","│ val_rouge2_precision                          0.007 │\n","│ val_rouge2_precision_epoch                    0.007 │\n","│ val_rouge2_recall                           0.24603 │\n","│ val_rouge2_recall_epoch                     0.24603 │\n","│ val_rougeL_fmeasure                         0.02634 │\n","│ val_rougeL_fmeasure_epoch                   0.02634 │\n","│ val_rougeL_precision                         0.0137 │\n","│ val_rougeL_precision_epoch                   0.0137 │\n","│ val_rougeL_recall                           0.43329 │\n","│ val_rougeL_recall_epoch                     0.43329 │\n","│ val_rougeLsum_fmeasure                      0.04078 │\n","│ val_rougeLsum_fmeasure_epoch                0.04078 │\n","│ val_rougeLsum_precision                     0.02124 │\n","│ val_rougeLsum_precision_epoch               0.02124 │\n","│ val_rougeLsum_recall                        0.65024 │\n","│ val_rougeLsum_recall_epoch                  0.65024 │\n","╰─────────────────────────────────────────────────────╯\n","Training saved a checkpoint for iteration 1 at: (local)/root/ray_results/TorchTrainer_2024-07-31_06-12-03/TorchTrainer_cdfb7_00000_0_2024-07-31_06-12-03/checkpoint_000000\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2024-07-31_06-12-03/TorchTrainer_cdfb7_00000_0_2024-07-31_06-12-03/checkpoint_000000)\n","Epoch 0: 100%|██████████| 1/1 [00:40<00:00,  0.02it/s, train_loss_step=10.50, train_rouge1_fmeasure_step=0.0201, train_rouge1_precision_step=0.0102, train_rouge1_recall_step=0.857, train_rouge2_fmeasure_step=0.0101, train_rouge2_precision_step=0.00509, train_rouge2_recall_step=0.462, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00933, train_rougeL_recall_step=0.786, train_rougeLsum_fmeasure_step=0.0201, train_rougeLsum_precision_step=0.0102, train_rougeLsum_recall_step=0.857, val_loss_step=10.60, val_rouge1_fmeasure_step=0.0432, val_rouge1_precision_step=0.0225, val_rouge1_recall_step=0.674, val_rouge2_fmeasure_step=0.0135, val_rouge2_precision_step=0.007, val_rouge2_recall_step=0.246, val_rougeL_fmeasure_step=0.0263, val_rougeL_precision_step=0.0137, val_rougeL_recall_step=0.433, val_rougeLsum_fmeasure_step=0.0408, val_rougeLsum_precision_step=0.0212, val_rougeLsum_recall_step=0.650, val_loss_epoch=10.60, val_rouge1_fmeasure_epoch=0.0432, val_rouge1_precision_epoch=0.0225, val_rouge1_recall_epoch=0.674, val_rouge2_fmeasure_epoch=0.0135, val_rouge2_precision_epoch=0.007, val_rouge2_recall_epoch=0.246, val_rougeL_fmeasure_epoch=0.0263, val_rougeL_precision_epoch=0.0137, val_rougeL_recall_epoch=0.433, val_rougeLsum_fmeasure_epoch=0.0408, val_rougeLsum_precision_epoch=0.0212, val_rougeLsum_recall_epoch=0.650, train_loss_epoch=10.50, train_rouge1_fmeasure_epoch=0.0201, train_rouge1_precision_epoch=0.0102, train_rouge1_recall_epoch=0.857, train_rouge2_fmeasure_epoch=0.0101, train_rouge2_precision_epoch=0.00509, train_rouge2_recall_epoch=0.462, train_rougeL_fmeasure_epoch=0.0184, train_rougeL_precision_epoch=0.00933, train_rougeL_recall_epoch=0.786, train_rougeLsum_fmeasure_epoch=0.0201, train_rougeLsum_precision_epoch=0.0102, train_rougeLsum_recall_epoch=0.857]\n","\u001b[36m(RayTrainWorker pid=118050)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n","\n","Training completed after 1 iterations at 2024-07-31 06:13:19. Total running time: 1min 15s\n","2024-07-31 06:13:19,107\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2024-07-31_06-12-03' in 0.0031s.\n","\n","Result(\n","  metrics={'train_loss': 10.509416580200195, 'train_loss_step': 10.509416580200195, 'train_rouge1_fmeasure': 0.020117351785302162, 'train_rouge1_fmeasure_step': 0.020117351785302162, 'train_rouge1_precision': 0.010178117081522942, 'train_rouge1_precision_step': 0.010178117081522942, 'train_rouge1_recall': 0.8571428656578064, 'train_rouge1_recall_step': 0.8571428656578064, 'train_rouge2_fmeasure': 0.010075566358864307, 'train_rouge2_fmeasure_step': 0.010075566358864307, 'train_rouge2_precision': 0.005093378480523825, 'train_rouge2_precision_step': 0.005093378480523825, 'train_rouge2_recall': 0.4615384638309479, 'train_rouge2_recall_step': 0.4615384638309479, 'train_rougeL_fmeasure': 0.018440905958414078, 'train_rougeL_fmeasure_step': 0.018440905958414078, 'train_rougeL_precision': 0.00932994019240141, 'train_rougeL_precision_step': 0.00932994019240141, 'train_rougeL_recall': 0.7857142686843872, 'train_rougeL_recall_step': 0.7857142686843872, 'train_rougeLsum_fmeasure': 0.020117351785302162, 'train_rougeLsum_fmeasure_step': 0.020117351785302162, 'train_rougeLsum_precision': 0.010178117081522942, 'train_rougeLsum_precision_step': 0.010178117081522942, 'train_rougeLsum_recall': 0.8571428656578064, 'train_rougeLsum_recall_step': 0.8571428656578064, 'val_loss': 10.615909576416016, 'val_loss_epoch': 10.615909576416016, 'val_rouge1_fmeasure': 0.043153487145900726, 'val_rouge1_fmeasure_epoch': 0.043153487145900726, 'val_rouge1_precision': 0.022483212873339653, 'val_rouge1_precision_epoch': 0.022483212873339653, 'val_rouge1_recall': 0.673677921295166, 'val_rouge1_recall_epoch': 0.673677921295166, 'val_rouge2_fmeasure': 0.013497984036803246, 'val_rouge2_fmeasure_epoch': 0.013497984036803246, 'val_rouge2_precision': 0.007004106417298317, 'val_rouge2_precision_epoch': 0.007004106417298317, 'val_rouge2_recall': 0.2460317611694336, 'val_rouge2_recall_epoch': 0.2460317611694336, 'val_rougeL_fmeasure': 0.02633698284626007, 'val_rougeL_fmeasure_epoch': 0.02633698284626007, 'val_rougeL_precision': 0.013699979521334171, 'val_rougeL_precision_epoch': 0.013699979521334171, 'val_rougeL_recall': 0.43329328298568726, 'val_rougeL_recall_epoch': 0.43329328298568726, 'val_rougeLsum_fmeasure': 0.04078381508588791, 'val_rougeLsum_fmeasure_epoch': 0.04078381508588791, 'val_rougeLsum_precision': 0.02123529277741909, 'val_rougeLsum_precision_epoch': 0.02123529277741909, 'val_rougeLsum_recall': 0.650240421295166, 'val_rougeLsum_recall_epoch': 0.650240421295166, 'train_loss_epoch': 10.509416580200195, 'train_rouge1_fmeasure_epoch': 0.020117351785302162, 'train_rouge1_precision_epoch': 0.010178117081522942, 'train_rouge1_recall_epoch': 0.8571428656578064, 'train_rouge2_fmeasure_epoch': 0.010075566358864307, 'train_rouge2_precision_epoch': 0.005093378480523825, 'train_rouge2_recall_epoch': 0.4615384638309479, 'train_rougeL_fmeasure_epoch': 0.018440905958414078, 'train_rougeL_precision_epoch': 0.00932994019240141, 'train_rougeL_recall_epoch': 0.7857142686843872, 'train_rougeLsum_fmeasure_epoch': 0.020117351785302162, 'train_rougeLsum_precision_epoch': 0.010178117081522942, 'train_rougeLsum_recall_epoch': 0.8571428656578064, 'epoch': 0, 'step': 0},\n","  path='/root/ray_results/TorchTrainer_2024-07-31_06-12-03/TorchTrainer_cdfb7_00000_0_2024-07-31_06-12-03',\n","  filesystem='local',\n","  checkpoint=Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2024-07-31_06-12-03/TorchTrainer_cdfb7_00000_0_2024-07-31_06-12-03/checkpoint_000000)\n",")\n","\u001b[0m"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":75}],"source":["#@title Start Training\n","# %%writefile run_train\n","#@markdown 실험 결과\n","#@markdown\n","#@markdown * batch_size <b>2</b> 넘기는 경우 OOM\n","#@markdown * DeepSpeed의 경우 GPU Ram 20GB로 7B finetuning 가능\n","#@markdown * DeepSpeed의 경우, 7B L4 GPU에서 사용 가능\n","#@markdown * 70B의 경우 RAM에서 Weight 가져오다 OOM\n","#@markdown * 1.5B T4 GPU에서 성공\n","\n","%%shell\n","export CUDA_VISIBLE_DEVICES=0\n","export CUDA_LAUNCH_BLOCKING=1\n","\n","\n","# python l_trainer.py fit \\\n","#     --trainer.max_epochs 4 \\\n","#     --model.learning_rate 5e-5 \\\n","#     --data.train_batch_size 4 \\\n","#     --data.eval_batch_size 1\\\n","#    --trainer.fast_dev_run 1\\\n","\n","python l_trainer.py"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"_XTsURtW6w2N","outputId":"c1a21187-0f4c-4518-a230-4313b333eb84"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-31 06:15:43.370607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-31 06:15:43.370670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-31 06:15:43.373186: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-31 06:15:44.522384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","Create sweep with ID: quoyhhvt\n","Sweep URL: https://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8teo2ra1 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 5.4943810242019087e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: OuteAI/Lite-Oute-1-65M\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevintb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240731_061549-8teo2ra1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtough-sweep-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/8teo2ra1\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[2024-07-31 06:16:01,779] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-31 06:16:05,960] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type                 | Params\n","-----------------------------------------------\n","0 | model | PeftModelForCausalLM | 1.5 B \n","-----------------------------------------------\n","1.1 M     Trainable params\n","1.5 B     Non-trainable params\n","1.5 B     Total params\n","6,183.574 Total estimated model params size (MB)\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","Epoch 0: 100% 121/121 [01:53<00:00,  1.07it/s, v_num=a1_1, train_loss_step=5.360, train_rouge1_fmeasure_step=0.026, train_rouge1_precision_step=0.0133, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0119, train_rouge2_precision_step=0.00604, train_rouge2_recall_step=0.333, train_rougeL_fmeasure_step=0.0166, train_rougeL_precision_step=0.00844, train_rougeL_recall_step=0.438, train_rougeLsum_fmeasure_step=0.026, train_rougeLsum_precision_step=0.0133, train_rougeLsum_recall_step=0.688] \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/20 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.50it/s]\u001b[A\n","Validation DataLoader 0:  50% 10/20 [00:07<00:07,  1.41it/s]\u001b[A\n","Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.46it/s]\u001b[A\n","Validation DataLoader 0: 100% 20/20 [00:13<00:00,  1.48it/s]\u001b[A\n","Epoch 0: 100% 121/121 [02:07<00:00,  1.05s/it, v_num=a1_1, train_loss_step=5.360, train_rouge1_fmeasure_step=0.026, train_rouge1_precision_step=0.0133, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0119, train_rouge2_precision_step=0.00604, train_rouge2_recall_step=0.333, train_rougeL_fmeasure_step=0.0166, train_rougeL_precision_step=0.00844, train_rougeL_recall_step=0.438, train_rougeLsum_fmeasure_step=0.026, train_rougeLsum_precision_step=0.0133, train_rougeLsum_recall_step=0.688, val_loss_step=5.780, val_rouge1_fmeasure_step=0.015, val_rouge1_precision_step=0.00758, val_rouge1_recall_step=0.771, val_rouge2_fmeasure_step=0.00851, val_rouge2_precision_step=0.00429, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0122, val_rougeL_precision_step=0.00615, val_rougeL_recall_step=0.625, val_rougeLsum_fmeasure_step=0.015, val_rougeLsum_precision_step=0.00758, val_rougeLsum_recall_step=0.771, val_loss_epoch=6.220, val_rouge1_fmeasure_epoch=0.0468, val_rouge1_precision_epoch=0.0252, val_rouge1_recall_epoch=0.633, val_rouge2_fmeasure_epoch=0.012, val_rouge2_precision_epoch=0.00632, val_rouge2_recall_epoch=0.283, val_rougeL_fmeasure_epoch=0.0297, val_rougeL_precision_epoch=0.0159, val_rougeL_recall_epoch=0.469, val_rougeLsum_fmeasure_epoch=0.0457, val_rougeLsum_precision_epoch=0.0247, val_rougeLsum_recall_epoch=0.627, train_loss_epoch=8.150, train_rouge1_fmeasure_epoch=0.0372, train_rouge1_precision_epoch=0.0198, train_rouge1_recall_epoch=0.638, train_rouge2_fmeasure_epoch=0.0111, train_rouge2_precision_epoch=0.00578, train_rouge2_recall_epoch=0.286, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.504, train_rougeLsum_fmeasure_epoch=0.0365, train_rougeLsum_precision_epoch=0.0195, train_rougeLsum_recall_epoch=0.632]`Trainer.fit` stopped: `max_epochs=1` reached.\n","Epoch 0: 100% 121/121 [02:26<00:00,  1.21s/it, v_num=a1_1, train_loss_step=5.360, train_rouge1_fmeasure_step=0.026, train_rouge1_precision_step=0.0133, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0119, train_rouge2_precision_step=0.00604, train_rouge2_recall_step=0.333, train_rougeL_fmeasure_step=0.0166, train_rougeL_precision_step=0.00844, train_rougeL_recall_step=0.438, train_rougeLsum_fmeasure_step=0.026, train_rougeLsum_precision_step=0.0133, train_rougeLsum_recall_step=0.688, val_loss_step=5.780, val_rouge1_fmeasure_step=0.015, val_rouge1_precision_step=0.00758, val_rouge1_recall_step=0.771, val_rouge2_fmeasure_step=0.00851, val_rouge2_precision_step=0.00429, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0122, val_rougeL_precision_step=0.00615, val_rougeL_recall_step=0.625, val_rougeLsum_fmeasure_step=0.015, val_rougeLsum_precision_step=0.00758, val_rougeLsum_recall_step=0.771, val_loss_epoch=6.220, val_rouge1_fmeasure_epoch=0.0468, val_rouge1_precision_epoch=0.0252, val_rouge1_recall_epoch=0.633, val_rouge2_fmeasure_epoch=0.012, val_rouge2_precision_epoch=0.00632, val_rouge2_recall_epoch=0.283, val_rougeL_fmeasure_epoch=0.0297, val_rougeL_precision_epoch=0.0159, val_rougeL_recall_epoch=0.469, val_rougeLsum_fmeasure_epoch=0.0457, val_rougeLsum_precision_epoch=0.0247, val_rougeLsum_recall_epoch=0.627, train_loss_epoch=8.150, train_rouge1_fmeasure_epoch=0.0372, train_rouge1_precision_epoch=0.0198, train_rouge1_recall_epoch=0.638, train_rouge2_fmeasure_epoch=0.0111, train_rouge2_precision_epoch=0.00578, train_rouge2_recall_epoch=0.286, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.504, train_rougeLsum_fmeasure_epoch=0.0365, train_rougeLsum_precision_epoch=0.0195, train_rougeLsum_recall_epoch=0.632]\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch ▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step ▄▃▄▂▁█▅▄▆▄▃▁▃▂▄▁▄▅▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step ▄▅▆▂▁█▄▄▅▃▂▁▄▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step ▄▄▅▂▁█▄▄▅▂▂▁▄▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step ▄▄▅▄▇▃▆▅▆▅██▃▆▅▇▁▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step ▅▆▅▂▄█▄▅▃▁▃▂█▂▄▆▅▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step ▅▆▅▂▄█▄▅▃▁▃▂█▂▄▆▅▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step ▂▂▂▃█▁▄▄▄▄▄▆▃▃▄▆▂▄▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step ▅▄▅▂▁█▄▄▄▃▂▁▄▂▂▃▄▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step ▄▄▅▂▁█▄▄▄▃▂▁▄▂▂▂▄▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step ▂▃▃▃▇▁▅▄▅▅▅█▂▆▄▆▁▃▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step ▄▅▆▂▁█▅▅▅▃▃▁▄▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step ▄▅▅▂▁█▅▅▅▂▂▁▄▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step ▄▄▅▄▇▃▆▅▆▅██▃▆▄▇▁▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch 8.14705\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch 0.03718\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch 0.01985\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch 0.63837\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch 0.01106\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch 0.00578\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch 0.28551\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch 0.02571\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch 0.0136\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch 0.50354\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch 0.03648\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch 0.01946\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch 0.63224\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch 6.22308\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step 5.77875\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch 0.04676\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step 0.01501\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch 0.02524\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step 0.00758\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch 0.63332\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step 0.77083\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch 0.01198\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step 0.00851\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch 0.00632\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step 0.00429\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch 0.28316\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step 0.48701\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch 0.02965\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step 0.01218\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch 0.01589\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step 0.00615\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch 0.469\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch 0.04575\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step 0.01501\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch 0.02468\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step 0.00758\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch 0.62688\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step 0.77083\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtough-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/8teo2ra1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240731_061549-8teo2ra1/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 05wv80ql with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001681911721139733\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: jjhsnail0822/danube-ko-1.8b-base\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240731_061909-05wv80ql\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mapricot-sweep-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/05wv80ql\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-31 06:19:20,900] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type                 | Params\n","-----------------------------------------------\n","0 | model | PeftModelForCausalLM | 1.5 B \n","-----------------------------------------------\n","1.1 M     Trainable params\n","1.5 B     Non-trainable params\n","1.5 B     Total params\n","6,183.574 Total estimated model params size (MB)\n","Epoch 0: 100% 121/121 [01:54<00:00,  1.06it/s, v_num=ql_1, train_loss_step=5.200, train_rouge1_fmeasure_step=0.0265, train_rouge1_precision_step=0.0135, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0169, train_rouge2_precision_step=0.00861, train_rouge2_recall_step=0.467, train_rougeL_fmeasure_step=0.0217, train_rougeL_precision_step=0.0111, train_rougeL_recall_step=0.562, train_rougeLsum_fmeasure_step=0.0265, train_rougeLsum_precision_step=0.0135, train_rougeLsum_recall_step=0.688]    \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/20 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.49it/s]\u001b[A\n","Validation DataLoader 0:  50% 10/20 [00:07<00:07,  1.40it/s]\u001b[A\n","Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.45it/s]\u001b[A\n","Validation DataLoader 0: 100% 20/20 [00:13<00:00,  1.47it/s]\u001b[A\n","Epoch 0: 100% 121/121 [02:08<00:00,  1.06s/it, v_num=ql_1, train_loss_step=5.200, train_rouge1_fmeasure_step=0.0265, train_rouge1_precision_step=0.0135, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0169, train_rouge2_precision_step=0.00861, train_rouge2_recall_step=0.467, train_rougeL_fmeasure_step=0.0217, train_rougeL_precision_step=0.0111, train_rougeL_recall_step=0.562, train_rougeLsum_fmeasure_step=0.0265, train_rougeLsum_precision_step=0.0135, train_rougeLsum_recall_step=0.688, val_loss_step=5.600, val_rouge1_fmeasure_step=0.0156, val_rouge1_precision_step=0.0079, val_rouge1_recall_step=0.792, val_rouge2_fmeasure_step=0.00873, val_rouge2_precision_step=0.0044, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0136, val_rougeL_precision_step=0.00688, val_rougeL_recall_step=0.688, val_rougeLsum_fmeasure_step=0.0156, val_rougeLsum_precision_step=0.0079, val_rougeLsum_recall_step=0.792, val_loss_epoch=6.120, val_rouge1_fmeasure_epoch=0.0466, val_rouge1_precision_epoch=0.0251, val_rouge1_recall_epoch=0.638, val_rouge2_fmeasure_epoch=0.0121, val_rouge2_precision_epoch=0.00638, val_rouge2_recall_epoch=0.286, val_rougeL_fmeasure_epoch=0.0297, val_rougeL_precision_epoch=0.0159, val_rougeL_recall_epoch=0.479, val_rougeLsum_fmeasure_epoch=0.0457, val_rougeLsum_precision_epoch=0.0246, val_rougeLsum_recall_epoch=0.630, train_loss_epoch=8.110, train_rouge1_fmeasure_epoch=0.0371, train_rouge1_precision_epoch=0.0198, train_rouge1_recall_epoch=0.643, train_rouge2_fmeasure_epoch=0.0111, train_rouge2_precision_epoch=0.00583, train_rouge2_recall_epoch=0.290, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.507, train_rougeLsum_fmeasure_epoch=0.0364, train_rougeLsum_precision_epoch=0.0194, train_rougeLsum_recall_epoch=0.636]`Trainer.fit` stopped: `max_epochs=1` reached.\n","Epoch 0: 100% 121/121 [02:26<00:00,  1.21s/it, v_num=ql_1, train_loss_step=5.200, train_rouge1_fmeasure_step=0.0265, train_rouge1_precision_step=0.0135, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0169, train_rouge2_precision_step=0.00861, train_rouge2_recall_step=0.467, train_rougeL_fmeasure_step=0.0217, train_rougeL_precision_step=0.0111, train_rougeL_recall_step=0.562, train_rougeLsum_fmeasure_step=0.0265, train_rougeLsum_precision_step=0.0135, train_rougeLsum_recall_step=0.688, val_loss_step=5.600, val_rouge1_fmeasure_step=0.0156, val_rouge1_precision_step=0.0079, val_rouge1_recall_step=0.792, val_rouge2_fmeasure_step=0.00873, val_rouge2_precision_step=0.0044, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0136, val_rougeL_precision_step=0.00688, val_rougeL_recall_step=0.688, val_rougeLsum_fmeasure_step=0.0156, val_rougeLsum_precision_step=0.0079, val_rougeLsum_recall_step=0.792, val_loss_epoch=6.120, val_rouge1_fmeasure_epoch=0.0466, val_rouge1_precision_epoch=0.0251, val_rouge1_recall_epoch=0.638, val_rouge2_fmeasure_epoch=0.0121, val_rouge2_precision_epoch=0.00638, val_rouge2_recall_epoch=0.286, val_rougeL_fmeasure_epoch=0.0297, val_rougeL_precision_epoch=0.0159, val_rougeL_recall_epoch=0.479, val_rougeLsum_fmeasure_epoch=0.0457, val_rougeLsum_precision_epoch=0.0246, val_rougeLsum_recall_epoch=0.630, train_loss_epoch=8.110, train_rouge1_fmeasure_epoch=0.0371, train_rouge1_precision_epoch=0.0198, train_rouge1_recall_epoch=0.643, train_rouge2_fmeasure_epoch=0.0111, train_rouge2_precision_epoch=0.00583, train_rouge2_recall_epoch=0.290, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.507, train_rougeLsum_fmeasure_epoch=0.0364, train_rougeLsum_precision_epoch=0.0194, train_rougeLsum_recall_epoch=0.636]\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch ▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step ▄▃▅▂▂█▅▄▆▃▃▁▃▂▄▁▄▅▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step ▄▄▅▂▁█▄▅▅▂▂▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step ▄▄▅▂▁█▄▅▅▂▂▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step ▃▃▄▄▇▂▅▅▆▃██▃▆▄▆▁▄▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step ▇▆▇▃▄█▄▆▅▁▅▃▇▃▄▅▆▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step ▆▆▇▂▃█▄▅▅▁▄▃▆▂▃▅▆▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step ▂▃▂▄█▁▄▅▄▄▅█▃▄▄▅▂▅▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step ▅▄▆▂▂█▄▄▄▃▂▁▄▂▂▃▄▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step ▄▄▅▂▁█▄▄▄▃▂▁▄▂▂▃▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step ▂▃▃▄▇▁▄▃▅▃▅█▁▅▄▆▁▂▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step ▄▅▆▂▁█▄▅▅▂▂▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step ▄▄▅▂▁█▄▅▅▂▂▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step ▃▃▄▄▇▂▅▅▆▃██▃▆▄▆▁▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch 8.10556\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch 0.03709\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch 0.01979\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch 0.64284\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch 0.01114\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch 0.00583\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch 0.29038\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch 0.02569\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch 0.01359\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch 0.5074\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch 0.03637\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch 0.0194\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch 0.63601\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch 6.1226\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step 5.60307\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch 0.04664\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step 0.01565\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch 0.02514\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step 0.0079\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch 0.63795\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step 0.79167\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch 0.01209\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step 0.00873\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch 0.00638\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step 0.0044\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch 0.28579\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step 0.48701\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch 0.02966\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step 0.01363\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch 0.01587\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step 0.00688\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch 0.4791\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch 0.04566\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step 0.01565\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch 0.02461\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step 0.0079\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch 0.62958\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step 0.79167\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mapricot-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/05wv80ql\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240731_061909-05wv80ql/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5m5fwjsn with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 7.185648902553063e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: OuteAI/Lite-Mistral-150M-v2-Instruct\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240731_062211-5m5fwjsn\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-sweep-3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/5m5fwjsn\u001b[0m\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-31 06:22:24,495] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type                 | Params\n","-----------------------------------------------\n","0 | model | PeftModelForCausalLM | 1.5 B \n","-----------------------------------------------\n","1.1 M     Trainable params\n","1.5 B     Non-trainable params\n","1.5 B     Total params\n","6,183.574 Total estimated model params size (MB)\n","Epoch 0: 100% 121/121 [01:54<00:00,  1.05it/s, v_num=sn_1, train_loss_step=5.280, train_rouge1_fmeasure_step=0.0253, train_rouge1_precision_step=0.0129, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0138, train_rouge2_precision_step=0.00703, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00936, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0253, train_rougeLsum_precision_step=0.0129, train_rougeLsum_recall_step=0.688]    \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/20 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.48it/s]\u001b[A\n","Validation DataLoader 0:  50% 10/20 [00:07<00:07,  1.39it/s]\u001b[A\n","Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.44it/s]\u001b[A\n","Validation DataLoader 0: 100% 20/20 [00:14<00:00,  1.43it/s]\u001b[A\n","Epoch 0: 100% 121/121 [02:08<00:00,  1.07s/it, v_num=sn_1, train_loss_step=5.280, train_rouge1_fmeasure_step=0.0253, train_rouge1_precision_step=0.0129, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0138, train_rouge2_precision_step=0.00703, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00936, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0253, train_rougeLsum_precision_step=0.0129, train_rougeLsum_recall_step=0.688, val_loss_step=5.790, val_rouge1_fmeasure_step=0.0148, val_rouge1_precision_step=0.00749, val_rouge1_recall_step=0.771, val_rouge2_fmeasure_step=0.0084, val_rouge2_precision_step=0.00424, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.012, val_rougeL_precision_step=0.00608, val_rougeL_recall_step=0.625, val_rougeLsum_fmeasure_step=0.0148, val_rougeLsum_precision_step=0.00749, val_rougeLsum_recall_step=0.771, val_loss_epoch=6.270, val_rouge1_fmeasure_epoch=0.0456, val_rouge1_precision_epoch=0.0245, val_rouge1_recall_epoch=0.637, val_rouge2_fmeasure_epoch=0.0116, val_rouge2_precision_epoch=0.00611, val_rouge2_recall_epoch=0.282, val_rougeL_fmeasure_epoch=0.0293, val_rougeL_precision_epoch=0.0157, val_rougeL_recall_epoch=0.477, val_rougeLsum_fmeasure_epoch=0.0446, val_rougeLsum_precision_epoch=0.024, val_rougeLsum_recall_epoch=0.631, train_loss_epoch=8.150, train_rouge1_fmeasure_epoch=0.037, train_rouge1_precision_epoch=0.0197, train_rouge1_recall_epoch=0.642, train_rouge2_fmeasure_epoch=0.011, train_rouge2_precision_epoch=0.00577, train_rouge2_recall_epoch=0.288, train_rougeL_fmeasure_epoch=0.0256, train_rougeL_precision_epoch=0.0135, train_rougeL_recall_epoch=0.502, train_rougeLsum_fmeasure_epoch=0.0364, train_rougeLsum_precision_epoch=0.0194, train_rougeLsum_recall_epoch=0.637]`Trainer.fit` stopped: `max_epochs=1` reached.\n","Epoch 0: 100% 121/121 [02:28<00:00,  1.23s/it, v_num=sn_1, train_loss_step=5.280, train_rouge1_fmeasure_step=0.0253, train_rouge1_precision_step=0.0129, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0138, train_rouge2_precision_step=0.00703, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0184, train_rougeL_precision_step=0.00936, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0253, train_rougeLsum_precision_step=0.0129, train_rougeLsum_recall_step=0.688, val_loss_step=5.790, val_rouge1_fmeasure_step=0.0148, val_rouge1_precision_step=0.00749, val_rouge1_recall_step=0.771, val_rouge2_fmeasure_step=0.0084, val_rouge2_precision_step=0.00424, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.012, val_rougeL_precision_step=0.00608, val_rougeL_recall_step=0.625, val_rougeLsum_fmeasure_step=0.0148, val_rougeLsum_precision_step=0.00749, val_rougeLsum_recall_step=0.771, val_loss_epoch=6.270, val_rouge1_fmeasure_epoch=0.0456, val_rouge1_precision_epoch=0.0245, val_rouge1_recall_epoch=0.637, val_rouge2_fmeasure_epoch=0.0116, val_rouge2_precision_epoch=0.00611, val_rouge2_recall_epoch=0.282, val_rougeL_fmeasure_epoch=0.0293, val_rougeL_precision_epoch=0.0157, val_rougeL_recall_epoch=0.477, val_rougeLsum_fmeasure_epoch=0.0446, val_rougeLsum_precision_epoch=0.024, val_rougeLsum_recall_epoch=0.631, train_loss_epoch=8.150, train_rouge1_fmeasure_epoch=0.037, train_rouge1_precision_epoch=0.0197, train_rouge1_recall_epoch=0.642, train_rouge2_fmeasure_epoch=0.011, train_rouge2_precision_epoch=0.00577, train_rouge2_recall_epoch=0.288, train_rougeL_fmeasure_epoch=0.0256, train_rougeL_precision_epoch=0.0135, train_rougeL_recall_epoch=0.502, train_rougeLsum_fmeasure_epoch=0.0364, train_rougeLsum_precision_epoch=0.0194, train_rougeLsum_recall_epoch=0.637]\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch ▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step ▄▃▅▂▂█▅▄▆▄▃▁▃▂▄▂▄▅▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step ▄▅▆▂▁█▄▅▅▃▂▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step ▄▅▆▂▁█▄▅▅▂▂▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step ▃▃▄▄▇▂▅▆▅▃██▃▇▄▇▁▃▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step ▆▆▄▃▄▇▃▅▄▂▃▂█▂▃▅▅▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step ▆▆▅▃▄▇▃▆▄▂▃▂█▂▃▅▅▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step ▂▂▂▄█▁▄▅▄▄▄▆▃▃▄▅▂▄▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step ▄▄▆▂▁█▄▄▄▃▂▁▄▂▂▃▄▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step ▄▄▅▂▁█▄▄▄▃▂▁▄▂▂▃▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step ▂▃▃▄▆▁▅▅▄▃▆█▂▅▄▇▁▃▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step ▄▅▆▂▁█▅▅▅▃▃▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step ▄▅▆▂▁█▅▅▅▃▂▁▅▂▂▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step ▂▄▄▄▇▂▅▆▅▃██▃▇▄▇▁▃▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch 8.14985\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch 0.03703\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch 0.01975\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch 0.64249\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch 0.01104\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch 0.00577\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch 0.28763\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch 0.02561\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch 0.01354\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch 0.50214\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch 0.03637\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch 0.01939\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch 0.63658\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch 6.26628\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step 5.78773\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch 0.04558\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step 0.01484\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch 0.02453\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step 0.00749\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch 0.63727\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step 0.77083\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch 0.01161\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step 0.0084\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch 0.00611\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step 0.00424\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch 0.28181\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step 0.48701\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch 0.02933\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step 0.01204\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch 0.01567\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step 0.00608\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch 0.47699\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch 0.0446\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step 0.01484\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch 0.02399\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step 0.00749\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch 0.63076\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step 0.77083\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33munique-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/5m5fwjsn\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240731_062211-5m5fwjsn/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 03z3piz9 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001110979881246672\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: numind/NuExtract\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240731_062508-03z3piz9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomic-sweep-4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/03z3piz9\u001b[0m\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-31 06:25:22,736] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type                 | Params\n","-----------------------------------------------\n","0 | model | PeftModelForCausalLM | 1.5 B \n","-----------------------------------------------\n","1.1 M     Trainable params\n","1.5 B     Non-trainable params\n","1.5 B     Total params\n","6,183.574 Total estimated model params size (MB)\n","Epoch 0: 100% 121/121 [01:54<00:00,  1.05it/s, v_num=z9_1, train_loss_step=5.390, train_rouge1_fmeasure_step=0.0258, train_rouge1_precision_step=0.0131, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0141, train_rouge2_precision_step=0.00718, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0188, train_rougeL_precision_step=0.00956, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0258, train_rougeLsum_precision_step=0.0131, train_rougeLsum_recall_step=0.688]  \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/20 [00:00<?, ?it/s]       \u001b[A\n","Validation DataLoader 0:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  25% 5/20 [00:03<00:10,  1.49it/s]\u001b[A\n","Validation DataLoader 0:  50% 10/20 [00:07<00:07,  1.40it/s]\u001b[A\n","Validation DataLoader 0:  75% 15/20 [00:10<00:03,  1.45it/s]\u001b[A\n","Validation DataLoader 0: 100% 20/20 [00:13<00:00,  1.47it/s]\u001b[A\n","Epoch 0: 100% 121/121 [02:08<00:00,  1.06s/it, v_num=z9_1, train_loss_step=5.390, train_rouge1_fmeasure_step=0.0258, train_rouge1_precision_step=0.0131, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0141, train_rouge2_precision_step=0.00718, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0188, train_rougeL_precision_step=0.00956, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0258, train_rougeLsum_precision_step=0.0131, train_rougeLsum_recall_step=0.688, val_loss_step=5.870, val_rouge1_fmeasure_step=0.0154, val_rouge1_precision_step=0.00779, val_rouge1_recall_step=0.792, val_rouge2_fmeasure_step=0.00862, val_rouge2_precision_step=0.00435, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0135, val_rougeL_precision_step=0.00679, val_rougeL_recall_step=0.688, val_rougeLsum_fmeasure_step=0.0154, val_rougeLsum_precision_step=0.00779, val_rougeLsum_recall_step=0.792, val_loss_epoch=6.290, val_rouge1_fmeasure_epoch=0.0464, val_rouge1_precision_epoch=0.025, val_rouge1_recall_epoch=0.635, val_rouge2_fmeasure_epoch=0.0119, val_rouge2_precision_epoch=0.00631, val_rouge2_recall_epoch=0.280, val_rougeL_fmeasure_epoch=0.0295, val_rougeL_precision_epoch=0.0158, val_rougeL_recall_epoch=0.477, val_rougeLsum_fmeasure_epoch=0.0454, val_rougeLsum_precision_epoch=0.0245, val_rougeLsum_recall_epoch=0.629, train_loss_epoch=8.170, train_rouge1_fmeasure_epoch=0.0373, train_rouge1_precision_epoch=0.0199, train_rouge1_recall_epoch=0.644, train_rouge2_fmeasure_epoch=0.0113, train_rouge2_precision_epoch=0.00591, train_rouge2_recall_epoch=0.294, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.505, train_rougeLsum_fmeasure_epoch=0.0365, train_rougeLsum_precision_epoch=0.0195, train_rougeLsum_recall_epoch=0.635]`Trainer.fit` stopped: `max_epochs=1` reached.\n","Epoch 0: 100% 121/121 [02:27<00:00,  1.22s/it, v_num=z9_1, train_loss_step=5.390, train_rouge1_fmeasure_step=0.0258, train_rouge1_precision_step=0.0131, train_rouge1_recall_step=0.688, train_rouge2_fmeasure_step=0.0141, train_rouge2_precision_step=0.00718, train_rouge2_recall_step=0.400, train_rougeL_fmeasure_step=0.0188, train_rougeL_precision_step=0.00956, train_rougeL_recall_step=0.500, train_rougeLsum_fmeasure_step=0.0258, train_rougeLsum_precision_step=0.0131, train_rougeLsum_recall_step=0.688, val_loss_step=5.870, val_rouge1_fmeasure_step=0.0154, val_rouge1_precision_step=0.00779, val_rouge1_recall_step=0.792, val_rouge2_fmeasure_step=0.00862, val_rouge2_precision_step=0.00435, val_rouge2_recall_step=0.487, val_rougeL_fmeasure_step=0.0135, val_rougeL_precision_step=0.00679, val_rougeL_recall_step=0.688, val_rougeLsum_fmeasure_step=0.0154, val_rougeLsum_precision_step=0.00779, val_rougeLsum_recall_step=0.792, val_loss_epoch=6.290, val_rouge1_fmeasure_epoch=0.0464, val_rouge1_precision_epoch=0.025, val_rouge1_recall_epoch=0.635, val_rouge2_fmeasure_epoch=0.0119, val_rouge2_precision_epoch=0.00631, val_rouge2_recall_epoch=0.280, val_rougeL_fmeasure_epoch=0.0295, val_rougeL_precision_epoch=0.0158, val_rougeL_recall_epoch=0.477, val_rougeLsum_fmeasure_epoch=0.0454, val_rougeLsum_precision_epoch=0.0245, val_rougeLsum_recall_epoch=0.629, train_loss_epoch=8.170, train_rouge1_fmeasure_epoch=0.0373, train_rouge1_precision_epoch=0.0199, train_rouge1_recall_epoch=0.644, train_rouge2_fmeasure_epoch=0.0113, train_rouge2_precision_epoch=0.00591, train_rouge2_recall_epoch=0.294, train_rougeL_fmeasure_epoch=0.0257, train_rougeL_precision_epoch=0.0136, train_rougeL_recall_epoch=0.505, train_rougeLsum_fmeasure_epoch=0.0365, train_rougeLsum_precision_epoch=0.0195, train_rougeLsum_recall_epoch=0.635]\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch ▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step ▄▃▅▂▂█▅▄▆▃▃▁▃▂▄▁▄▅▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step ▄▄▆▂▁█▄▅▅▃▂▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step ▄▄▅▂▁█▄▄▅▃▂▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step ▃▃▄▅▇▂▅▅▆▃██▃▆▅▇▁▃▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step ▅▅▅▂▅█▃▆▅▁▃▂█▂▃▅▄▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step ▅▅▅▂▄█▃▅▅▁▂▂▇▂▃▄▄▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step ▂▂▂▃█▁▃▅▄▃▃▆▃▃▃▅▂▄▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step ▄▄▅▂▁█▄▄▄▃▂▁▄▂▂▃▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step ▄▄▅▂▁█▄▄▄▃▂▁▄▂▂▃▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step ▂▂▃▄▆▁▄▅▅▃▅█▂▅▄▆▁▂▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step ▄▅▆▂▁█▄▅▅▃▃▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step ▄▅▆▂▁█▄▅▅▃▂▁▅▂▂▂▅▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step ▃▃▄▅▇▂▅▅▆▃██▃▆▄▇▁▃▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                           epoch 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss_epoch 8.16844\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge1_fmeasure_epoch 0.03729\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge1_precision_epoch 0.01989\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge1_recall_epoch 0.64352\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rouge2_fmeasure_epoch 0.01131\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rouge2_precision_epoch 0.00591\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rouge2_recall_epoch 0.29364\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_rougeL_fmeasure_epoch 0.02572\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeL_precision_epoch 0.0136\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_rougeL_recall_epoch 0.50454\n","\u001b[34m\u001b[1mwandb\u001b[0m:  train_rougeLsum_fmeasure_epoch 0.03648\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_rougeLsum_precision_epoch 0.01945\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_rougeLsum_recall_epoch 0.63509\n","\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/global_step 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss_epoch 6.29185\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   val_loss_step 5.86754\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_fmeasure_epoch 0.04639\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge1_fmeasure_step 0.01544\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge1_precision_epoch 0.02501\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge1_precision_step 0.00779\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge1_recall_epoch 0.63515\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge1_recall_step 0.79167\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_fmeasure_epoch 0.01195\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rouge2_fmeasure_step 0.00862\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rouge2_precision_epoch 0.00631\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rouge2_precision_step 0.00435\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rouge2_recall_epoch 0.27989\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rouge2_recall_step 0.48701\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_fmeasure_epoch 0.0295\n","\u001b[34m\u001b[1mwandb\u001b[0m:        val_rougeL_fmeasure_step 0.01345\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeL_precision_epoch 0.01579\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeL_precision_step 0.00679\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val_rougeL_recall_epoch 0.47696\n","\u001b[34m\u001b[1mwandb\u001b[0m:          val_rougeL_recall_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_fmeasure_epoch 0.04544\n","\u001b[34m\u001b[1mwandb\u001b[0m:     val_rougeLsum_fmeasure_step 0.01544\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_rougeLsum_precision_epoch 0.02448\n","\u001b[34m\u001b[1mwandb\u001b[0m:    val_rougeLsum_precision_step 0.00779\n","\u001b[34m\u001b[1mwandb\u001b[0m:      val_rougeLsum_recall_epoch 0.62941\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_rougeLsum_recall_step 0.79167\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcomic-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/03z3piz9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240731_062508-03z3piz9/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2upbjbyi with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0004886649233439632\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: HuggingFaceTB/SmolLM-135M\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240731_062806-2upbjbyi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvital-sweep-5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/sweeps/quoyhhvt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/2upbjbyi\u001b[0m\n"]}],"source":["#@title Start Sweep\n","\n","%%shell\n","export CUDA_VISIBLE_DEVICES=0\n","export CUDA_LAUNCH_BLOCKING=1\n","\n","python l_sweep.py"]},{"cell_type":"markdown","metadata":{"id":"OXFgDjet3ScB"},"source":["# Model Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"outputs":[],"source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ft7lndGneWWe"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyOEIz6q6DmICzxJnSE5mbx7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}