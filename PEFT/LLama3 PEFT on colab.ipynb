{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyM0HP56Dl0GF1+ADCAzA7pk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1719533197963,"user_tz":-540,"elapsed":643,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"c5a36de0-3f3a-4d4f-e11d-e4e5c0cb31f6"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf","executionInfo":{"status":"ok","timestamp":1719533207883,"user_tz":-540,"elapsed":9923,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iCRpjtCdPPRx","executionInfo":{"status":"ok","timestamp":1719532007814,"user_tz":-540,"elapsed":12079,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"572017b5-6ebd-4901-af7e-52c9f338893b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"google/gemma-7b\" # @param [\"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1719534360268,"user_tz":-540,"elapsed":378,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"c0a7e398-db09-4960-92c5-a5db73637a9e"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","from datasets import load_dataset\n","from evaluate import load\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","metric = load(\"rouge\")"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1719534488618,"user_tz":-540,"elapsed":554,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","outputId":"874c94b6-806d-4c96-eb64-11312f88aa81"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            return {\"should_training_stop\": True}\n","        else:\n","            return {}\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1719534502536,"user_tz":-540,"elapsed":437,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"85e2e3c2-76ac-4321-e859-143acd462412","cellView":"form"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"id":"iX1Vp8-2PQN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}