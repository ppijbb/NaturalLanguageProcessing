{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyM385GCWwe8aqg3rltzLpgr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi6ZScMwq0Lj","executionInfo":{"status":"ok","timestamp":1720136458261,"user_tz":-540,"elapsed":1021,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"8c1c5cfc-19b8-460e-9d8d-8c6a7318c771"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul  4 23:40:57 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   52C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]\n","deepspeed\n","colossalai\n","wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1720420595623,"user_tz":-540,"elapsed":519,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"bb7fb4fb-0ac5-43d6-fc67-7d58d9a29e51"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!CUDA_EXT=1 DS_BUILD=1 pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf","executionInfo":{"status":"ok","timestamp":1720420785135,"user_tz":-540,"elapsed":188649,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login --add-to-git-credential\n"],"metadata":{"id":"iCRpjtCdPPRx","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720421590737,"user_tz":-540,"elapsed":805610,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"4bef6727-296d-48fa-f8de-1417f4c2f553"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["#@title Weight and Bias Train Logger Login\n","#@markdown weight and bias 로그인\n","!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"3i2AiD0181GO","executionInfo":{"status":"ok","timestamp":1720421598599,"user_tz":-540,"elapsed":7866,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"b211605d-e0e5-4ee0-8ec4-65bad70ff1d4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown  |모델       | Normal   | DeepSpeed |\n","#@markdown  |---        | ---      | ---       |\n","#@markdown  |Llama3-8B  |  X       |   O       |\n","#@markdown  |Mistral-7B |  X       |   O       |\n","#@markdown  |Llama3-70B |  X       |   X       |\n","\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"meta-llama/Meta-Llama-3-8B\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1720421598600,"user_tz":-540,"elapsed":4,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"a830c33f-1652-42fa-e914-db4075d82717"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 85))\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 90))\n","\n","\n","def preprocess_function(sample, max_source_length, max_target_length, padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"],\n","                       max_length=max_target_length,\n","                       padding=padding,\n","                       truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else 1) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","dataset = dataset.map(preprocess_function,\n","                      batched=True,\n","                      remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                      fn_kwargs={\n","                          \"max_source_length\": max_source_length,\n","                           \"max_target_length\": max_source_length\n","                          },)\n","\n","if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    dataset = dataset.map(lambda x: x,\n","                          batched=True,\n","                          remove_columns=[\"token_type_ids\"], )\n"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1720421601503,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8a9c420-22af-474e-8ecc-d13c4da9f3d7","cellView":"form"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"adamw_hf\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1720421603201,"user_tz":-540,"elapsed":801,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"93a07318-53d2-4d78-e788-a631c2df6cd0","cellView":"form"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"id":"iX1Vp8-2PQN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training code to Lightning module"],"metadata":{"id":"d5KC2KSMllGB"}},{"cell_type":"code","source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","\n","import lightning as L\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, data_collator, train_batch_size, eval_batch_size,training_args,):\n","        super().__init__()\n","        self.train_dataset = dataset[\"train\"]\n","        self.val_dataset = dataset[\"validation\"]\n","        self.test_dataset = dataset[\"test\"]\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          num_workers=8,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSF7QaH1zhQ9","executionInfo":{"status":"ok","timestamp":1720421605269,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"d69511ac-f7ad-47d7-a425-6cdfaae6972d","cellView":"form"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_datamodule.py\n"]}]},{"cell_type":"code","source":["#@title Lightning Model\n","%%writefile l_model.py\n","\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from bitsandbytes.optim import AdamW, Lion\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","from finetuning_datasets import dataset\n","from torchmetrics.functional.text.rouge import rouge_score\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, data_collator, learning_rate: float = 2e-5):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.learning_rate = learning_rate\n","\n","    def _get_rouge_score(self, predictions, labels):\n","        generated_tokens = predictions.argmax(dim=-1)\n","        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        return rouge_score(preds=decoded_preds, target=decoded_labels)\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss,\n","                 prog_bar=True, on_step=True, on_epoch=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"train_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=True, on_epoch=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss,\n","                 prog_bar=True, on_step=True, on_epoch=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"val_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=False, on_epoch=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = Lion(params=self.model.parameters(),\n","                         lr=self.learning_rate,\n","                         weight_decay=0.01,\n","                         optim_bits=32,)\n","        scheduler = CosineAnnealingWarmRestarts(optimizer,\n","                                                T_0=10,\n","                                                T_mult=2,\n","                                                eta_min=0.00001)\n","        # scheduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"min\")\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"monitor\": \"val_loss\",\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","\n","            },\n","        }\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agrktrAylkal","executionInfo":{"status":"ok","timestamp":1720420300856,"user_tz":-540,"elapsed":1078,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"e54e7a61-4ab0-4530-8b67-419beb03211a","cellView":"form"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_model.py\n"]}]},{"cell_type":"code","source":["#@title Trainer\n","%%writefile l_trainer.py\n","\n","import os\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n","from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","\n","\n","if __name__ == \"__main__\":\n","    L.pytorch.cli_lightning_logo()\n","    training_args = LightningArgumentParser()\n","    cli = LightningCLI(\n","        model_class=LLamaFTLightningModule,\n","        datamodule_class=FTDataModule,\n","        seed_everything_default=42,\n","        trainer_defaults={\n","            \"reload_dataloaders_every_n_epochs\": 1,\n","            \"strategy\": \"deepspeed\",\n","            \"precision\": \"bf16-mixed\",\n","            \"profiler\": \"PassThroughProfiler\",\n","            \"logger\": [WandbLogger(project=\"LLM-Finetuning\"),],\n","            \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=5), LearningRateMonitor()]\n","        },\n","        save_config_callback=None)\n","    # cli.add_arguments_to_parser(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZzH2TmzqP-","executionInfo":{"status":"ok","timestamp":1720421606354,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"21b7bd05-db72-44f6-84bf-3da6dc5952e6","cellView":"form"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing l_trainer.py\n"]}]},{"cell_type":"code","source":["#@title Start Training\n","#@markdown 실험 결과\n","#@markdown\n","#@markdown * batch_size <b>2</b> 넘기는 경우 OOM\n","#@markdown * DeepSpeed의 경우 GPU Ram 20GB로 7B finetuning 가능\n","#@markdown * DeepSpeed의 경우, 7B L4 GPU에서 사용 가능\n","#@markdown * 70B의 경우 RAM에서 Weight 가져오다 OOM\n","%%shell\n","\n","python l_trainer.py fit \\\n","    --trainer.max_epochs 4 \\\n","    --model.learning_rate 5e-5 \\\n","    --data.train_batch_size 2 \\\n","    --data.eval_batch_size 2\n","\n","#    --trainer.fast_dev_run 1\\"],"metadata":{"id":"RYALPHD3PyBD","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","outputId":"e1fc2b64-ca2e-4aab-a7ee-c9fed8772753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-08 06:53:34.078670: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-08 06:53:34.078720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-08 06:53:34.080150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-08 06:53:35.306934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","config.json: 100% 654/654 [00:00<00:00, 3.08MB/s]\n","model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 85.6MB/s]\n","Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n","model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00004.safetensors:   1% 41.9M/4.98G [00:00<00:14, 334MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   2% 94.4M/4.98G [00:00<00:12, 401MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   3% 147M/4.98G [00:00<00:10, 439MB/s] \u001b[A\n","model-00001-of-00004.safetensors:   4% 210M/4.98G [00:00<00:10, 473MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   5% 262M/4.98G [00:00<00:09, 487MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   7% 325M/4.98G [00:00<00:09, 508MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   8% 388M/4.98G [00:00<00:08, 530MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   9% 451M/4.98G [00:00<00:08, 545MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  10% 514M/4.98G [00:01<00:08, 548MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  12% 577M/4.98G [00:01<00:08, 522MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  13% 640M/4.98G [00:01<00:08, 522MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  14% 703M/4.98G [00:01<00:08, 520MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  15% 755M/4.98G [00:01<00:08, 515MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  16% 807M/4.98G [00:01<00:08, 505MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  17% 860M/4.98G [00:01<00:08, 505MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  18% 912M/4.98G [00:01<00:08, 485MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  19% 965M/4.98G [00:01<00:08, 496MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  20% 1.02G/4.98G [00:02<00:08, 491MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  21% 1.07G/4.98G [00:02<00:07, 498MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  23% 1.13G/4.98G [00:02<00:07, 506MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  24% 1.18G/4.98G [00:02<00:07, 504MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:02<00:07, 505MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  26% 1.29G/4.98G [00:02<00:07, 504MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  27% 1.34G/4.98G [00:02<00:07, 506MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  28% 1.39G/4.98G [00:02<00:07, 502MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  29% 1.45G/4.98G [00:02<00:07, 503MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  30% 1.50G/4.98G [00:03<00:07, 436MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  31% 1.55G/4.98G [00:03<00:08, 420MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  32% 1.60G/4.98G [00:03<00:09, 347MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  33% 1.65G/4.98G [00:03<00:10, 328MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  34% 1.69G/4.98G [00:03<00:11, 297MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  35% 1.73G/4.98G [00:03<00:13, 237MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  35% 1.76G/4.98G [00:04<00:16, 199MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  36% 1.79G/4.98G [00:04<00:18, 171MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  36% 1.81G/4.98G [00:04<00:20, 157MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  37% 1.84G/4.98G [00:04<00:21, 150MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  37% 1.86G/4.98G [00:05<00:22, 136MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  38% 1.88G/4.98G [00:05<00:23, 129MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  38% 1.90G/4.98G [00:05<00:23, 132MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  39% 1.95G/4.98G [00:05<00:15, 201MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  40% 2.00G/4.98G [00:05<00:11, 261MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  41% 2.06G/4.98G [00:05<00:09, 309MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  42% 2.11G/4.98G [00:05<00:08, 347MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  43% 2.16G/4.98G [00:05<00:07, 378MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  44% 2.21G/4.98G [00:06<00:06, 405MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  46% 2.26G/4.98G [00:06<00:06, 421MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  47% 2.32G/4.98G [00:06<00:06, 439MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  48% 2.37G/4.98G [00:06<00:05, 456MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  49% 2.42G/4.98G [00:06<00:05, 468MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  50% 2.49G/4.98G [00:06<00:05, 490MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  51% 2.55G/4.98G [00:06<00:04, 505MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  52% 2.60G/4.98G [00:06<00:04, 503MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  54% 2.66G/4.98G [00:06<00:04, 509MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  55% 2.72G/4.98G [00:07<00:04, 503MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  56% 2.78G/4.98G [00:07<00:04, 509MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  57% 2.83G/4.98G [00:07<00:04, 510MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  58% 2.89G/4.98G [00:07<00:03, 523MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  59% 2.95G/4.98G [00:07<00:03, 519MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  60% 3.00G/4.98G [00:07<00:03, 499MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  61% 3.05G/4.98G [00:07<00:03, 492MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  62% 3.10G/4.98G [00:07<00:04, 440MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  63% 3.16G/4.98G [00:08<00:04, 367MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  64% 3.20G/4.98G [00:08<00:05, 312MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  65% 3.24G/4.98G [00:08<00:05, 318MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  66% 3.28G/4.98G [00:08<00:05, 327MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  67% 3.32G/4.98G [00:08<00:04, 342MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  68% 3.37G/4.98G [00:08<00:04, 329MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  68% 3.41G/4.98G [00:08<00:04, 348MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  69% 3.45G/4.98G [00:09<00:04, 336MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  70% 3.49G/4.98G [00:09<00:05, 295MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  71% 3.52G/4.98G [00:09<00:05, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  71% 3.55G/4.98G [00:09<00:05, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  72% 3.59G/4.98G [00:09<00:05, 258MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  73% 3.62G/4.98G [00:09<00:05, 257MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  73% 3.65G/4.98G [00:09<00:05, 251MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  74% 3.68G/4.98G [00:09<00:05, 238MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  75% 3.71G/4.98G [00:10<00:07, 165MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  76% 3.76G/4.98G [00:10<00:05, 223MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  77% 3.82G/4.98G [00:10<00:04, 275MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  78% 3.87G/4.98G [00:10<00:03, 319MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  79% 3.92G/4.98G [00:10<00:02, 355MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  80% 3.97G/4.98G [00:10<00:02, 379MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  81% 4.03G/4.98G [00:11<00:02, 397MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  82% 4.07G/4.98G [00:11<00:02, 393MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  83% 4.11G/4.98G [00:11<00:02, 394MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  84% 4.16G/4.98G [00:11<00:01, 409MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  85% 4.22G/4.98G [00:11<00:01, 416MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  86% 4.27G/4.98G [00:11<00:01, 381MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  87% 4.31G/4.98G [00:11<00:01, 390MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  88% 4.36G/4.98G [00:11<00:01, 403MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  89% 4.41G/4.98G [00:11<00:01, 414MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  90% 4.47G/4.98G [00:12<00:01, 428MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  91% 4.52G/4.98G [00:12<00:01, 431MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  92% 4.57G/4.98G [00:12<00:00, 432MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  93% 4.62G/4.98G [00:12<00:00, 443MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  94% 4.68G/4.98G [00:12<00:00, 455MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  95% 4.73G/4.98G [00:12<00:00, 465MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  96% 4.78G/4.98G [00:12<00:00, 447MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  97% 4.83G/4.98G [00:12<00:00, 420MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  98% 4.89G/4.98G [00:13<00:00, 376MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  99% 4.93G/4.98G [00:13<00:00, 293MB/s]\u001b[A\n","model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:13<00:00, 366MB/s]\n","Downloading shards:  25% 1/4 [00:13<00:41, 13.90s/it]\n","model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00004.safetensors:   1% 41.9M/5.00G [00:00<00:12, 403MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   2% 94.4M/5.00G [00:00<00:11, 445MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 147M/5.00G [00:00<00:12, 399MB/s] \u001b[A\n","model-00002-of-00004.safetensors:   4% 199M/5.00G [00:00<00:11, 415MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 252M/5.00G [00:00<00:10, 441MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 304M/5.00G [00:00<00:10, 450MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 357M/5.00G [00:00<00:11, 389MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 398M/5.00G [00:00<00:11, 385MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 440M/5.00G [00:01<00:12, 369MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 482M/5.00G [00:01<00:12, 374MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 524M/5.00G [00:01<00:11, 379MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 577M/5.00G [00:01<00:11, 399MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 619M/5.00G [00:01<00:11, 391MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 671M/5.00G [00:01<00:10, 408MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 724M/5.00G [00:01<00:09, 438MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 776M/5.00G [00:01<00:09, 427MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 828M/5.00G [00:02<00:10, 386MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 870M/5.00G [00:02<00:10, 383MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 912M/5.00G [00:02<00:11, 361MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 954M/5.00G [00:02<00:10, 369MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 996M/5.00G [00:02<00:11, 362MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.05G/5.00G [00:02<00:10, 385MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.10G/5.00G [00:02<00:09, 394MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:02<00:09, 393MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.18G/5.00G [00:03<00:09, 393MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.23G/5.00G [00:03<00:10, 374MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.27G/5.00G [00:03<00:09, 384MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.31G/5.00G [00:03<00:09, 383MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.36G/5.00G [00:03<00:09, 396MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.42G/5.00G [00:03<00:08, 406MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.47G/5.00G [00:03<00:08, 418MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.51G/5.00G [00:03<00:08, 411MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.55G/5.00G [00:03<00:08, 402MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.59G/5.00G [00:04<00:08, 406MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.65G/5.00G [00:04<00:08, 407MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.70G/5.00G [00:04<00:08, 412MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.74G/5.00G [00:04<00:08, 404MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.78G/5.00G [00:04<00:08, 387MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.82G/5.00G [00:04<00:08, 379MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.88G/5.00G [00:04<00:07, 398MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.92G/5.00G [00:04<00:08, 367MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.97G/5.00G [00:04<00:07, 389MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 2.02G/5.00G [00:05<00:07, 418MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.08G/5.00G [00:05<00:06, 426MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.13G/5.00G [00:05<00:06, 423MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.18G/5.00G [00:05<00:06, 420MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.23G/5.00G [00:05<00:06, 421MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.29G/5.00G [00:05<00:06, 413MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.33G/5.00G [00:05<00:06, 414MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.37G/5.00G [00:05<00:06, 400MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.41G/5.00G [00:06<00:06, 397MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.46G/5.00G [00:06<00:06, 400MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.52G/5.00G [00:06<00:05, 419MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.57G/5.00G [00:06<00:05, 428MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.62G/5.00G [00:06<00:08, 280MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.67G/5.00G [00:06<00:07, 313MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.72G/5.00G [00:06<00:06, 332MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.77G/5.00G [00:07<00:06, 367MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.82G/5.00G [00:07<00:05, 395MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.87G/5.00G [00:07<00:05, 386MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.92G/5.00G [00:07<00:05, 387MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.96G/5.00G [00:07<00:05, 387MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 3.01G/5.00G [00:07<00:04, 415MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.06G/5.00G [00:07<00:04, 413MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.11G/5.00G [00:07<00:04, 380MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.16G/5.00G [00:08<00:05, 345MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.20G/5.00G [00:08<00:05, 327MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.24G/5.00G [00:08<00:05, 323MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.28G/5.00G [00:08<00:04, 346MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.33G/5.00G [00:08<00:04, 376MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.39G/5.00G [00:08<00:03, 404MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.43G/5.00G [00:08<00:03, 395MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.47G/5.00G [00:08<00:03, 395MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.51G/5.00G [00:09<00:03, 401MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.55G/5.00G [00:09<00:03, 379MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.60G/5.00G [00:09<00:04, 311MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.65G/5.00G [00:09<00:03, 350MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.69G/5.00G [00:09<00:03, 364MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.74G/5.00G [00:09<00:03, 390MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.80G/5.00G [00:09<00:02, 408MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.84G/5.00G [00:09<00:02, 408MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.88G/5.00G [00:09<00:02, 408MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.92G/5.00G [00:10<00:02, 400MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.97G/5.00G [00:10<00:02, 410MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.03G/5.00G [00:10<00:02, 419MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.08G/5.00G [00:10<00:02, 416MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.13G/5.00G [00:10<00:01, 436MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.18G/5.00G [00:10<00:01, 439MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.25G/5.00G [00:10<00:01, 472MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.30G/5.00G [00:10<00:01, 484MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.35G/5.00G [00:11<00:01, 454MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.40G/5.00G [00:11<00:01, 446MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.46G/5.00G [00:11<00:01, 418MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.51G/5.00G [00:11<00:01, 381MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.55G/5.00G [00:11<00:01, 349MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.59G/5.00G [00:11<00:01, 336MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.63G/5.00G [00:11<00:01, 345MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.68G/5.00G [00:11<00:00, 357MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.72G/5.00G [00:14<00:04, 56.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.75G/5.00G [00:14<00:04, 59.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.77G/5.00G [00:14<00:03, 63.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.79G/5.00G [00:15<00:02, 70.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.81G/5.00G [00:15<00:02, 77.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.83G/5.00G [00:15<00:01, 87.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.85G/5.00G [00:15<00:01, 96.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.88G/5.00G [00:15<00:01, 101MB/s] \u001b[A\n","model-00002-of-00004.safetensors:  98% 4.90G/5.00G [00:16<00:01, 85.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.92G/5.00G [00:16<00:00, 93.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:16<00:00, 303MB/s]\n","Downloading shards:  50% 2/4 [00:30<00:31, 15.58s/it]\n","model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n","model-00003-of-00004.safetensors:   1% 41.9M/4.92G [00:00<00:12, 384MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   2% 94.4M/4.92G [00:00<00:10, 446MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   3% 147M/4.92G [00:00<00:10, 457MB/s] \u001b[A\n","model-00003-of-00004.safetensors:   4% 199M/4.92G [00:00<00:09, 473MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   5% 252M/4.92G [00:00<00:09, 476MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   6% 304M/4.92G [00:00<00:10, 440MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   7% 357M/4.92G [00:00<00:10, 430MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   8% 409M/4.92G [00:00<00:10, 439MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   9% 461M/4.92G [00:01<00:10, 444MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  10% 514M/4.92G [00:01<00:10, 434MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  12% 566M/4.92G [00:01<00:09, 436MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  13% 619M/4.92G [00:01<00:09, 440MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  14% 671M/4.92G [00:01<00:09, 452MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  15% 724M/4.92G [00:01<00:09, 463MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  16% 776M/4.92G [00:01<00:09, 455MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  17% 828M/4.92G [00:02<00:13, 306MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  18% 891M/4.92G [00:02<00:11, 363MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  19% 954M/4.92G [00:02<00:09, 411MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  21% 1.02G/4.92G [00:02<00:08, 440MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  22% 1.07G/4.92G [00:02<00:08, 455MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  23% 1.12G/4.92G [00:02<00:08, 440MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  24% 1.17G/4.92G [00:02<00:10, 360MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  25% 1.22G/4.92G [00:02<00:11, 332MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  26% 1.26G/4.92G [00:03<00:11, 309MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  26% 1.30G/4.92G [00:03<00:12, 281MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  27% 1.33G/4.92G [00:03<00:13, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  28% 1.36G/4.92G [00:03<00:14, 254MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  29% 1.41G/4.92G [00:03<00:12, 289MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  29% 1.45G/4.92G [00:03<00:10, 318MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  30% 1.49G/4.92G [00:03<00:09, 343MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  31% 1.53G/4.92G [00:04<00:09, 363MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  32% 1.57G/4.92G [00:04<00:08, 378MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  33% 1.63G/4.92G [00:04<00:08, 403MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  34% 1.68G/4.92G [00:04<00:07, 416MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  35% 1.73G/4.92G [00:04<00:07, 423MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  36% 1.78G/4.92G [00:04<00:07, 433MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  37% 1.84G/4.92G [00:04<00:07, 408MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  38% 1.88G/4.92G [00:04<00:07, 400MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  39% 1.93G/4.92G [00:04<00:07, 414MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  40% 1.97G/4.92G [00:05<00:07, 413MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  41% 2.01G/4.92G [00:05<00:08, 326MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  42% 2.07G/4.92G [00:05<00:07, 366MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  43% 2.12G/4.92G [00:05<00:06, 400MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  44% 2.17G/4.92G [00:05<00:06, 419MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  45% 2.22G/4.92G [00:05<00:06, 418MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  46% 2.28G/4.92G [00:05<00:06, 408MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  47% 2.32G/4.92G [00:05<00:06, 405MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  48% 2.36G/4.92G [00:06<00:06, 393MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.41G/4.92G [00:06<00:05, 418MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  50% 2.46G/4.92G [00:06<00:05, 434MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.52G/4.92G [00:06<00:05, 417MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.57G/4.92G [00:06<00:05, 396MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  53% 2.61G/4.92G [00:06<00:05, 388MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  54% 2.66G/4.92G [00:06<00:05, 408MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  55% 2.71G/4.92G [00:06<00:05, 397MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  56% 2.75G/4.92G [00:07<00:05, 367MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  57% 2.80G/4.92G [00:07<00:05, 391MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  58% 2.85G/4.92G [00:07<00:04, 417MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  59% 2.90G/4.92G [00:07<00:04, 425MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  60% 2.96G/4.92G [00:07<00:04, 431MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  61% 3.01G/4.92G [00:07<00:04, 412MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  62% 3.05G/4.92G [00:07<00:04, 392MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  63% 3.10G/4.92G [00:07<00:04, 411MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  64% 3.15G/4.92G [00:08<00:04, 399MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  65% 3.19G/4.92G [00:08<00:04, 369MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  66% 3.23G/4.92G [00:08<00:04, 345MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  67% 3.28G/4.92G [00:08<00:04, 375MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.32G/4.92G [00:08<00:04, 379MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.37G/4.92G [00:08<00:04, 378MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  69% 3.41G/4.92G [00:08<00:04, 345MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  70% 3.45G/4.92G [00:08<00:04, 354MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  71% 3.49G/4.92G [00:08<00:04, 349MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  72% 3.53G/4.92G [00:09<00:03, 346MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  73% 3.58G/4.92G [00:09<00:03, 349MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.62G/4.92G [00:09<00:03, 343MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.66G/4.92G [00:09<00:03, 354MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  75% 3.70G/4.92G [00:09<00:04, 300MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  76% 3.74G/4.92G [00:09<00:04, 291MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  77% 3.77G/4.92G [00:09<00:03, 295MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  77% 3.81G/4.92G [00:10<00:03, 286MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  78% 3.84G/4.92G [00:10<00:04, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  79% 3.88G/4.92G [00:10<00:03, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  80% 3.93G/4.92G [00:10<00:02, 329MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  81% 3.98G/4.92G [00:10<00:02, 364MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  82% 4.03G/4.92G [00:10<00:02, 374MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  83% 4.08G/4.92G [00:10<00:02, 394MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  84% 4.13G/4.92G [00:10<00:01, 396MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  85% 4.17G/4.92G [00:11<00:01, 387MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  86% 4.22G/4.92G [00:11<00:01, 387MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  87% 4.26G/4.92G [00:11<00:01, 391MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  88% 4.31G/4.92G [00:11<00:01, 414MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  89% 4.36G/4.92G [00:11<00:01, 432MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  90% 4.41G/4.92G [00:11<00:01, 443MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  91% 4.47G/4.92G [00:11<00:00, 452MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  92% 4.52G/4.92G [00:11<00:00, 459MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  93% 4.57G/4.92G [00:11<00:00, 417MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  94% 4.62G/4.92G [00:12<00:00, 398MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  95% 4.67G/4.92G [00:12<00:00, 392MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  96% 4.71G/4.92G [00:12<00:00, 393MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.75G/4.92G [00:12<00:00, 397MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.79G/4.92G [00:12<00:00, 398MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  98% 4.83G/4.92G [00:12<00:00, 386MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  99% 4.88G/4.92G [00:12<00:00, 394MB/s]\u001b[A\n","model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:12<00:00, 382MB/s]\n","Downloading shards:  75% 3/4 [00:43<00:14, 14.46s/it]\n","model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n","model-00004-of-00004.safetensors:   4% 41.9M/1.17G [00:00<00:03, 346MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   7% 83.9M/1.17G [00:00<00:02, 371MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  11% 126M/1.17G [00:00<00:02, 380MB/s] \u001b[A\n","model-00004-of-00004.safetensors:  14% 168M/1.17G [00:00<00:04, 225MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  17% 199M/1.17G [00:00<00:03, 243MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  20% 231M/1.17G [00:00<00:03, 261MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  23% 273M/1.17G [00:00<00:03, 298MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  28% 325M/1.17G [00:01<00:02, 346MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  31% 367M/1.17G [00:01<00:02, 323MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  35% 409M/1.17G [00:01<00:02, 345MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  39% 451M/1.17G [00:01<00:01, 361MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  43% 503M/1.17G [00:01<00:01, 383MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  48% 556M/1.17G [00:01<00:01, 414MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  52% 608M/1.17G [00:02<00:05, 111MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  57% 661M/1.17G [00:02<00:03, 147MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  62% 724M/1.17G [00:03<00:02, 197MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  67% 786M/1.17G [00:03<00:01, 247MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  72% 839M/1.17G [00:03<00:01, 287MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  76% 891M/1.17G [00:03<00:00, 299MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  80% 933M/1.17G [00:03<00:00, 319MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  83% 975M/1.17G [00:03<00:00, 327MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  88% 1.03G/1.17G [00:03<00:00, 355MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  92% 1.08G/1.17G [00:03<00:00, 381MB/s]\u001b[A\n","model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:04<00:00, 283MB/s]\n","Downloading shards: 100% 4/4 [00:48<00:00, 12.09s/it]\n","Loading checkpoint shards: 100% 4/4 [00:07<00:00,  1.83s/it]\n","generation_config.json: 100% 177/177 [00:00<00:00, 1.07MB/s]\n","tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 83.7MB/s]\n","tokenizer.json: 100% 9.09M/9.09M [00:01<00:00, 8.35MB/s]\n","special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 423kB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Downloading builder script: 100% 3.36k/3.36k [00:00<00:00, 18.0MB/s]\n","Downloading readme: 100% 7.04k/7.04k [00:00<00:00, 25.6MB/s]\n","Downloading data: 100% 2.94M/2.94M [00:00<00:00, 74.8MB/s]\n","Generating train split: 100% 14732/14732 [00:00<00:00, 17351.76 examples/s]\n","Generating test split: 100% 819/819 [00:00<00:00, 3101.37 examples/s]\n","Generating validation split: 100% 818/818 [00:00<00:00, 3226.72 examples/s]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 20.6MB/s]\n","Map:   0% 0/15551 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Map: 100% 15551/15551 [00:01<00:00, 8679.92 examples/s]\n","Map: 100% 15551/15551 [00:00<00:00, 36379.89 examples/s]\n","Map: 100% 14732/14732 [00:06<00:00, 2291.38 examples/s]\n","Map: 100% 819/819 [00:00<00:00, 2317.76 examples/s]\n","Map: 100% 818/818 [00:00<00:00, 2336.56 examples/s]\n","\n","\u001b[0;35m\n","                    ####\n","                ###########\n","             ####################\n","         ############################\n","    #####################################\n","##############################################\n","#########################  ###################\n","#######################    ###################\n","####################      ####################\n","##################       #####################\n","################        ######################\n","#####################        #################\n","######################     ###################\n","#####################    #####################\n","####################   #######################\n","###################  #########################\n","##############################################\n","    #####################################\n","         ############################\n","             ####################\n","                  ##########\n","                     ####\n","\u001b[0m\n","\n","Seed set to 42\n","/usr/local/lib/python3.10/dist-packages/jsonargparse/_typehints.py:1439: JsonargparseWarning: \n","    Unable to serialize instance <lightning.pytorch.loggers.wandb.WandbLogger object at 0x7e3227cc22c0>\n","\n","  warning(val)\n","[2024-07-08 06:54:59,456] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","[rank: 0] Seed set to 42\n","initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevintb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240708_065502-18902ewr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-wind-13\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kevintb/LLM-Finetuning/runs/18902ewr\u001b[0m\n","Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","[2024-07-08 06:55:13,148] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","\n","  | Name  | Type             | Params | Mode\n","--------------------------------------------------\n","0 | model | LlamaForCausalLM | 8.0 B  | eval\n","--------------------------------------------------\n","3.4 M     Trainable params\n","8.0 B     Non-trainable params\n","8.0 B     Total params\n","32,134.676Total estimated model params size (MB)\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Sanity Checking DataLoader 0: 100% 2/2 [00:02<00:00,  1.25s/it]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge1_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rouge2_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeL_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_rougeLsum_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","Epoch 0: 100% 7366/7366 [55:05<00:00,  2.23it/s, v_num=2ewr, train_loss_step=0.823, train_rouge1_fmeasure_step=0.154, train_rouge1_precision_step=0.393, train_rouge1_recall_step=0.0981, train_rouge2_fmeasure_step=0.000, train_rouge2_precision_step=0.000, train_rouge2_recall_step=0.000, train_rougeL_fmeasure_step=0.128, train_rougeL_precision_step=0.310, train_rougeL_recall_step=0.0829, train_rougeLsum_fmeasure_step=0.154, train_rougeLsum_precision_step=0.393, train_rougeLsum_recall_step=0.0981]   \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/409 [00:00<?, ?it/s]      \u001b[A\n","Validation DataLoader 0:   0% 0/409 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   5% 20/409 [00:04<01:18,  4.95it/s]\u001b[A\n","Validation DataLoader 0:  10% 40/409 [00:08<01:14,  4.94it/s]\u001b[A\n","Validation DataLoader 0:  15% 60/409 [00:12<01:10,  4.93it/s]\u001b[A\n","Validation DataLoader 0:  20% 80/409 [00:16<01:06,  4.93it/s]\u001b[A\n","Validation DataLoader 0:  24% 100/409 [00:20<01:02,  4.93it/s]\u001b[A\n","Validation DataLoader 0:  29% 120/409 [00:24<00:58,  4.93it/s]\u001b[A\n","Validation DataLoader 0:  34% 140/409 [00:28<00:54,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  39% 160/409 [00:32<00:50,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  44% 180/409 [00:36<00:46,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  49% 200/409 [00:40<00:42,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  54% 220/409 [00:44<00:38,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  59% 240/409 [00:48<00:34,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  64% 260/409 [00:52<00:30,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  68% 280/409 [00:56<00:26,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  73% 300/409 [01:00<00:22,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  78% 320/409 [01:04<00:18,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  83% 340/409 [01:09<00:14,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  88% 360/409 [01:13<00:09,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  93% 380/409 [01:17<00:05,  4.92it/s]\u001b[A\n","Validation DataLoader 0:  98% 400/409 [01:21<00:01,  4.92it/s]\u001b[A\n","Validation DataLoader 0: 100% 409/409 [01:23<00:00,  4.92it/s]\u001b[A\n","Epoch 0: 100% 7366/7366 [56:29<00:00,  2.17it/s, v_num=2ewr, train_loss_step=0.823, train_rouge1_fmeasure_step=0.154, train_rouge1_precision_step=0.393, train_rouge1_recall_step=0.0981, train_rouge2_fmeasure_step=0.000, train_rouge2_precision_step=0.000, train_rouge2_recall_step=0.000, train_rougeL_fmeasure_step=0.128, train_rougeL_precision_step=0.310, train_rougeL_recall_step=0.0829, train_rougeLsum_fmeasure_step=0.154, train_rougeLsum_precision_step=0.393, train_rougeLsum_recall_step=0.0981, val_loss_step=0.827, val_loss_epoch=0.789, val_rouge1_fmeasure=0.114, val_rouge1_precision=0.261, val_rouge1_recall=0.0781, val_rouge2_fmeasure=0.000435, val_rouge2_precision=0.00147, val_rouge2_recall=0.00026, val_rougeL_fmeasure=0.0986, val_rougeL_precision=0.222, val_rougeL_recall=0.0683, val_rougeLsum_fmeasure=0.106, val_rougeLsum_precision=0.242, val_rougeLsum_recall=0.0728]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge1_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge1_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge1_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge2_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge2_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rouge2_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeL_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeL_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeL_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeLsum_fmeasure', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeLsum_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_rougeLsum_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","Epoch 1:  17% 1240/7366 [09:17<45:52,  2.23it/s, v_num=2ewr, train_loss_step=0.622, train_rouge1_fmeasure_step=0.136, train_rouge1_precision_step=0.283, train_rouge1_recall_step=0.0972, train_rouge2_fmeasure_step=0.000, train_rouge2_precision_step=0.000, train_rouge2_recall_step=0.000, train_rougeL_fmeasure_step=0.136, train_rougeL_precision_step=0.283, train_rougeL_recall_step=0.0972, train_rougeLsum_fmeasure_step=0.136, train_rougeLsum_precision_step=0.283, train_rougeLsum_recall_step=0.0972, val_loss_step=0.827, val_loss_epoch=0.789, val_rouge1_fmeasure=0.114, val_rouge1_precision=0.261, val_rouge1_recall=0.0781, val_rouge2_fmeasure=0.000435, val_rouge2_precision=0.00147, val_rouge2_recall=0.00026, val_rougeL_fmeasure=0.0986, val_rougeL_precision=0.222, val_rougeL_recall=0.0683, val_rougeLsum_fmeasure=0.106, val_rougeLsum_precision=0.242, val_rougeLsum_recall=0.0728, train_loss_epoch=0.850, train_rouge1_fmeasure_epoch=0.119, train_rouge1_precision_epoch=0.271, train_rouge1_recall_epoch=0.0845, train_rouge2_fmeasure_epoch=0.000985, train_rouge2_precision_epoch=0.00238, train_rouge2_recall_epoch=0.000968, train_rougeL_fmeasure_epoch=0.102, train_rougeL_precision_epoch=0.228, train_rougeL_recall_epoch=0.0721, train_rougeLsum_fmeasure_epoch=0.108, train_rougeLsum_precision_epoch=0.246, train_rougeLsum_recall_epoch=0.0767]"]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}