{"cells":[{"cell_type":"markdown","metadata":{"id":"nXC6xmOlFtyz"},"source":["# 1. Package Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1720136458261,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"vi6ZScMwq0Lj","outputId":"8c1c5cfc-19b8-460e-9d8d-8c6a7318c771"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul  4 23:40:57 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   52C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1720767356205,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"KpkmrFpqYISS","outputId":"cae1942a-d0ee-4f1d-8a53-0c97c98adae2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]\n","deepspeed\n","colossalai\n","wandb\n","tensorrt\n","nvidia-modelopt --index https://pypi.nvidia.com"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qt3RP1onC5Uf"},"outputs":[],"source":["#@title Install Packages\n","%%capture\n","!CUDA_EXT=1 DS_BUILD=1 pip install --no-cache -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iCRpjtCdPPRx"},"outputs":[],"source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login --add-to-git-credential\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3i2AiD0181GO"},"outputs":[],"source":["#@title Weight and Bias Train Logger Login\n","#@markdown weight and bias 로그인\n","!wandb login"]},{"cell_type":"markdown","metadata":{"id":"IfgXZtBZFyVE"},"source":["# 2. Load Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RO8x9zxrFb1-"},"outputs":[],"source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown  |모델       | Normal   | DeepSpeed |\n","#@markdown  |---        | ---      | ---       |\n","#@markdown  |Llama3-8B  |  X       |   O       |\n","#@markdown  |Mistral-7B |  X       |   O       |\n","#@markdown  |Llama3-70B |  X       |   X       |\n","\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers.utils import PaddingStrategy\n","from transformers.tokenization_utils_base import TruncationStrategy\n","from datasets import load_dataset\n","from random import randint\n","\n","\n","base_model_id = \"Qwen/Qwen2-1.5B-Instruct\" # @param [\"Gunulhona/tb_pretrained_sts\", \"Gunulhona/tb_pretrained\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", \"EleutherAI/polyglot-ko-12.8b\", \"vilm/vulture-40b\", \"arcee-ai/Arcee-Spark\", \"Qwen/Qwen2-1.5B-Instruct\", \"OuteAI/Lite-Mistral-150M\", \"google/gemma-2b-it\"] {allow-input: true}\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    add_special_tokens=True,\n","    trust_remote_code=True)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","tokenizer.padding_side = \"right\"\n","tokenizer.truncation_side = \"right\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wK7h7vdKX0r7"},"source":["#3. Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AQd5nc9cYixp"},"outputs":[],"source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","from finetuning_datafunctions import formatting, preprocess_function, SumDataCallator\n","\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","raw_dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\",  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([raw_dataset[\"train\"], raw_dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 100)) + int(np.percentile(input_lenghts, 10))\n","max_source_length = min(4096, max_source_length)\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 100)) + int(np.percentile(target_lenghts, 10))\n","max_target_length = min(4096, max_target_length)\n","\n","dataset = raw_dataset.map(preprocess_function,\n","                          batched=True,\n","                          remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                          fn_kwargs={\n","                              \"max_source_length\": max_source_length,\n","                              \"max_target_length\": max_target_length\n","                             },)\n","# dataset = raw_dataset\n","# if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","#     dataset = dataset.map(lambda x: x,\n","#                           batched=True,\n","#                           remove_columns=[\"token_type_ids\"], )\n"]},{"cell_type":"code","source":["#@title Data Modules\n","#@markdown Data Module Functions\n","%%writefile finetuning_datafunctions.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","\n","def formatting(sample,\n","               max_source_length,\n","               max_target_length,\n","               padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    model_inputs, labels = [], []\n","    for dialogue, summary in zip(sample[\"dialogue\"], sample[\"summary\"]):\n","        chat_template = [\n","            # {\n","            #     \"role\": \"system\",\n","            #     \"content\": \"You are a friendly chatbot who always responds with summary\",\n","            # },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Summarize the following dialogue\\n\\n{dialogue}\"\n","            },\n","\n","        ]\n","        label_template = [{\n","                \"role\": \"assistant\",\n","                \"content\": f\"{summary}\"\n","        }]\n","\n","        chat_message = tokenizer.apply_chat_template(conversation=chat_template,\n","                                                     tokenize=False,\n","                                                     add_generateion_prompt=False, )\n","        bot_message = tokenizer.apply_chat_template(conversation=label_template,\n","                                                    tokenize=False,\n","                                                    add_generateion_prompt=False, )\n","        model_inputs.append(chat_message)\n","        labels.append(bot_message)\n","\n","    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    # model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    # labels = tokenizer(text_target=sample[\"summary\"],\n","    #                    max_length=max_target_length,\n","    #                    padding=padding,\n","    #                    truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    # if padding == \"max_length\":\n","    #     labels[\"input_ids\"] = [\n","    #         [(l if l != tokenizer.pad_token_id else 1) for l in label] for label in labels[\"input_ids\"]\n","    #     ]\n","    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs, labels\n","\n","def preprocess_function(sample, max_source_length, max_target_length):\n","    templated_text, labels = formatting(sample, max_source_length, max_target_length)\n","\n","    return {\n","        \"input_ids\": templated_text,\n","        \"labels\": labels\n","    }\n","\n","class CallatorOutput:\n","    def __init__(self, input_ids, attention_mask, labels):\n","        self._input_ids = input_ids\n","        self._attention_mask = attention_mask\n","        self._labels = labels\n","\n","    def __len__(self,):\n","        return len(self._input_ids)\n","\n","    def __getitem__(self, key):\n","        match key:\n","            case \"input_ids\":\n","                return self._input_ids\n","            case \"attention_mask\":\n","                return self._attention_mask\n","            case \"labels\":\n","                return self._labels\n","            case _:\n","                raise KeyError(f\"Key {key} not found\")\n","\n","    def __setitem__(self, key, value):\n","        match key:\n","            case \"input_ids\":\n","                self._input_ids = value\n","            case \"attention_mask\":\n","                self._attention_mask = value\n","            case \"labels\":\n","                self._labels = value\n","            case _:\n","                raise KeyError(f\"Key {key} not found\")\n","\n","    @property\n","    def input_ids(self):\n","        return self._input_ids\n","\n","    @input_ids.setter\n","    def input_ids(self, value):\n","        self._input_ids = value\n","\n","    @property\n","    def attention_mask(self):\n","        return self._attention_mask\n","\n","    @attention_mask.setter\n","    def attention_mask(self, value):\n","        self._attention_mask = value\n","\n","    @property\n","    def labels(self):\n","        return self._labels\n","\n","    @labels.setter\n","    def labels(self, value):\n","        self._labels = value\n","\n","\n","\n","class SumDataCallator(DataCollatorForLanguageModeling):\n","    def __init__(self, tokenizer, max_length):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def _tokenizing(self, text):\n","        return self.tokenizer(text,\n","                              truncation=True,\n","                              padding=\"max_length\",\n","                              max_length=self.max_length,\n","                              return_tensors=\"pt\")\n","\n","    def __call__(self, batch):\n","        input_text = []\n","        labels = []\n","        for b in batch:\n","            input_text += [b[\"input_ids\"]]\n","            labels += [b[\"labels\"]]\n","        input_tokens = self._tokenizing(input_text)\n","        label_tokens = self._tokenizing(labels)\n","\n","        return CallatorOutput(**{\n","            \"input_ids\": input_tokens['input_ids'],\n","            \"attention_mask\": input_tokens['attention_mask'],\n","            \"labels\": label_tokens['input_ids']\n","        })\n","        # raise Exception(\"STOP\")\n","\n"],"metadata":{"cellView":"form","id":"_6hq8eyYLvCn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3C1Fs6eeX4Ri"},"source":["#4. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wnRyfucpWeK5"},"outputs":[],"source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding, DataCollatorForLanguageModeling\n","from trl import SFTTrainer, SFTConfig\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer, lora_config\n","from finetuning_datasets import dataset, metric, max_source_length\n","from finetuning_datafunctions import SumDataCallator, formatting, preprocess_function\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","\n","# data_collator = DataCollatorWithPadding(\n","#     tokenizer=tokenizer,\n","#     padding=True,\n","#     max_length=max_source_length,\n","#     return_tensors=\"pt\")\n","data_collator = SumDataCallator(tokenizer, max_length=max_source_length)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    # eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    warmup_steps=0.03,\n","    gradient_accumulation_steps=4,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"paged_adamw_8bit\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = SFTTrainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n","    peft_config=lora_config,\n","    # formatting_func=formatting,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"code","source":["%%writefile deepspeed_config.yaml\n","compute_environment: LOCAL_MACHINE\n","debug: false\n","deepspeed_config:\n","  deepspeed_multinode_launcher: standard\n","  gradient_accumulation_steps: 4\n","  offload_optimizer_device: none\n","  offload_param_device: none\n","  zero3_init_flag: true\n","  zero3_save_16bit_model: true\n","  zero_stage: 2\n","distributed_type: DEEPSPEED\n","downcast_bf16: 'no'\n","machine_rank: 0\n","main_training_function: main\n","mixed_precision: bf16\n","num_machines: 1\n","num_processes: 8\n","rdzv_backend: static\n","same_network: true\n","tpu_env: []\n","tpu_use_cluster: false\n","tpu_use_sudo: false\n","use_cpu: false"],"metadata":{"id":"uysQB-wlnmt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":34271,"status":"error","timestamp":1720750848926,"user":{"displayName":"정권환","userId":"03859214150473665717"},"user_tz":-540},"id":"iX1Vp8-2PQN8","outputId":"a6c54711-cbdf-4e4d-86e4-0689a6c2d460"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-07-12 02:20:18,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","[2024-07-12 02:20:19,671] torch.distributed.run: [WARNING] \n","[2024-07-12 02:20:19,671] torch.distributed.run: [WARNING] *****************************************\n","[2024-07-12 02:20:19,671] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n","[2024-07-12 02:20:19,671] torch.distributed.run: [WARNING] *****************************************\n","2024-07-12 02:20:24.759804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.759861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.759861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.759876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.759889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.759893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.759901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.760168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.760202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.761853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761860: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:24.761902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-12 02:20:24.761933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-12 02:20:24.763294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-12 02:20:26.535082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.536475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.536511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.536542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.539909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.541722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.541987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-12 02:20:26.561197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[2024-07-12 02:20:29,510] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,538] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,550] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,574] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,610] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,666] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2024-07-12 02:20:29,843] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n","\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n","\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n","\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n","\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n","\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","config.json: 100% 660/660 [00:00<00:00, 4.88MB/s]\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","model.safetensors: 100% 3.09G/3.09G [00:10<00:00, 291MB/s]\n","Traceback (most recent call last):\n","  File \"/content/train.py\", line 10, in <module>\n","    from peft_model import peft_model, tokenizer, lora_config\n","  File \"/content/peft_model.py\", line 23, in <module>\n","    peft_model = AutoModelForCausalLM.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n","    return model_class.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n","    state_dict = load_state_dict(resolved_archive_file)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 506, in load_state_dict\n","    with safe_open(checkpoint_file, framework=\"pt\") as f:\n","RuntimeError: unable to open file </root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors> in read-only mode: No such file or directory (2)\n","Traceback (most recent call last):\n","  File \"/content/train.py\", line 10, in <module>\n","    from peft_model import peft_model, tokenizer, lora_config\n","  File \"/content/peft_model.py\", line 23, in <module>\n","    peft_model = AutoModelForCausalLM.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n","    return model_class.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n","Traceback (most recent call last):\n","  File \"/content/train.py\", line 10, in <module>\n","    from peft_model import peft_model, tokenizer, lora_config\n","  File \"/content/peft_model.py\", line 23, in <module>\n","    peft_model = AutoModelForCausalLM.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n","    return model_class.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3310, in from_pretrained\n","    state_dict = load_state_dict(resolved_archive_file)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 506, in load_state_dict\n","    with safe_open(checkpoint_file, framework=\"pt\") as f:\n","RuntimeError: unable to open file </root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors> in read-only mode: No such file or directory (2)\n","    with safe_open(resolved_archive_file, framework=\"pt\") as f:\n","RuntimeError: unable to open file </root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors> in read-only mode: No such file or directory (2)\n","[2024-07-12 02:20:46,990] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12153 closing signal SIGTERM\n","[2024-07-12 02:20:46,990] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12154 closing signal SIGTERM\n","[2024-07-12 02:20:46,990] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12155 closing signal SIGTERM\n","[2024-07-12 02:20:46,993] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12156 closing signal SIGTERM\n","[2024-07-12 02:20:46,993] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12157 closing signal SIGTERM\n","[2024-07-12 02:20:46,993] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12158 closing signal SIGTERM\n","[2024-07-12 02:20:46,993] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 12160 closing signal SIGTERM\n","[2024-07-12 02:20:47,329] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 6 (pid: 12159) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1082, in launch_command\n","    deepspeed_launcher(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 786, in deepspeed_launcher\n","    distrib_run.run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","train.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2024-07-12_02:20:46\n","  host      : 7b4d4101a381\n","  rank      : 6 (local_rank: 6)\n","  exitcode  : 1 (pid: 12159)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"Command '#@title Huggingface Deepspeed Trainer\nexport CUDA_VISIBLE_DEVICES=0\nexport CUDA_LAUNCH_BLOCKING=1\nexport TORCH_USE_CUDA_DSA=0\nexport HF_DATASETS_CACHE='/content/hf_cache/'\naccelerate launch --config_file \"deepspeed_config.yaml\"  train.py\n' returned non-zero exit status 1.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-d041f5d36de3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#@title Huggingface Deepspeed Trainer\\nexport CUDA_VISIBLE_DEVICES=0\\nexport CUDA_LAUNCH_BLOCKING=1\\nexport TORCH_USE_CUDA_DSA=0\\nexport HF_DATASETS_CACHE=\\'/content/hf_cache/\\'\\naccelerate launch --config_file \"deepspeed_config.yaml\"  train.py\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    138\u001b[0m           \u001b[0mreturncode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       )\n","\u001b[0;31mCalledProcessError\u001b[0m: Command '#@title Huggingface Deepspeed Trainer\nexport CUDA_VISIBLE_DEVICES=0\nexport CUDA_LAUNCH_BLOCKING=1\nexport TORCH_USE_CUDA_DSA=0\nexport HF_DATASETS_CACHE='/content/hf_cache/'\naccelerate launch --config_file \"deepspeed_config.yaml\"  train.py\n' returned non-zero exit status 1."]}],"source":["%%shell\n","#@title Huggingface Deepspeed Trainer\n","export CUDA_VISIBLE_DEVICES=0\n","export CUDA_LAUNCH_BLOCKING=1\n","export TORCH_USE_CUDA_DSA=0\n","export HF_DATASETS_CACHE='/content/hf_cache/'\n","accelerate launch --config_file \"deepspeed_config.yaml\"  train.py"]},{"cell_type":"markdown","metadata":{"id":"d5KC2KSMllGB"},"source":["## Training code to Lightning module"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YSF7QaH1zhQ9"},"outputs":[],"source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","\n","import lightning as L\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset,  max_source_length\n","from finetuning_datafunctions import SumDataCallator\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, data_collator, train_batch_size, eval_batch_size, training_args,):\n","        super().__init__()\n","        self.train_dataset = dataset[\"train\"]\n","        self.val_dataset = dataset[\"validation\"]\n","        self.test_dataset = dataset[\"test\"]\n","        self.data_collator = SumDataCallator(tokenizer, max_length=max_source_length)\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          num_workers=8,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"agrktrAylkal"},"outputs":[],"source":["#@title Lightning Model\n","%%writefile l_model.py\n","\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from bitsandbytes.optim import AdamW, Lion\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","from finetuning_datasets import dataset\n","from torchmetrics.functional.text.rouge import rouge_score\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, data_collator, learning_rate: float = 2e-5):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.learning_rate = learning_rate\n","\n","    def _get_rouge_score(self, predictions, labels):\n","        generated_tokens = predictions.argmax(dim=-1)\n","        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        return rouge_score(preds=decoded_preds, target=decoded_labels)\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(input_ids=batch.input_ids,\n","                             attention_mask=batch.attention_mask,\n","                             labels=batch.labels)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss,\n","                 prog_bar=True, on_step=True, on_epoch=True, batch_size=self.trainer.datamodule.train_batch_size, sync_dist=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"train_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=True, on_epoch=True, batch_size=self.trainer.datamodule.train_batch_size, sync_dist=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(input_ids=batch.input_ids,\n","                             attention_mask=batch.attention_mask,\n","                             labels=batch.labels)\n","        rouge_score = self._get_rouge_score(outputs.logits, batch.labels)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss,\n","                 prog_bar=True, on_step=True, on_epoch=True, batch_size=self.trainer.datamodule.eval_batch_size, sync_dist=True)\n","        for k, v in rouge_score.items():\n","            self.log(f\"val_{k}\",\n","                     v,\n","                     prog_bar=True, on_step=False, on_epoch=True, batch_size=self.trainer.datamodule.eval_batch_size, sync_dist=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = Lion(params=self.model.parameters(),\n","                         lr=self.learning_rate,\n","                         weight_decay=0.01,\n","                         optim_bits=32,)\n","        scheduler = CosineAnnealingWarmRestarts(optimizer,\n","                                                T_0=10,\n","                                                T_mult=2,\n","                                                eta_min=0.00001)\n","        # scheduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"min\")\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"monitor\": \"val_loss\",\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","\n","            },\n","        }\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vFZzH2TmzqP-"},"outputs":[],"source":["#@title Trainer\n","%%writefile l_trainer.py\n","\n","import os\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy\n","from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n","\n","\n","if __name__ == \"__main__\":\n","    L.pytorch.cli_lightning_logo()\n","    training_args = LightningArgumentParser()\n","    cli = LightningCLI(\n","        model_class=LLamaFTLightningModule,\n","        datamodule_class=FTDataModule,\n","        seed_everything_default=42,\n","        trainer_defaults={\n","            \"reload_dataloaders_every_n_epochs\": 1,\n","            \"strategy\": \"deepspeed\",\n","            \"precision\": \"bf16-mixed\",\n","            \"accumulate_grad_batches\": 4,\n","            \"profiler\": \"PassThroughProfiler\",\n","            \"logger\": [WandbLogger(project=\"LLM-Finetuning\"),],\n","            \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=5), LearningRateMonitor()]\n","        },\n","        save_config_callback=None)\n","    # cli.add_arguments_to_parser(training_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RYALPHD3PyBD"},"outputs":[],"source":["#@title Start Training\n","#@markdown 실험 결과\n","#@markdown\n","#@markdown * batch_size <b>2</b> 넘기는 경우 OOM\n","#@markdown * DeepSpeed의 경우 GPU Ram 20GB로 7B finetuning 가능\n","#@markdown * DeepSpeed의 경우, 7B L4 GPU에서 사용 가능\n","#@markdown * 70B의 경우 RAM에서 Weight 가져오다 OOM\n","#@markdown * 1.5B T4 GPU에서 성공\n","\n","%%shell\n","\n","python l_trainer.py fit \\\n","    --trainer.max_epochs 4 \\\n","    --model.learning_rate 5e-5 \\\n","    --data.train_batch_size 2 \\\n","    --data.eval_batch_size 2\n","\n","#    --trainer.fast_dev_run 1\\"]},{"cell_type":"markdown","metadata":{"id":"OXFgDjet3ScB"},"source":["# Model Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"outputs":[],"source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"]},{"cell_type":"code","source":[],"metadata":{"id":"ft7lndGneWWe"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyOvriEBfrfPmYD2Y8QxziUm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}