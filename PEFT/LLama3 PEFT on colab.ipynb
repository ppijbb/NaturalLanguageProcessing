{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyP6Hk8WWVEUI4TAc+nE01V4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi6ZScMwq0Lj","executionInfo":{"status":"ok","timestamp":1719805536792,"user_tz":-540,"elapsed":325,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"fa4dbd5d-25a4-43dd-bf7a-e02fbd19c68a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1719819906293,"user_tz":-540,"elapsed":449,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"0ad26403-ea7e-4e43-a1e8-c65193ad03b4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf","executionInfo":{"status":"ok","timestamp":1719819914628,"user_tz":-540,"elapsed":7925,"user":{"displayName":"정권환","userId":"03859214150473665717"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iCRpjtCdPPRx","executionInfo":{"status":"ok","timestamp":1719795905257,"user_tz":-540,"elapsed":566125,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"79b2b166-19f7-42b1-e74a-5be91b236d15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown      LLama3-8B => OOM\n","#@markdown      Mistral-7B => OOM\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"Gunulhona/tb_pretrained_sts\" # @param [\"Gunulhona/tb_pretrained_sts\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1719820194143,"user_tz":-540,"elapsed":403,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"a90a137c-5006-44b1-9da2-eb8d289c087a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 85))\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 90))\n","\n","\n","def preprocess_function(sample, max_source_length, max_target_length, padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","dataset = dataset.map(preprocess_function,\n","                      batched=True,\n","                      remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                      fn_kwargs={\n","                          \"max_source_length\": max_source_length,\n","                           \"max_target_length\": max_source_length\n","                          },)\n","\n","if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    dataset = dataset.map(lambda x: x,\n","                          batched=True,\n","                          remove_columns=[\"token_type_ids\"], )\n"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1719820194594,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"97794e61-b28d-4beb-a511-28a4004a90ad"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"adamw_hf\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1719820196785,"user_tz":-540,"elapsed":386,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"68c04139-7668-4ec9-fec5-d2af2431089f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iX1Vp8-2PQN8","executionInfo":{"status":"ok","timestamp":1719819935296,"user_tz":-540,"elapsed":303,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"608d22cd-ed65-4634-e3dd-e384a00b3530"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"markdown","source":["## Training code to Lightning module"],"metadata":{"id":"d5KC2KSMllGB"}},{"cell_type":"code","source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","import lightning as L\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, tokenizer, data_collator, training_args):\n","        super().__init__()\n","        self.train_dataset = train_dataset\n","        self.val_dataset = val_dataset\n","        self.test_dataset = val_dataset\n","        self.tokenizer = tokenizer\n","        self.data_collator = data_collator\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.training_args.per_device_train_batch_size if eval_mode else self.training_args.per_device_eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSF7QaH1zhQ9","executionInfo":{"status":"ok","timestamp":1719821018719,"user_tz":-540,"elapsed":481,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"30bb25e0-2090-46af-b7c4-e2de2ebe1413"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_datamodule.py\n"]}]},{"cell_type":"code","source":["#@title Lightning Model\n","%%writefile l_model.py\n","import lightning as L\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, model, tokenizer, data_collator, training_args):\n","        super().__init__()\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.data_collator = data_collator\n","        self.training_args = training_args\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss)\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.model.parameters(),\n","                                 lr=self.training_args.learning_rate)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agrktrAylkal","executionInfo":{"status":"ok","timestamp":1719821019054,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"0ec5537a-5d81-4445-a42b-7016ae88923b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_model.py\n"]}]},{"cell_type":"code","source":["%%writefile l_trainer.py\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","\n","if __name__ == \"__main__\":\n","    training_args= LightningArgumentParser()\n","    data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","    cli = LightningCLI(model_class=LLamaFTLightningModule,#(model=peft_model,\n","                                                          #tokenizer=tokenizer,\n","                                                          #data_collator=data_collator,\n","                                                          #training_args=training_args),\n","                       datamodule_class=FTDataModule,#(train_dataset=dataset[\"train\"],\n","                                                     #val_dataset=dataset[\"validation\"],\n","                                                     #test_dataset=dataset[\"test\"],\n","                                                     #data_collator=data_collator,\n","                                                     #tokenizer=tokenizer,\n","                                                     #training_args=training_args),\n","                       seed_everything_default=42)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZzH2TmzqP-","executionInfo":{"status":"ok","timestamp":1719822811472,"user_tz":-540,"elapsed":298,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"8c6f8c7c-028c-4284-887b-0e93c42cc80e"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_trainer.py\n"]}]},{"cell_type":"code","source":["!python l_trainer.py -c --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYALPHD3PyBD","outputId":"5e90cd05-ad92-4237-979a-55759951bae0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-01 08:33:57.910230: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-01 08:33:57.910291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-01 08:33:57.911185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-01 08:33:57.916619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-01 08:33:59.031888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}