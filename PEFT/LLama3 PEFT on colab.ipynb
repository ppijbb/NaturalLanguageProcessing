{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1pKdrhoXVtDSTpllUjqXthswIFxteyRju","authorship_tag":"ABX9TyOIR31TfP9mjKBjqjKIIVLD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Package Installation"],"metadata":{"id":"nXC6xmOlFtyz"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi6ZScMwq0Lj","executionInfo":{"status":"ok","timestamp":1719805536792,"user_tz":-540,"elapsed":325,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"fa4dbd5d-25a4-43dd-bf7a-e02fbd19c68a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"code","source":["#@title Requirements\n","%%writefile requirements.txt\n","peft\n","fire\n","accelerator\n","transformers\n","datasets\n","evaluate\n","pyarrow\n","galore-torch\n","pytorch-ignite\n","rouge-score\n","nltk\n","py7zr\n","optimum[exporters]\n","trl\n","lightning\n","jsonargparse[signatures]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KpkmrFpqYISS","executionInfo":{"status":"ok","timestamp":1719879175276,"user_tz":-540,"elapsed":3,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"6f7a01a5-9735-46cb-a40b-17edceb7bdb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","source":["#@title Install Packages\n","%%capture\n","!pip install --no-cache -r requirements.txt"],"metadata":{"cellView":"form","id":"qt3RP1onC5Uf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Huggingface Login\n","#@markdown huggingface weight 를 이용하고 싶다면 로그인 필수\n","!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iCRpjtCdPPRx","executionInfo":{"status":"ok","timestamp":1719879273875,"user_tz":-540,"elapsed":7874,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"f5af2197-c13e-4eaf-b7e9-df693aa2f839"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# 2. Load Model\n"],"metadata":{"id":"IfgXZtBZFyVE"}},{"cell_type":"code","source":["#@title Get peft model from huggingface\n","#@markdown Colab 고용량 Ram CPU에서 가능한 범위 ~8B(테스트 중)\n","#@markdown\n","#@markdown      LLama3-8B => OOM\n","#@markdown      Mistral-7B => OOM\n","%%writefile peft_model.py\n","\n","import os\n","import fire\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from peft import LoraConfig\n","from peft import inject_adapter_in_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from random import randint\n","\n","base_model_id = \"Gunulhona/tb_pretrained_sts\" # @param [\"Gunulhona/tb_pretrained_sts\", \"google/flan-t5-xxl\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen2-7B-Instruct\", \"google/gemma-7b\", \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"] {allow-input: true}\n","\n","peft_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n","\n","# adapter configuration\n","lora_config = LoraConfig(\n","    target_modules=[\"q_proj\", \"k_proj\"],\n","    init_lora_weights=\"gaussian\", #\"gaussian\", \"pissa\", \"pissa_niter_{n}\", \"loftq\", False\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    inference_mode=False,\n","    use_dora=False,\n",")\n","\n","# peft_model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n","inject_adapter_in_model(lora_config, peft_model, \"adapter_1\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","tokenizer.model_input_names=['input_ids', 'attention_mask']\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"RO8x9zxrFb1-","executionInfo":{"status":"ok","timestamp":1719879277737,"user_tz":-540,"elapsed":320,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"25a1a97b-ba78-48d4-f20f-d425de93b3b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing peft_model.py\n"]}]},{"cell_type":"markdown","source":["#3. Load Dataset"],"metadata":{"id":"wK7h7vdKX0r7"}},{"cell_type":"code","source":["#@title Load data From huggingface datasets\n","#@markdown summary task에 대해 우선적으로 실험\n","%%writefile finetuning_datasets.py\n","import numpy as np\n","from datasets import load_dataset, concatenate_datasets\n","\n","from evaluate import load\n","from peft_model import tokenizer\n","\n","dataset_path = \"Samsung/samsum\" # @param [\"Samsung/samsum\", \"emozilla/soda_synthetic_dialogue\", \"frcp/summary-alpaca-v01\"] {allow-input: true}\n","\n","dataset = load_dataset(\n","  dataset_path,\n","  trust_remote_code=True,\n","  revision=\"main\"  # tag name, or branch name, or commit hash\n",")\n","\n","metric = load(\"rouge\")\n","full_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","tokenized_inputs = full_dataset.map(\n","    lambda x: tokenizer(x[\"dialogue\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","\n","input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n","# take 85 percentile of max length for better utilization\n","max_source_length = int(np.percentile(input_lenghts, 85))\n","\n","tokenized_targets = full_dataset.map(\n","    lambda x: tokenizer(x[\"summary\"], truncation=True),\n","    batched=True,\n","    remove_columns=[\"dialogue\", \"summary\"])\n","target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n","# take 90 percentile of max length for better utilization\n","max_target_length = int(np.percentile(target_lenghts, 90))\n","\n","\n","def preprocess_function(sample, max_source_length, max_target_length, padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, )\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True,)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","dataset = dataset.map(preprocess_function,\n","                      batched=True,\n","                      remove_columns=[\"dialogue\", \"summary\", \"id\"],\n","                      fn_kwargs={\n","                          \"max_source_length\": max_source_length,\n","                           \"max_target_length\": max_source_length\n","                          },)\n","\n","if any([d for d in dataset.values() if \"token_type_ids\" in d.features]):\n","    dataset = dataset.map(lambda x: x,\n","                          batched=True,\n","                          remove_columns=[\"token_type_ids\"], )\n"],"metadata":{"id":"AQd5nc9cYixp","executionInfo":{"status":"ok","timestamp":1719879281233,"user_tz":-540,"elapsed":331,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ed1f9ad-e096-47bb-b1c8-686cccaa0407","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing finetuning_datasets.py\n"]}]},{"cell_type":"markdown","source":["#4. Train"],"metadata":{"id":"3C1Fs6eeX4Ri"}},{"cell_type":"code","source":["#@title Start Training\n","#@markdown transformers trainer 이용, 추후 lightning 으로 이전 가능\n","%%writefile train.py\n","import nltk\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import TrainingArguments, Trainer, TrainerCallback\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from ignite.metrics import Rouge\n","\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset, metric\n","\n","\n","# Callback Class\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True\n","\n","        return control\n","\n","# metric function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"llm_output\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    # use_cpu=True,\n","    # load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    push_to_hub=True,\n","    logging_steps=1000,\n","    save_steps=1000,\n","    fp16=True,\n","    save_total_limit=3,\n","    # logging_dir=\"llm_output/logs\",\n","    optim=\"adamw_hf\",\n","    report_to=\"tensorboard\",\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()],\n",")\n","\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnRyfucpWeK5","executionInfo":{"status":"ok","timestamp":1719879282468,"user_tz":-540,"elapsed":2,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"0be79a51-b4a1-4ecf-ce4d-0b843445614c","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iX1Vp8-2PQN8","executionInfo":{"status":"ok","timestamp":1719819935296,"user_tz":-540,"elapsed":303,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"608d22cd-ed65-4634-e3dd-e384a00b3530"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"markdown","source":["## Training code to Lightning module"],"metadata":{"id":"d5KC2KSMllGB"}},{"cell_type":"code","source":["#@title Lightning Data Moudle\n","%%writefile l_datamodule.py\n","import lightning as L\n","from torch.utils.data import DataLoader\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","class FTDataModule(L.LightningDataModule):\n","    def __init__(self, train_dataset, val_dataset, test_dataset, data_collator, train_batch_size, eval_batch_size,training_args,):\n","        super().__init__()\n","        self.train_dataset = dataset[\"train\"]\n","        self.val_dataset = dataset[\"validation\"]\n","        self.test_dataset = dataset[\"test\"]\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.training_args = training_args\n","\n","    def _get_dataloader(self, dataset, eval_mode: bool = False):\n","        return DataLoader(dataset=dataset,\n","                          batch_size=self.train_batch_size if eval_mode else self.eval_batch_size,\n","                          shuffle=not eval_mode,\n","                          collate_fn=self.data_collator)\n","\n","    def train_dataloader(self):\n","        return self._get_dataloader(dataset=self.train_dataset)\n","\n","    def val_dataloader(self):\n","        return self._get_dataloader(dataset=self.val_dataset, eval_mode=True)\n","\n","    def test_dataloader(self):\n","        return self._get_dataloader(dataset=self.test_dataset, eval_mode=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSF7QaH1zhQ9","executionInfo":{"status":"ok","timestamp":1719900166281,"user_tz":-540,"elapsed":328,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"4d26868b-8b0b-4c3b-f1d2-826afae04b2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_datamodule.py\n"]}]},{"cell_type":"code","source":["#@title Lightning Model\n","%%writefile l_model.py\n","import lightning as L\n","import torch\n","\n","from transformers import DataCollatorForSeq2Seq\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","class LLamaFTLightningModule(L.LightningModule):\n","    def __init__(self, data_collator, training_args):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = peft_model\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n","        self.training_args = training_args\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss = outputs.loss\n","        self.log(\"train_loss\",\n","                 loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        val_loss = outputs.loss\n","        self.log(\"val_loss\",\n","                 val_loss)\n","\n","    # def configure_optimizers(self):\n","    #     return torch.optim.AdamW(self.model.parameters(),\n","    #                              lr=self.training_args.learning_rate)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agrktrAylkal","executionInfo":{"status":"ok","timestamp":1719900166760,"user_tz":-540,"elapsed":1,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"75a0e0c3-1b00-4201-c794-63f24bbd83e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_model.py\n"]}]},{"cell_type":"code","source":["%%writefile l_trainer.py\n","import lightning as L\n","from lightning.pytorch.cli import LightningCLI, LightningArgumentParser\n","from transformers import DataCollatorForSeq2Seq\n","\n","from l_datamodule import FTDataModule\n","from l_model import LLamaFTLightningModule\n","from peft_model import peft_model, tokenizer\n","from finetuning_datasets import dataset\n","\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = LightningArgumentParser()\n","    cli = LightningCLI(model_class=LLamaFTLightningModule,\n","                       datamodule_class=FTDataModule,\n","                       seed_everything_default=42,)\n","    cli.add_arguments_to_parser(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZzH2TmzqP-","executionInfo":{"status":"ok","timestamp":1719892146546,"user_tz":-540,"elapsed":1,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"15370d8d-ed8b-42e7-e9a6-7df18ef29242"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting l_trainer.py\n"]}]},{"cell_type":"code","source":["!python l_trainer.py fit --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EM1Q0wiAkCtR","executionInfo":{"status":"ok","timestamp":1719892228491,"user_tz":-540,"elapsed":30029,"user":{"displayName":"정권환","userId":"03859214150473665717"}},"outputId":"1f3f4056-6dd7-4855-d20f-0eecc008d061"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-02 03:50:03.768577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-02 03:50:03.768641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-02 03:50:03.770224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-02 03:50:03.778885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-02 03:50:05.198348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","Some weights of BartForCausalLM were not initialized from the model checkpoint at Gunulhona/tb_pretrained_sts and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","usage: l_trainer.py [options] fit [-h] [-c CONFIG] [--print_config \b[=flags]]\n","                                  [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n","                                  [--trainer.accelerator.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.accelerator ACCELERATOR]\n","                                  [--trainer.strategy.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.strategy STRATEGY] [--trainer.devices DEVICES]\n","                                  [--trainer.num_nodes NUM_NODES] [--trainer.precision PRECISION]\n","                                  [--trainer.logger.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.logger LOGGER]\n","                                  [--trainer.callbacks.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.callbacks CALLBACKS]\n","                                  [--trainer.fast_dev_run FAST_DEV_RUN]\n","                                  [--trainer.max_epochs MAX_EPOCHS]\n","                                  [--trainer.min_epochs MIN_EPOCHS]\n","                                  [--trainer.max_steps MAX_STEPS] [--trainer.min_steps MIN_STEPS]\n","                                  [--trainer.max_time MAX_TIME]\n","                                  [--trainer.limit_train_batches LIMIT_TRAIN_BATCHES]\n","                                  [--trainer.limit_val_batches LIMIT_VAL_BATCHES]\n","                                  [--trainer.limit_test_batches LIMIT_TEST_BATCHES]\n","                                  [--trainer.limit_predict_batches LIMIT_PREDICT_BATCHES]\n","                                  [--trainer.overfit_batches OVERFIT_BATCHES]\n","                                  [--trainer.val_check_interval VAL_CHECK_INTERVAL]\n","                                  [--trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n","                                  [--trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n","                                  [--trainer.log_every_n_steps LOG_EVERY_N_STEPS]\n","                                  [--trainer.enable_checkpointing {true,false,null}]\n","                                  [--trainer.enable_progress_bar {true,false,null}]\n","                                  [--trainer.enable_model_summary {true,false,null}]\n","                                  [--trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n","                                  [--trainer.gradient_clip_val GRADIENT_CLIP_VAL]\n","                                  [--trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n","                                  [--trainer.deterministic DETERMINISTIC]\n","                                  [--trainer.benchmark {true,false,null}]\n","                                  [--trainer.inference_mode {true,false}]\n","                                  [--trainer.use_distributed_sampler {true,false}]\n","                                  [--trainer.profiler.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.profiler PROFILER]\n","                                  [--trainer.detect_anomaly {true,false}]\n","                                  [--trainer.barebones {true,false}]\n","                                  [--trainer.plugins.help CLASS_PATH_OR_NAME]\n","                                  [--trainer.plugins PLUGINS]\n","                                  [--trainer.sync_batchnorm {true,false}]\n","                                  [--trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n","                                  [--trainer.default_root_dir DEFAULT_ROOT_DIR] [--model CONFIG]\n","                                  [--model.model MODEL] [--model.tokenizer TOKENIZER]\n","                                  [--model.data_collator DATA_COLLATOR]\n","                                  [--model.training_args TRAINING_ARGS] [--data CONFIG]\n","                                  [--data.train_dataset TRAIN_DATASET]\n","                                  [--data.val_dataset VAL_DATASET]\n","                                  [--data.test_dataset TEST_DATASET] [--data.tokenizer TOKENIZER]\n","                                  [--data.data_collator DATA_COLLATOR]\n","                                  [--data.training_args TRAINING_ARGS]\n","                                  [--optimizer.help CLASS_PATH_OR_NAME]\n","                                  [--optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n","                                  [--lr_scheduler.help CLASS_PATH_OR_NAME]\n","                                  [--lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE]\n","                                  [--ckpt_path CKPT_PATH]\n","\n","Runs the full optimization routine.\n","\n","options:\n","  -h, --help            Show this help message and exit.\n","  -c CONFIG, --config CONFIG\n","                        Path to a configuration file in json or yaml format.\n","  --print_config \b[=flags]\n","                        Print the configuration after applying all other arguments and exit. The\n","                        optional flags customizes the output and are one or more keywords\n","                        separated by comma. The supported flags are: comments, skip_default,\n","                        skip_null.\n","  --seed_everything SEED_EVERYTHING\n","                        Set to an int to run seed_everything with this value before classes\n","                        instantiation.Set to True to use a random seed. (type: Union[bool, int],\n","                        default: 42)\n","\n","Customize every aspect of training via flags:\n","  --trainer CONFIG      Path to a configuration file.\n","  --trainer.accelerator.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Accelerator and exit.\n","  --trainer.accelerator ACCELERATOR\n","                        Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\",\n","                        \"mps\", \"auto\") as well as custom accelerator instances. (type: Union[str,\n","                        Accelerator], default: auto, known subclasses:\n","                        lightning.pytorch.accelerators.CPUAccelerator,\n","                        lightning.pytorch.accelerators.CUDAAccelerator,\n","                        lightning.pytorch.accelerators.MPSAccelerator,\n","                        lightning.pytorch.accelerators.XLAAccelerator)\n","  --trainer.strategy.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Strategy and exit.\n","  --trainer.strategy STRATEGY\n","                        Supports different training strategies with aliases as well custom\n","                        strategies. Default: ``\"auto\"``. (type: Union[str, Strategy], default:\n","                        auto, known subclasses: lightning.pytorch.strategies.DDPStrategy,\n","                        lightning.pytorch.strategies.DeepSpeedStrategy,\n","                        lightning.pytorch.strategies.XLAStrategy,\n","                        lightning.pytorch.strategies.FSDPStrategy,\n","                        lightning.pytorch.strategies.ModelParallelStrategy,\n","                        lightning.pytorch.strategies.SingleDeviceStrategy,\n","                        lightning.pytorch.strategies.SingleDeviceXLAStrategy)\n","  --trainer.devices DEVICES, --trainer.devices+ DEVICES\n","                        The devices to use. Can be set to a positive number (int or str), a\n","                        sequence of device indices (list or str), the value ``-1`` to indicate all\n","                        available devices should be used, or ``\"auto\"`` for automatic selection\n","                        based on the chosen accelerator. Default: ``\"auto\"``. (type:\n","                        Union[List[int], str, int], default: auto)\n","  --trainer.num_nodes NUM_NODES\n","                        Number of GPU nodes for distributed training. Default: ``1``. (type: int,\n","                        default: 1)\n","  --trainer.precision PRECISION\n","                        Double precision (64, '64' or '64-true'), full precision (32, '32' or\n","                        '32-true'), 16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed\n","                        precision ('bf16', 'bf16-mixed'). Can be used on CPU, GPU, TPUs, or HPUs.\n","                        Default: ``'32-true'``. (type: Union[Literal[64, 32, 16],\n","                        Literal['transformer-engine', 'transformer-engine-float16', '16-true',\n","                        '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'],\n","                        Literal['64', '32', '16', 'bf16'], null], default: null)\n","  --trainer.logger.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Logger and exit.\n","  --trainer.logger LOGGER, --trainer.logger+ LOGGER\n","                        Logger (or iterable collection of loggers) for experiment tracking. A\n","                        ``True`` value uses the default ``TensorBoardLogger`` if it is installed,\n","                        otherwise ``CSVLogger``. ``False`` will disable logging. If multiple\n","                        loggers are provided, local files (checkpoints, profiler traces, etc.) are\n","                        saved in the ``log_dir`` of the first logger. Default: ``True``. (type:\n","                        Union[Logger, Iterable[Logger], bool, null], default: null, known\n","                        subclasses: lightning.pytorch.loggers.logger.DummyLogger,\n","                        lightning.pytorch.loggers.CometLogger,\n","                        lightning.pytorch.loggers.CSVLogger,\n","                        lightning.pytorch.loggers.MLFlowLogger,\n","                        lightning.pytorch.loggers.NeptuneLogger,\n","                        lightning.pytorch.loggers.TensorBoardLogger,\n","                        lightning.pytorch.loggers.WandbLogger, typing.Iterable)\n","  --trainer.callbacks.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Callback and exit.\n","  --trainer.callbacks CALLBACKS, --trainer.callbacks+ CALLBACKS\n","                        Add a callback or list of callbacks. Default: ``None``. (type:\n","                        Union[List[Callback], Callback, null], default: null, known subclasses:\n","                        lightning.Callback, lightning.pytorch.callbacks.BatchSizeFinder,\n","                        lightning.pytorch.callbacks.Checkpoint,\n","                        lightning.pytorch.callbacks.ModelCheckpoint,\n","                        lightning.pytorch.callbacks.OnExceptionCheckpoint,\n","                        lightning.pytorch.callbacks.DeviceStatsMonitor,\n","                        lightning.pytorch.callbacks.EarlyStopping,\n","                        lightning.pytorch.callbacks.BaseFinetuning,\n","                        lightning.pytorch.callbacks.BackboneFinetuning,\n","                        lightning.pytorch.callbacks.GradientAccumulationScheduler,\n","                        lightning.pytorch.callbacks.LambdaCallback,\n","                        lightning.pytorch.callbacks.LearningRateFinder,\n","                        lightning.pytorch.callbacks.LearningRateMonitor,\n","                        lightning.pytorch.callbacks.ModelSummary,\n","                        lightning.pytorch.callbacks.RichModelSummary,\n","                        lightning.pytorch.callbacks.BasePredictionWriter,\n","                        lightning.pytorch.callbacks.ProgressBar,\n","                        lightning.pytorch.callbacks.RichProgressBar,\n","                        lightning.pytorch.callbacks.TQDMProgressBar,\n","                        lightning.pytorch.callbacks.Timer,\n","                        lightning.pytorch.callbacks.ModelPruning,\n","                        lightning.pytorch.callbacks.SpikeDetection,\n","                        lightning.pytorch.callbacks.StochasticWeightAveraging,\n","                        lightning.pytorch.callbacks.ThroughputMonitor,\n","                        lightning.pytorch.cli.SaveConfigCallback)\n","  --trainer.fast_dev_run FAST_DEV_RUN\n","                        Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train,\n","                        val and test to find any bugs (ie: a sort of unit test). Default:\n","                        ``False``. (type: Union[int, bool], default: False)\n","  --trainer.max_epochs MAX_EPOCHS\n","                        Stop training once this number of epochs is reached. Disabled by default\n","                        (None). If both max_epochs and max_steps are not specified, defaults to\n","                        ``max_epochs = 1000``. To enable infinite training, set ``max_epochs =\n","                        -1``. (type: Optional[int], default: null)\n","  --trainer.min_epochs MIN_EPOCHS\n","                        Force training for at least these many epochs. Disabled by default (None).\n","                        (type: Optional[int], default: null)\n","  --trainer.max_steps MAX_STEPS\n","                        Stop training after this number of steps. Disabled by default (-1). If\n","                        ``max_steps = -1`` and ``max_epochs = None``, will default to ``max_epochs\n","                        = 1000``. To enable infinite training, set ``max_epochs`` to ``-1``.\n","                        (type: int, default: -1)\n","  --trainer.min_steps MIN_STEPS\n","                        Force training for at least these number of steps. Disabled by default\n","                        (``None``). (type: Optional[int], default: null)\n","  --trainer.max_time MAX_TIME\n","                        Stop training after this amount of time has passed. Disabled by default\n","                        (``None``). The time duration can be specified in the format DD:HH:MM:SS\n","                        (days, hours, minutes seconds), as a :class:`datetime.timedelta`, or a\n","                        dictionary with keys that will be passed to :class:`datetime.timedelta`.\n","                        (type: Union[str, timedelta, Dict[str, int], null], default: null)\n","  --trainer.limit_train_batches LIMIT_TRAIN_BATCHES\n","                        How much of training dataset to check (float = fraction, int =\n","                        num_batches). Default: ``1.0``. (type: Union[int, float, null], default:\n","                        null)\n","  --trainer.limit_val_batches LIMIT_VAL_BATCHES\n","                        How much of validation dataset to check (float = fraction, int =\n","                        num_batches). Default: ``1.0``. (type: Union[int, float, null], default:\n","                        null)\n","  --trainer.limit_test_batches LIMIT_TEST_BATCHES\n","                        How much of test dataset to check (float = fraction, int = num_batches).\n","                        Default: ``1.0``. (type: Union[int, float, null], default: null)\n","  --trainer.limit_predict_batches LIMIT_PREDICT_BATCHES\n","                        How much of prediction dataset to check (float = fraction, int =\n","                        num_batches). Default: ``1.0``. (type: Union[int, float, null], default:\n","                        null)\n","  --trainer.overfit_batches OVERFIT_BATCHES\n","                        Overfit a fraction of training/validation data (float) or a set number of\n","                        batches (int). Default: ``0.0``. (type: Union[int, float], default: 0.0)\n","  --trainer.val_check_interval VAL_CHECK_INTERVAL\n","                        How often to check the validation set. Pass a ``float`` in the range [0.0,\n","                        1.0] to check after a fraction of the training epoch. Pass an ``int`` to\n","                        check after a fixed number of training batches. An ``int`` value can only\n","                        be higher than the number of training batches when\n","                        ``check_val_every_n_epoch=None``, which validates after every ``N``\n","                        training batches across epochs or during iteration-based training.\n","                        Default: ``1.0``. (type: Union[int, float, null], default: null)\n","  --trainer.check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n","                        Perform a validation loop every after every `N` training epochs. If\n","                        ``None``, validation will be done solely based on the number of training\n","                        batches, requiring ``val_check_interval`` to be an integer value. Default:\n","                        ``1``. (type: Optional[int], default: 1)\n","  --trainer.num_sanity_val_steps NUM_SANITY_VAL_STEPS\n","                        Sanity check runs n validation batches before starting the training\n","                        routine. Set it to `-1` to run all batches in all validation dataloaders.\n","                        Default: ``2``. (type: Optional[int], default: null)\n","  --trainer.log_every_n_steps LOG_EVERY_N_STEPS\n","                        How often to log within steps. Default: ``50``. (type: Optional[int],\n","                        default: null)\n","  --trainer.enable_checkpointing {true,false,null}\n","                        If ``True``, enable checkpointing. It will configure a default\n","                        ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks`. Default:\n","                        ``True``. (type: Optional[bool], default: null)\n","  --trainer.enable_progress_bar {true,false,null}\n","                        Whether to enable to progress bar by default. Default: ``True``. (type:\n","                        Optional[bool], default: null)\n","  --trainer.enable_model_summary {true,false,null}\n","                        Whether to enable model summarization by default. Default: ``True``.\n","                        (type: Optional[bool], default: null)\n","  --trainer.accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n","                        Accumulates gradients over k batches before stepping the optimizer.\n","                        Default: 1. (type: int, default: 1)\n","  --trainer.gradient_clip_val GRADIENT_CLIP_VAL\n","                        The value at which to clip gradients. Passing ``gradient_clip_val=None``\n","                        disables gradient clipping. If using Automatic Mixed Precision (AMP), the\n","                        gradients will be unscaled before. Default: ``None``. (type: Union[int,\n","                        float, null], default: null)\n","  --trainer.gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n","                        The gradient clipping algorithm to use. Pass\n","                        ``gradient_clip_algorithm=\"value\"`` to clip by value, and\n","                        ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will be\n","                        set to ``\"norm\"``. (type: Optional[str], default: null)\n","  --trainer.deterministic DETERMINISTIC\n","                        If ``True``, sets whether PyTorch operations must use deterministic\n","                        algorithms. Set to ``\"warn\"`` to use deterministic algorithms whenever\n","                        possible, throwing warnings on operations that don't support deterministic\n","                        mode. If not set, defaults to ``False``. Default: ``None``. (type:\n","                        Union[bool, Literal['warn'], null], default: null)\n","  --trainer.benchmark {true,false,null}\n","                        The value (``True`` or ``False``) to set\n","                        ``torch.backends.cudnn.benchmark`` to. The value for\n","                        ``torch.backends.cudnn.benchmark`` set in the current session will be used\n","                        (``False`` if not manually set). If\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic` is\n","                        set to ``True``, this will default to ``False``. Override to manually set\n","                        a different value. Default: ``None``. (type: Optional[bool], default:\n","                        null)\n","  --trainer.inference_mode {true,false}\n","                        Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad`\n","                        during evaluation (``validate``/``test``/``predict``). (type: bool,\n","                        default: True)\n","  --trainer.use_distributed_sampler {true,false}\n","                        Whether to wrap the DataLoader's sampler with\n","                        :class:`torch.utils.data.DistributedSampler`. If not specified this is\n","                        toggled automatically for strategies that require it. By default, it will\n","                        add ``shuffle=True`` for the train sampler and ``shuffle=False`` for\n","                        validation/test/predict samplers. If you want to disable this logic, you\n","                        can pass ``False`` and add your own distributed sampler in the dataloader\n","                        hooks. If ``True`` and a distributed sampler was already added, Lightning\n","                        will not replace the existing one. For iterable-style datasets, we don't\n","                        do this automatically. (type: bool, default: True)\n","  --trainer.profiler.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Profiler and exit.\n","  --trainer.profiler PROFILER\n","                        To profile individual steps during training and assist in identifying\n","                        bottlenecks. Default: ``None``. (type: Union[Profiler, str, null],\n","                        default: null, known subclasses:\n","                        lightning.pytorch.profilers.AdvancedProfiler,\n","                        lightning.pytorch.profilers.PassThroughProfiler,\n","                        lightning.pytorch.profilers.PyTorchProfiler,\n","                        lightning.pytorch.profilers.SimpleProfiler,\n","                        lightning.pytorch.profilers.XLAProfiler)\n","  --trainer.detect_anomaly {true,false}\n","                        Enable anomaly detection for the autograd engine. Default: ``False``.\n","                        (type: bool, default: False)\n","  --trainer.barebones {true,false}\n","                        Whether to run in \"barebones mode\", where all features that may impact raw\n","                        speed are disabled. This is meant for analyzing the Trainer overhead and\n","                        is discouraged during regular training runs. The following features are\n","                        deactivated: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_\n","                        checkpointing`,\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.logger`, :paramref:`\n","                        ~lightning.pytorch.trainer.trainer.Trainer.enable_progress_bar`,\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.log_every_n_steps`, \n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_model_summary\n","                        `, :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_st\n","                        eps`, :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run`,\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.detect_anomaly`,\n","                        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.profiler`,\n","                        :meth:`~lightning.pytorch.core.LightningModule.log`,\n","                        :meth:`~lightning.pytorch.core.LightningModule.log_dict`. (type: bool,\n","                        default: False)\n","  --trainer.plugins.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of\n","                        {Precision,ClusterEnvironment,CheckpointIO,LayerSync} and exit.\n","  --trainer.plugins PLUGINS, --trainer.plugins+ PLUGINS\n","                        Plugins allow modification of core behavior like ddp and amp, and enable\n","                        custom lightning plugins. Default: ``None``. (type: Union[Precision,\n","                        ClusterEnvironment, CheckpointIO, LayerSync, List[Union[Precision,\n","                        ClusterEnvironment, CheckpointIO, LayerSync]], null], default: null, known\n","                        subclasses: lightning.pytorch.plugins.Precision,\n","                        lightning.pytorch.plugins.MixedPrecision,\n","                        lightning.pytorch.plugins.BitsandbytesPrecision,\n","                        lightning.pytorch.plugins.DeepSpeedPrecision,\n","                        lightning.pytorch.plugins.DoublePrecision,\n","                        lightning.pytorch.plugins.FSDPPrecision,\n","                        lightning.pytorch.plugins.HalfPrecision,\n","                        lightning.pytorch.plugins.TransformerEnginePrecision,\n","                        lightning.pytorch.plugins.XLAPrecision,\n","                        lightning.fabric.plugins.environments.KubeflowEnvironment,\n","                        lightning.fabric.plugins.environments.LightningEnvironment,\n","                        lightning.fabric.plugins.environments.LSFEnvironment,\n","                        lightning.fabric.plugins.environments.MPIEnvironment,\n","                        lightning.fabric.plugins.environments.SLURMEnvironment,\n","                        lightning.fabric.plugins.environments.TorchElasticEnvironment,\n","                        lightning.fabric.plugins.environments.XLAEnvironment,\n","                        lightning.fabric.plugins.TorchCheckpointIO,\n","                        lightning.fabric.plugins.XLACheckpointIO,\n","                        lightning.pytorch.plugins.AsyncCheckpointIO,\n","                        lightning.pytorch.plugins.TorchSyncBatchNorm)\n","  --trainer.sync_batchnorm {true,false}\n","                        Synchronize batch norm layers between process groups/whole world. Default:\n","                        ``False``. (type: bool, default: False)\n","  --trainer.reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n","                        Set to a positive integer to reload dataloaders every n epochs. Default:\n","                        ``0``. (type: int, default: 0)\n","  --trainer.default_root_dir DEFAULT_ROOT_DIR\n","                        Default path for logs and weights when no logger/ckpt_callback passed.\n","                        Default: ``os.getcwd()``. Can be remote file paths such as\n","                        `s3://mybucket/path` or 'hdfs://path/' (type: Union[str, Path, null],\n","                        default: null)\n","\n","<class 'l_model.LLamaFTLightningModule'>:\n","  --model CONFIG        Path to a configuration file.\n","  --model.model MODEL   (type: Optional[Any], default: null)\n","  --model.tokenizer TOKENIZER\n","                        (type: Optional[Any], default: null)\n","  --model.data_collator DATA_COLLATOR\n","                        (type: Optional[Any], default: null)\n","  --model.training_args TRAINING_ARGS\n","                        (type: Optional[Any], default: null)\n","\n","<class 'l_datamodule.FTDataModule'>:\n","  --data CONFIG         Path to a configuration file.\n","  --data.train_dataset TRAIN_DATASET\n","                        (type: Optional[Any], default: null)\n","  --data.val_dataset VAL_DATASET\n","                        (type: Optional[Any], default: null)\n","  --data.test_dataset TEST_DATASET\n","                        (type: Optional[Any], default: null)\n","  --data.tokenizer TOKENIZER\n","                        (type: Optional[Any], default: null)\n","  --data.data_collator DATA_COLLATOR\n","                        (type: Optional[Any], default: null)\n","  --data.training_args TRAINING_ARGS\n","                        (type: Optional[Any], default: null)\n","\n","Base class for all optimizers:\n","  --optimizer.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of Optimizer and exit.\n","  --optimizer CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n","                        One or more arguments specifying \"class_path\" and \"init_args\" for any\n","                        subclass of Optimizer. (type: <class 'Optimizer'>, known subclasses:\n","                        torch.optim.Optimizer, torch.optim.Adadelta, torch.optim.Adagrad,\n","                        torch.optim.Adam, torch.optim.AdamW, torch.optim.Adamax, torch.optim.ASGD,\n","                        torch.optim.NAdam, torch.optim.RAdam, torch.optim.RMSprop,\n","                        torch.optim.Rprop, torch.optim.SGD, torch.optim.SparseAdam,\n","                        torch.optim.LBFGS, accelerate.optimizer.AcceleratedOptimizer,\n","                        accelerate.utils.MegatronLMOptimizerWrapper,\n","                        bitsandbytes.optim.optimizer.Optimizer8bit,\n","                        bitsandbytes.optim.optimizer.Optimizer2State, bitsandbytes.optim.Adam,\n","                        bitsandbytes.optim.Adam8bit, bitsandbytes.optim.Adam32bit,\n","                        bitsandbytes.optim.PagedAdam, bitsandbytes.optim.PagedAdam8bit,\n","                        bitsandbytes.optim.PagedAdam32bit, bitsandbytes.optim.AdamW,\n","                        bitsandbytes.optim.AdamW8bit, bitsandbytes.optim.AdamW32bit,\n","                        bitsandbytes.optim.PagedAdamW, bitsandbytes.optim.PagedAdamW8bit,\n","                        bitsandbytes.optim.PagedAdamW32bit, bitsandbytes.optim.LAMB,\n","                        bitsandbytes.optim.LAMB8bit, bitsandbytes.optim.LAMB32bit,\n","                        bitsandbytes.optim.optimizer.Optimizer1State, bitsandbytes.optim.Adagrad,\n","                        bitsandbytes.optim.Adagrad8bit, bitsandbytes.optim.Adagrad32bit,\n","                        bitsandbytes.optim.LARS, bitsandbytes.optim.LARS8bit,\n","                        bitsandbytes.optim.LARS32bit, bitsandbytes.optim.Lion,\n","                        bitsandbytes.optim.Lion8bit, bitsandbytes.optim.Lion32bit,\n","                        bitsandbytes.optim.PagedLion, bitsandbytes.optim.PagedLion8bit,\n","                        bitsandbytes.optim.PagedLion32bit, bitsandbytes.optim.RMSprop,\n","                        bitsandbytes.optim.RMSprop8bit, bitsandbytes.optim.RMSprop32bit,\n","                        bitsandbytes.optim.SGD, bitsandbytes.optim.SGD8bit,\n","                        bitsandbytes.optim.SGD32bit, bitsandbytes.optim.adam.AnalysisAdam,\n","                        bitsandbytes.optim.PytorchLARS,\n","                        transformers.trainer_pt_utils.LayerWiseDummyOptimizer)\n","\n","(<class 'torch.optim.lr_scheduler.LRScheduler'>, <class 'lightning.pytorch.cli.ReduceLROnPlateau'>):\n","  --lr_scheduler.help CLASS_PATH_OR_NAME\n","                        Show the help for the given subclass of {LRScheduler,ReduceLROnPlateau}\n","                        and exit.\n","  --lr_scheduler CONFIG | CLASS_PATH_OR_NAME | .INIT_ARG_NAME VALUE\n","                        One or more arguments specifying \"class_path\" and \"init_args\" for any\n","                        subclass of {LRScheduler,ReduceLROnPlateau}. (type: Union[LRScheduler,\n","                        ReduceLROnPlateau], known subclasses:\n","                        torch.optim.lr_scheduler.LRScheduler, torch.optim.lr_scheduler.LambdaLR,\n","                        torch.optim.lr_scheduler.MultiplicativeLR,\n","                        torch.optim.lr_scheduler.StepLR, torch.optim.lr_scheduler.MultiStepLR,\n","                        torch.optim.lr_scheduler.ConstantLR, torch.optim.lr_scheduler.LinearLR,\n","                        torch.optim.lr_scheduler.ExponentialLR,\n","                        torch.optim.lr_scheduler.SequentialLR,\n","                        torch.optim.lr_scheduler.PolynomialLR,\n","                        torch.optim.lr_scheduler.CosineAnnealingLR,\n","                        torch.optim.lr_scheduler.ChainedScheduler,\n","                        torch.optim.lr_scheduler.ReduceLROnPlateau,\n","                        lightning.pytorch.cli.ReduceLROnPlateau,\n","                        torch.optim.lr_scheduler.CyclicLR,\n","                        torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n","                        torch.optim.lr_scheduler.OneCycleLR, torch.optim.swa_utils.SWALR,\n","                        transformers.trainer_pt_utils.LayerWiseDummyScheduler)\n","\n","Runs the full optimization routine:\n","  --ckpt_path CKPT_PATH\n","                        Path/URL of the checkpoint from which training is resumed. Could also be\n","                        one of two special keywords ``\"last\"`` and ``\"hpc\"``. If there is no\n","                        checkpoint file at the path, an exception is raised. (type: Union[str,\n","                        Path, null], default: null)\n"]}]},{"cell_type":"code","source":["%%bash\n","\n","python l_trainer.py fit \\\n","    --trainer.max_epochs 4 \\\n","    --data.train_batch_size 2 \\\n","    --data.eval_batch_size 2 \\\n","    --optimizer torch.optim.AdamW \\\n","    --optimizer.lr 2e-5\n"],"metadata":{"id":"RYALPHD3PyBD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"OXFgDjet3ScB"}},{"cell_type":"code","source":["#@title ONNX model save\n","#@markdown ONNX 로 모델 변형 후 저장\n","from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCausalLM\n","\n","model_checkpoint = \"./\" #@param{\"type\":\"string\"}\n","save_directory = \"./\" #@param{\"type\":\"string\"}\n","\n","ort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)\n","ort_model.save_pretrained(save_directory)"],"metadata":{"cellView":"form","id":"2z7GPQJv28QO"},"execution_count":null,"outputs":[]}]}